\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Hands-on Machine Learning with R},
            pdfauthor={Brad Boehmke \& Brandon Greenwell},
            colorlinks=true,
            linkcolor=Maroon,
            filecolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}
\usepackage{amsmath}
\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\newenvironment{block}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{icons/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{note}
  {\begin{block}{note}}
  {\end{block}}
\newenvironment{caution}
  {\begin{block}{caution}}
  {\end{block}}
\newenvironment{important}
  {\begin{block}{important}}
  {\end{block}}
\newenvironment{tip}
  {\begin{block}{tip}}
  {\end{block}}
\newenvironment{warning}
  {\begin{block}{warning}}
  {\end{block}}

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Hands-on Machine Learning with R}
\author{Brad Boehmke \& Brandon Greenwell}
\date{2019-06-26}

\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
Dedication TBD
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


Welcome to \emph{Hands-on Machine Learning with R}. This book provides hands-on modules for many of the most common machine learning methods to include:

\begin{itemize}
\tightlist
\item
  Generalized low rank models
\item
  Clustering algorithms
\item
  Autoencoders
\item
  Regularized models
\item
  Random forests
\item
  Gradient boosting machines
\item
  Deep neural networks
\item
  Stacking / super learners
\item
  and more!
\end{itemize}

You will learn how to build and tune these various models with R packages that have been tested and approved due to their ability to scale well. However, our motivation in almost every case is to describe the techniques in a way that helps develop intuition for its strengths and weaknesses. For the most part, we minimize mathematical complexity when possible but also provide resources to get deeper into the details if desired.

\hypertarget{who-should-read-this}{%
\section*{Who should read this}\label{who-should-read-this}}


We intend this work to be a practitioner's guide to the machine learning process and a place where one can come to learn about the approach and to gain intuition about the many commonly used, modern, and powerful methods accepted in the machine learning community. If you are familiar with the analytic methodologies, this book may still serve as a reference for how to work with the various R packages for implementation. While an abundance of videos, blog posts, and tutorials exist online, we have long been frustrated by the lack of consistency, completeness, and bias towards singular packages for implementation. This is what inspired this book.

This book is not meant to be an introduction to R or to programming in general; as we assume the reader has familiarity with the R language to include defining functions, managing R objects, controlling the flow of a program, and other basic tasks. If not, we would refer you to \href{http://r4ds.had.co.nz/index.html}{R for Data Science} \citep{wickham2016r} to learn the fundamentals of data science with R such as importing, cleaning, transforming, visualizing, and exploring your data. For those looking to advance their R programming skills and knowledge of the languge, we would refer you to \href{http://adv-r.had.co.nz/}{Advanced R} \citep{wickham2014advanced}. Nor is this book designed to be a deep dive into the theory and math underpinning machine learning algorithms. Several books already exist that do great justice in this arena (i.e. \href{https://web.stanford.edu/~hastie/ElemStatLearn/}{Elements of Statistical Learning} \citep{esl}, \href{https://web.stanford.edu/~hastie/CASI/}{Computer Age Statistical Inference} \citep{efron2016computer}, \href{http://www.deeplearningbook.org/}{Deep Learning} \citep{goodfellow2016deep}).

Instead, this book is meant to help R users learn to use the machine learning stack within R, which includes using various R packages such as \textbf{glmnet}, \textbf{h2o}, \textbf{ranger}, \textbf{xgboost}, \textbf{lime}, and others to effectively model and gain insight from your data. The book favors a hands-on approach, growing an intuitive understanding of machine learning through concrete examples and just a little bit of theory. While you can read this book without opening R, we highly recommend you experiment with the code examples provided throughout.

\hypertarget{why-r}{%
\section*{Why R}\label{why-r}}


R has emerged over the last couple decades as a first-class tool for scientific computing tasks, and has been a consistent leader in implementing statistical methodologies for analyzing data. The usefulness of R for data science stems from the large, active, and growing ecosystem of third-party packages: \textbf{tidyverse} for common data analysis activities; \textbf{h2o}, \textbf{ranger}, \textbf{xgboost}, and others for fast and scalable machine learning; \textbf{iml}, \textbf{pdp}, \textbf{vip}, and others for machine learning interpretability; and many more tools will be mentioned throughout the pages that follow.

\hypertarget{conventions-used-in-this-book}{%
\section*{Conventions used in this book}\label{conventions-used-in-this-book}}


The following typographical conventions are used in this book:

\begin{itemize}
\tightlist
\item
  \textbf{\emph{strong italic}}: indicates new terms,
\item
  \textbf{bold}: indicates package \& file names,
\item
  \texttt{inline\ code}: monospaced highlighted text indicates functions or other commands that could be typed literally by the user,
\item
  code chunk: indicates commands or other text that could be typed literally by the user
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{+}\StringTok{ }\DecValTok{2}
\CommentTok{## [1] 3}
\end{Highlighting}
\end{Shaded}

In addition to the general text used throughout, you will notice the following code chunks with images, which signify:

\begin{tip}
Signifies a tip or suggestion
\end{tip}

\begin{note}
Signifies a general note
\end{note}

\begin{warning}
Signifies a warning or caution
\end{warning}

\hypertarget{additional-resources}{%
\section*{Additional resources}\label{additional-resources}}


There are many great resources available to learn about machine learning. Throughout the chapters we try to include many of the resources that we have found extremely useful for digging deeper into the methodology and applying with code. However, due to print restrictions, the hard copy version of this book limits the concepts and methods discussed. Online supplementary material exists at \url{https://github.com/koalaverse/hands-on-machine-learning-with-r}. The additional material will accumulate over time and include extended chapter material (i.e., random forest package benchmarking) along with brand new content we couldn't fit in (i.e., random hyperparameter search). In addition, you can download the data used throughout the book, find teaching resources (i.e., slides and exercises), and more.

\hypertarget{feedback}{%
\section*{Feedback}\label{feedback}}


Reader comments are greatly appreciated. To report errors or bugs please post an issue at \url{https://github.com/koalaverse/hands-on-machine-learning-with-r/issues}.

\hypertarget{acknowledgments}{%
\section*{Acknowledgments}\label{acknowledgments}}


TBD

\hypertarget{software-information}{%
\section*{Software information}\label{software-information}}


An online version of this book is available at \url{http://bit.ly/HOML_with_R}. The source of the book along with additional content is available at \url{https://github.com/koalaverse/hands-on-machine-learning-with-r}. The book is powered by \url{https://bookdown.org} which makes it easy to turn R markdown files into HTML, PDF, and EPUB.

This book was built with the following packages and R version. All code was executed on 2017 MacBook Pro with a 2.9 GHz Intel Core i7 processor, 16 GB of memory, 2133 MHz speed, and double data rate synchronous dynamic random access memory (DDR3).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# packages used}
\NormalTok{pkgs <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}
  \StringTok{"AmesHousing"}\NormalTok{,}
  \StringTok{"bookdown"}\NormalTok{,}
  \StringTok{"caret"}\NormalTok{,}
  \StringTok{"cluster"}\NormalTok{,}
  \StringTok{"DALEX"}\NormalTok{,}
  \StringTok{"data.table"}\NormalTok{,}
  \StringTok{"dplyr"}\NormalTok{,}
  \StringTok{"dslabs"}\NormalTok{,}
  \StringTok{"e1071"}\NormalTok{,}
  \StringTok{"earth"}\NormalTok{,}
  \StringTok{"emo"}\NormalTok{,}
  \StringTok{"extracat"}\NormalTok{,}
  \StringTok{"factoextra"}\NormalTok{,}
  \StringTok{"ggplot2"}\NormalTok{,}
  \StringTok{"gbm"}\NormalTok{,}
  \StringTok{"glmnet"}\NormalTok{,}
  \StringTok{"h2o"}\NormalTok{,}
  \StringTok{"iml"}\NormalTok{,}
  \StringTok{"ipred"}\NormalTok{,}
  \StringTok{"keras"}\NormalTok{,}
  \StringTok{"kernlab"}\NormalTok{,}
  \StringTok{"MASS"}\NormalTok{,}
  \StringTok{"mclust"}\NormalTok{,}
  \StringTok{"mlbench"}\NormalTok{,}
  \StringTok{"pBrackets"}\NormalTok{,}
  \StringTok{"pdp"}\NormalTok{,}
  \StringTok{"pls"}\NormalTok{,}
  \StringTok{"pROC"}\NormalTok{,}
  \StringTok{"purrr"}\NormalTok{,}
  \StringTok{"ranger"}\NormalTok{,}
  \StringTok{"recipes"}\NormalTok{,}
  \StringTok{"reshape2"}\NormalTok{,}
  \StringTok{"ROCR"}\NormalTok{,}
  \StringTok{"rpart"}\NormalTok{,}
  \StringTok{"rpart.plot"}\NormalTok{,}
  \StringTok{"rsample"}\NormalTok{,}
  \StringTok{"tfruns"}\NormalTok{,}
  \StringTok{"tfestimators"}\NormalTok{,}
  \StringTok{"vip"}\NormalTok{,}
  \StringTok{"xgboost"}
\NormalTok{)}

\CommentTok{# package & session info}
\NormalTok{sessioninfo}\OperatorTok{::}\KeywordTok{session_info}\NormalTok{(pkgs)}
\CommentTok{#> - Session info --------------------------------------}
\CommentTok{#>  setting  value                       }
\CommentTok{#>  version  R version 3.6.0 (2019-04-26)}
\CommentTok{#>  os       macOS Sierra 10.12.6        }
\CommentTok{#>  system   x86_64, darwin15.6.0        }
\CommentTok{#>  ui       RStudio                     }
\CommentTok{#>  language (EN)                        }
\CommentTok{#>  collate  en_US.UTF-8                 }
\CommentTok{#>  ctype    en_US.UTF-8                 }
\CommentTok{#>  tz       America/New_York            }
\CommentTok{#>  date     2019-06-26                  }
\CommentTok{#> }
\CommentTok{#> - Packages ------------------------------------------}
\CommentTok{#>  ! package       * version    date       lib}
\CommentTok{#>    abind           1.4-5      2016-07-21 [1]}
\CommentTok{#>    AmesHousing     0.0.3      2017-12-17 [1]}
\CommentTok{#>    assertthat      0.2.1      2019-03-21 [1]}
\CommentTok{#>    backports       1.1.4      2019-04-10 [1]}
\CommentTok{#>    base64enc       0.1-3      2015-07-28 [1]}
\CommentTok{#>    BH              1.69.0-1   2019-01-07 [1]}
\CommentTok{#>    bitops          1.0-6      2013-08-17 [1]}
\CommentTok{#>    bookdown        0.11       2019-05-28 [1]}
\CommentTok{#>    boot            1.3-22     2019-04-02 [1]}
\CommentTok{#>    car             3.0-3      2019-05-27 [1]}
\CommentTok{#>    carData         3.0-2      2018-09-30 [1]}
\CommentTok{#>    caret         * 6.0-84     2019-04-27 [1]}
\CommentTok{#>    caTools         1.17.1.2   2019-03-06 [1]}
\CommentTok{#>    cellranger      1.1.0      2016-07-27 [1]}
\CommentTok{#>    checkmate       1.9.3      2019-05-03 [1]}
\CommentTok{#>    class           7.3-15     2019-01-01 [1]}
\CommentTok{#>    cli             1.1.0      2019-03-19 [1]}
\CommentTok{#>    clipr           0.6.0      2019-04-15 [1]}
\CommentTok{#>    cluster         2.0.8      2019-04-05 [1]}
\CommentTok{#>    codetools       0.2-16     2018-12-24 [1]}
\CommentTok{#>    colorspace      1.4-1      2019-03-18 [1]}
\CommentTok{#>    config          0.3        2018-03-27 [1]}
\CommentTok{#>    cowplot         0.9.4      2019-01-08 [1]}
\CommentTok{#>    crayon          1.3.4      2017-09-16 [1]}
\CommentTok{#>    curl            3.3        2019-01-10 [1]}
\CommentTok{#>    DALEX           0.3.0      2019-03-25 [1]}
\CommentTok{#>    data.table      1.12.2     2019-04-07 [1]}
\CommentTok{#>    dendextend      1.12.0     2019-05-11 [1]}
\CommentTok{#>    digest          0.6.19     2019-05-20 [1]}
\CommentTok{#>    dplyr         * 0.8.1      2019-05-14 [1]}
\CommentTok{#>    dslabs          0.5.2      2018-12-19 [1]}
\CommentTok{#>    e1071           1.7-1      2019-03-19 [1]}
\CommentTok{#>    earth         * 5.1.1      2019-04-12 [1]}
\CommentTok{#>    ellipse         0.4.1      2018-01-05 [1]}
\CommentTok{#>    ellipsis        0.1.0      2019-02-19 [1]}
\CommentTok{#>    emo             0.0.0.9000 2019-05-03 [1]}
\CommentTok{#>    evaluate        0.14       2019-05-28 [1]}
\CommentTok{#>  R extracat        <NA>       <NA>       [?]}
\CommentTok{#>    factoextra      1.0.5      2017-08-22 [1]}
\CommentTok{#>    FactoMineR      1.41       2018-05-04 [1]}
\CommentTok{#>    fansi           0.4.0      2018-10-05 [1]}
\CommentTok{#>    flashClust      1.01-2     2012-08-21 [1]}
\CommentTok{#>    forcats       * 0.4.0      2019-02-17 [1]}
\CommentTok{#>    foreach       * 1.4.4      2017-12-12 [1]}
\CommentTok{#>    foreign         0.8-71     2018-07-20 [1]}
\CommentTok{#>    forge           0.2.0      2019-02-26 [1]}
\CommentTok{#>    Formula       * 1.2-3      2018-05-03 [1]}
\CommentTok{#>    gbm             2.1.5      2019-01-14 [1]}
\CommentTok{#>    gdata           2.18.0     2017-06-06 [1]}
\CommentTok{#>    generics        0.0.2      2018-11-29 [1]}
\CommentTok{#>    ggplot2       * 3.1.1      2019-04-07 [1]}
\CommentTok{#>    ggpubr          0.2        2018-11-15 [1]}
\CommentTok{#>    ggrepel         0.8.1      2019-05-07 [1]}
\CommentTok{#>    ggsci           2.9        2018-05-14 [1]}
\CommentTok{#>    ggsignif        0.5.0      2019-02-20 [1]}
\CommentTok{#>    glmnet        * 2.0-16     2018-04-02 [1]}
\CommentTok{#>    glue            1.3.1.9000 2019-05-03 [1]}
\CommentTok{#>    gower           0.2.0      2019-03-07 [1]}
\CommentTok{#>    gplots        * 3.0.1.1    2019-01-27 [1]}
\CommentTok{#>    gridExtra       2.3        2017-09-09 [1]}
\CommentTok{#>    gtable          0.3.0      2019-03-25 [1]}
\CommentTok{#>    gtools          3.8.1      2018-06-26 [1]}
\CommentTok{#>    h2o           * 3.22.1.1   2019-01-10 [1]}
\CommentTok{#>    haven           2.1.0      2019-02-19 [1]}
\CommentTok{#>    highr           0.8        2019-03-20 [1]}
\CommentTok{#>    hms             0.4.2      2018-03-10 [1]}
\CommentTok{#>    htmltools       0.3.6      2017-04-28 [1]}
\CommentTok{#>    iml             0.9.0      2019-02-05 [1]}
\CommentTok{#>    inum            1.0-1      2019-04-25 [1]}
\CommentTok{#>    ipred           0.9-9      2019-04-28 [1]}
\CommentTok{#>    iterators       1.0.10     2018-07-13 [1]}
\CommentTok{#>    jsonlite        1.6        2018-12-07 [1]}
\CommentTok{#>    keras           2.2.4.1    2019-04-05 [1]}
\CommentTok{#>    kernlab         0.9-27     2018-08-10 [1]}
\CommentTok{#>    KernSmooth      2.23-15    2015-06-29 [1]}
\CommentTok{#>    knitr         * 1.23       2019-05-18 [1]}
\CommentTok{#>    labeling        0.3        2014-08-23 [1]}
\CommentTok{#>    lattice       * 0.20-38    2018-11-04 [1]}
\CommentTok{#>    lava            1.6.5      2019-02-12 [1]}
\CommentTok{#>    lazyeval        0.2.2      2019-03-15 [1]}
\CommentTok{#>    leaps           3.0        2017-01-10 [1]}
\CommentTok{#>    libcoin         1.0-4      2019-02-28 [1]}
\CommentTok{#>    lme4            1.1-21     2019-03-05 [1]}
\CommentTok{#>    lubridate       1.7.4      2018-04-11 [1]}
\CommentTok{#>    magrittr        1.5        2014-11-22 [1]}
\CommentTok{#>    maptools        0.9-5      2019-02-18 [1]}
\CommentTok{#>    markdown        1.0        2019-06-07 [1]}
\CommentTok{#>    MASS            7.3-51.4   2019-03-31 [1]}
\CommentTok{#>    Matrix        * 1.2-17     2019-03-22 [1]}
\CommentTok{#>    MatrixModels    0.4-1      2015-08-22 [1]}
\CommentTok{#>    mclust          5.4.3      2019-03-14 [1]}
\CommentTok{#>    Metrics         0.1.4      2018-07-09 [1]}
\CommentTok{#>    mgcv            1.8-28     2019-03-21 [1]}
\CommentTok{#>    mime            0.7        2019-06-11 [1]}
\CommentTok{#>    minqa           1.2.4      2014-10-09 [1]}
\CommentTok{#>    mlbench         2.1-1      2012-07-10 [1]}
\CommentTok{#>    ModelMetrics    1.2.2      2018-11-03 [1]}
\CommentTok{#>    munsell         0.5.0      2018-06-12 [1]}
\CommentTok{#>    mvtnorm         1.0-10     2019-03-05 [1]}
\CommentTok{#>    nlme            3.1-139    2019-04-09 [1]}
\CommentTok{#>    nloptr          1.2.1      2018-10-03 [1]}
\CommentTok{#>    nnet            7.3-12     2016-02-02 [1]}
\CommentTok{#>    numDeriv        2016.8-1   2016-08-27 [1]}
\CommentTok{#>    openxlsx        4.1.0.1    2019-05-28 [1]}
\CommentTok{#>    partykit        1.2-3      2019-01-31 [1]}
\CommentTok{#>    pbkrtest        0.4-7      2017-03-15 [1]}
\CommentTok{#>    pBrackets       1.0        2014-10-17 [1]}
\CommentTok{#>    pdp           * 0.7.0      2018-08-27 [1]}
\CommentTok{#>    pillar          1.4.1      2019-05-28 [1]}
\CommentTok{#>    pkgconfig       2.0.2      2018-08-16 [1]}
\CommentTok{#>    plogr           0.2.0      2018-03-25 [1]}
\CommentTok{#>    plotmo        * 3.5.4      2019-04-06 [1]}
\CommentTok{#>    plotrix       * 3.7-5      2019-04-07 [1]}
\CommentTok{#>    pls           * 2.7-1      2019-03-23 [1]}
\CommentTok{#>    plyr            1.8.4      2016-06-08 [1]}
\CommentTok{#>    polynom         1.4-0      2019-03-22 [1]}
\CommentTok{#>    prediction      0.3.6.2    2019-01-31 [1]}
\CommentTok{#>    prettyunits     1.0.2      2015-07-13 [1]}
\CommentTok{#>    pROC            1.14.0     2019-03-12 [1]}
\CommentTok{#>    processx        3.3.0      2019-03-10 [1]}
\CommentTok{#>    prodlim         2018.04.18 2018-04-18 [1]}
\CommentTok{#>    progress        1.2.2      2019-05-16 [1]}
\CommentTok{#>    ps              1.3.0      2018-12-21 [1]}
\CommentTok{#>    purrr         * 0.3.2      2019-03-15 [1]}
\CommentTok{#>    quantreg        5.38       2018-12-18 [1]}
\CommentTok{#>    R6              2.4.0      2019-02-14 [1]}
\CommentTok{#>    ranger          0.11.2     2019-03-07 [1]}
\CommentTok{#>    RColorBrewer    1.1-2      2014-12-07 [1]}
\CommentTok{#>    Rcpp            1.0.1      2019-03-17 [1]}
\CommentTok{#>    RcppEigen       0.3.3.5.0  2018-11-24 [1]}
\CommentTok{#>    RcppRoll        0.3.0      2018-06-05 [1]}
\CommentTok{#>    RCurl           1.95-4.12  2019-03-04 [1]}
\CommentTok{#>    readr         * 1.3.1      2018-12-21 [1]}
\CommentTok{#>    readxl          1.3.1      2019-03-13 [1]}
\CommentTok{#>    recipes       * 0.1.5      2019-03-21 [1]}
\CommentTok{#>    rematch         1.0.1      2016-04-21 [1]}
\CommentTok{#>    reshape2        1.4.3      2017-12-11 [1]}
\CommentTok{#>    reticulate      1.12       2019-04-12 [1]}
\CommentTok{#>    rio             0.5.16     2018-11-26 [1]}
\CommentTok{#>    rlang           0.3.4      2019-04-07 [1]}
\CommentTok{#>    rmarkdown       1.13       2019-05-22 [1]}
\CommentTok{#>    ROCR          * 1.0-7      2015-03-26 [1]}
\CommentTok{#>    rpart           4.1-15     2019-04-12 [1]}
\CommentTok{#>    rpart.plot      3.0.7      2019-04-12 [1]}
\CommentTok{#>    rsample       * 0.0.4      2019-01-07 [1]}
\CommentTok{#>    rstudioapi      0.10       2019-03-19 [1]}
\CommentTok{#>    scales          1.0.0      2018-08-09 [1]}
\CommentTok{#>    scatterplot3d   0.3-41     2018-03-14 [1]}
\CommentTok{#>    sp              1.3-1      2018-06-05 [1]}
\CommentTok{#>    SparseM         1.77       2017-04-23 [1]}
\CommentTok{#>    SQUAREM         2017.10-1  2017-10-07 [1]}
\CommentTok{#>    stringi         1.4.3      2019-03-12 [1]}
\CommentTok{#>    stringr       * 1.4.0      2019-02-10 [1]}
\CommentTok{#>    survival        2.44-1.1   2019-04-01 [1]}
\CommentTok{#>    TeachingDemos * 2.10       2016-02-12 [1]}
\CommentTok{#>    tensorflow      1.13.1     2019-04-05 [1]}
\CommentTok{#>    tfestimators    1.9.1      2018-11-07 [1]}
\CommentTok{#>    tfruns          1.4        2018-08-25 [1]}
\CommentTok{#>    tibble        * 2.1.2      2019-05-29 [1]}
\CommentTok{#>    tidyr         * 0.8.3      2019-03-01 [1]}
\CommentTok{#>    tidyselect      0.2.5      2018-10-11 [1]}
\CommentTok{#>    timeDate        3043.102   2018-02-21 [1]}
\CommentTok{#>    tinytex         0.13       2019-05-14 [1]}
\CommentTok{#>    utf8            1.1.4      2018-05-24 [1]}
\CommentTok{#>    vctrs           0.1.0      2018-11-29 [1]}
\CommentTok{#>    vip           * 0.1.2.9000 2019-06-04 [1]}
\CommentTok{#>    viridis         0.5.1      2018-03-29 [1]}
\CommentTok{#>    viridisLite     0.3.0      2018-02-01 [1]}
\CommentTok{#>    whisker         0.3-2      2013-04-28 [1]}
\CommentTok{#>    withr           2.1.2      2018-03-15 [1]}
\CommentTok{#>    xfun            0.7        2019-05-14 [1]}
\CommentTok{#>    xgboost         0.82.1     2019-03-11 [1]}
\CommentTok{#>    yaImpute        1.0-31     2019-01-09 [1]}
\CommentTok{#>    yaml            2.2.0      2018-07-25 [1]}
\CommentTok{#>    zeallot         0.1.0      2018-01-28 [1]}
\CommentTok{#>    zip             2.0.2      2019-05-13 [1]}
\CommentTok{#>  source                         }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  Github (hadley/emo@02a5206)    }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  <NA>                           }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  Github (tidyverse/glue@ea0edcb)}
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  Github (koalaverse/vip@9d537bb)}
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#> }
\CommentTok{#> [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library}
\CommentTok{#> }
\CommentTok{#>  R -- Package was removed from disk.}
\end{Highlighting}
\end{Shaded}

\mainmatter

\hypertarget{part-fundamentals}{%
\part{Fundamentals}\label{part-fundamentals}}

\hypertarget{intro}{%
\chapter{Introduction to Machine Learning}\label{intro}}

Machine learning (ML) continues to grow in importance for many organizations across nearly all domains. Some example applications of machine learning in practice include:

\begin{itemize}
\tightlist
\item
  Predicting the likelihood of a patient returning to the hospital (\emph{readmission}) within 30 days of discharge.
\item
  Segmenting customers based on common attributes or purchasing behavior for targeted marketing.
\item
  Predicting coupon redemption rates for a given marketing campaign.
\item
  Predicting customer churn so an organization can perform preventative intervention.
\item
  And many more!
\end{itemize}

In essence, these tasks all seek to learn from data. To address each scenario, we can use a given set of \emph{features}\index{features} to train an algorithm and extract insights. These algorithms, or \emph{learners}\index{learners}, can be classified according to the amount and type of supervision needed during training. The two main groups this book focuses on are: \textbf{\emph{supervised learners}} which construct predictive models, and \textbf{\emph{unsupervised learners}} which build descriptive models. Which type you will need to use depends on the learning task you hope to accomplish.

\hypertarget{supervised-learning}{%
\section{Supervised learning}\label{supervised-learning}}

A \textbf{\emph{predictive model}}\index{predictive model} is used for tasks that involve the prediction of a given output (or target) using other variables (or features) in the data set. Or, as stated by \citet[p.~2]{apm}, predictive modeling is ``\ldots{}the process of developing a mathematical tool or model that generates an accurate prediction.'' The learning algorithm in a predictive model attempts to discover and model the relationships among the target variable (the variable being predicted) and the other features (aka predictor variables). Examples of predictive modeling include:

\begin{itemize}
\tightlist
\item
  using customer attributes to predict the probability of the customer churning in the next 6 weeks;
\item
  using home attributes to predict the sales price;
\item
  using employee attributes to predict the likelihood of attrition;
\item
  using patient attributes and symptoms to predict the risk of readmission;
\item
  using production attributes to predict time to market.
\end{itemize}

Each of these examples have a defined learning task; they each intend to use attributes (\(X\)) to predict an outcome measurement (\(Y\)).

\begin{note}
Throughout this text we'll use various terms interchangeably for:

\begin{itemize}
\tightlist
\item
  \(X\): ``predictor variables'', ``independent variables'',
  ``attributes'', ``features'', ``predictors''
\item
  \(Y\): ``target variable'', ``dependent variable'', ``response'',
  ``outcome measurement''
\end{itemize}
\end{note}

The predictive modeling examples above describe what is known as \emph{supervised learning}\index{supervised learning}. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible.

\begin{note}
In supervised learning, the training data you feed the algorithm
includes the target values. Consequently, the solutions can be used to
help \emph{supervise} the training process to find the optimal algorithm
parameters.
\end{note}

Most supervised learning problems can be bucketed into one of two categories: \emph{regression}\index{regression} or \emph{classification}\index{classification}, which we discuss next.

\hypertarget{regression-problems}{%
\subsection{Regression problems}\label{regression-problems}}

When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a \textbf{\emph{regression problem}} (not to be confused with linear regression modeling). Regression problems revolve around predicting output that falls on a continuum. In the examples above, predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous. This means, given the combination of predictor values, the response value could fall anywhere along some continuous spectrum (e.g., the predicted sales price of a particular home could be between \$80,000 and \$755,000). Figure \ref{fig:regression-problem} illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along a plane.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/regression-problem-1} 

}

\caption{Average home sales price as a function of year built and total square footage.}\label{fig:regression-problem}
\end{figure}

\hypertarget{classification-problems}{%
\subsection{Classification problems}\label{classification-problems}}

When the objective of our supervised learning is to predict a categorical outcome, we refer to this as a \textbf{\emph{classification problem}}. Classification problems most commonly revolve around predicting a binary or multinomial response measure such as:

\begin{itemize}
\tightlist
\item
  Did a customer redeem a coupon (coded as yes/no or 1/0).
\item
  Did a customer churn (coded as yes/no or 1/0).
\item
  Did a customer click on our online ad (coded as yes/no or 1/0).
\item
  Classifying customer reviews:

  \begin{itemize}
  \tightlist
  \item
    Binary: positive vs.~negative.
  \item
    Multinomial: extremely negative to extremely positive on a 0--5 Likert scale.
  \end{itemize}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/classification_problem} 

}

\caption{Classification problem modeling 'Yes'/'No' response based on three features.}\label{fig:classification-problem}
\end{figure}

However, when we apply machine learning models for classification problems, rather than predict a particular class (i.e., ``yes'' or ``no''), we often want to predict the \emph{probability} of a particular class (i.e., yes: 0.65, no: 0.35). By default, the class with the highest predicted probability becomes the predicted class. Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability). However, the essence of the problem still makes it a classification problem.

Although there are machine learning algorithms that can be applied to regression problems but not classification and vice versa, most of the supervised learning algorithms we cover in this book can be applied to both. These algorithms have become the most popular machine learning applications in recent years.

\hypertarget{unsupervised-learning}{%
\section{Unsupervised learning}\label{unsupervised-learning}}

\textbf{\emph{Unsupervised learning}}\index{unsupervised learning}, in contrast to supervised learning, includes a set of statistical tools to better understand and describe your data, but performs the analysis without a target variable. In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., \emph{clustering}) or the columns (i.e., \emph{dimension reduction}); however, the motive in each case is quite different.

The goal of \textbf{\emph{clustering}}\index{clustering} is to segment observations into similar groups based on the observed variables; for example, to divide consumers into different homogeneous groups, a process known as market segmentation. In \textbf{dimension reduction}\index{dimension reduction}, we are often concerned with reducing the number of variables in a data set. For example, classical linear regression models break down in the presence of highly correlated features. Some dimension reduction techniques can be used to reduce the feature set to a potentially smaller set of uncorrelated variables. Such a reduced feature set is often used as input to downstream supervised learning models (e.g., principal component regression).

Unsupervised learning is often performed as part of an exploratory data analysis (EDA). However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e., linear regression), then it is possible to check our work by seeing how well our model predicts the response \emph{Y} on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don't know the true answer---the problem is unsupervised!

Despide its subjectivity, the importance of unsupervised learning should not be overlooked and such techniques are often used in organizations to:

\begin{itemize}
\tightlist
\item
  Divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment.
\item
  Identify groups of online shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers.
\item
  Identify products that have similar purchasing behavior so that managers can manage them as product groups.
\end{itemize}

These questions, and many more, can be addressed with unsupervised learning. Moreover, the outputs of an unsupervised learning models can be used as inputs to downstream supervised learning models.

\hypertarget{roadmap}{%
\section{Roadmap}\label{roadmap}}

The goal of this book is to provide effective tools for uncovering relevant and useful patterns in your data by using R's ML stack. We begin by providing an overview of the ML modeling process and discussing fundamental concepts that will carry through the rest of the book. These include feature engineering, data splitting, model validation and tuning, and performance measurement. These concepts will be discussed in Chapters \ref{process}-\ref{engineering}.

Chapters \ref{linear-regression}-\ref{svm} focus on common supervised learners ranging from simpler linear regression models to the more complicated gradient boosting machines and deep neural networks. Here we will illustrate the fundamental concepts of each base learning algorithm and how to tune its hyperparameters to maximize predictive performance.

Chapters \ref{stacking}-\ref{iml} delve into more advanced approaches to maximize effectiveness, efficiency, and interpretation of your ML models. We discuss how to combine multiple models to create a stacked model (aka \emph{super learner}\index{super learnier}), which allows you to combine the strengths from each base learner and further maximize predictive accuracy. We then illustrate how to make the training and validation process more efficient with automated ML (aka AutoML). Finally, we illustrate many ways to extract insight from your ``black box'' models with various ML interpretation techniques.

The latter part of the book focuses on unsupervised techniques aimed at reducing the dimensions of your data for more effective data representation (Chapters \ref{pca}-\ref{autoencoders}) and identifying common groups among your observations with clustering techniques (Chapters \ref{kmeans}-\ref{model-clustering}).

\hypertarget{data}{%
\section{The data sets}\label{data}}

The data sets chosen for this book allow us to illustrate the different features of the presented machine learning algorithms. Since the goal of this book is to demonstrate how to implement R's ML stack, we make the assumption that you have already spent significant time cleaning and getting to know your data via EDA. This would allow you to perform many necessary tasks prior to the ML tasks outlined in this book such as:

\begin{itemize}
\tightlist
\item
  Feature selection (i.e., removing unnecessary variables and retaining only those variables you wish to include in your modeling process).
\item
  Recoding variable names and values so that they are meaningful and more interpretable.
\item
  Recoding, removing, or some other approach to handling missing values.
\end{itemize}

Consequently, the exemplar data sets we use throughout this book have, for the most part, gone through the necessary cleaning processes. In some cases we illustrate concepts with stereotypical data sets (i.e. \texttt{mtcars}, \texttt{iris}, \texttt{geyser}); however, we tend to focus most of our discussion around the following data sets:

\begin{itemize}
\tightlist
\item
  Property sales information as described in \citet{de2011ames}.

  \begin{itemize}
  \tightlist
  \item
    \textbf{problem type}: supervised regression
  \item
    \textbf{response variable}: \texttt{Sale\_Price} (i.e., \$195,000, \$215,000)
  \item
    \textbf{features}: 80
  \item
    \textbf{observations}: 2,930
  \item
    \textbf{objective}: use property attributes to predict the sale price of a home
  \item
    \textbf{access}: provided by the \texttt{AmesHousing} package \citep{R-ames}
  \item
    \textbf{more details}: See \texttt{?AmesHousing::ames\_raw}
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# access data}
\NormalTok{ames <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{()}

\CommentTok{# initial dimension}
\KeywordTok{dim}\NormalTok{(ames)}
\CommentTok{## [1] 2930   81}

\CommentTok{# response variable}
\KeywordTok{head}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Sale_Price)}
\CommentTok{## [1] 215000 105000 172000 244000 189900 195500}
\end{Highlighting}
\end{Shaded}

  \begin{note}
    You can see the entire data cleaning process to transform the raw Ames
    housing data (\texttt{AmesHousing::ames\_raw}) to the final clean data
    (\texttt{AmesHousing::make\_ames}) that we will use in machine learning
    algorithms throughout this book at:

    \url{https://github.com/topepo/AmesHousing/blob/master/R/make_ames.R}
    \end{note}
\item
  Employee attrition information originally provided by \href{https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/}{IBM Watson Analytics Lab}.

  \begin{itemize}
  \tightlist
  \item
    \textbf{problem type}: supervised binomial classification
  \item
    \textbf{response variable}: \texttt{Attrition} (i.e., ``Yes'', ``No'')
  \item
    \textbf{features}: 30
  \item
    \textbf{observations}: 1,470
  \item
    \textbf{objective}: use employee attributes to predict if they will attrit (leave the company)
  \item
    \textbf{access}: provided by the \texttt{rsample} package \citep{R-rsample}
  \item
    \textbf{more details}: See \texttt{?rsample::attrition}
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# access data}
\NormalTok{attrition <-}\StringTok{ }\NormalTok{rsample}\OperatorTok{::}\NormalTok{attrition}

\CommentTok{# initial dimension}
\KeywordTok{dim}\NormalTok{(attrition)}
\CommentTok{## [1] 1470   31}

\CommentTok{# response variable}
\KeywordTok{head}\NormalTok{(attrition}\OperatorTok{$}\NormalTok{Attrition)}
\CommentTok{## [1] Yes No  Yes No  No  No }
\CommentTok{## Levels: No Yes}
\end{Highlighting}
\end{Shaded}
\item
  Image information for handwritten numbers originally presented to AT\&T Bell Lab's to help build automatic mail-sorting machines for the USPS. Has been used since early 1990s to compare machine learning performance on pattern recognition (i.e., \citet{lecun1990handwritten}; \citet{lecun1998gradient}; \citet{cirecsan2012multi}).

  \begin{itemize}
  \tightlist
  \item
    \textbf{Problem type}: supervised multinomial classification
  \item
    \textbf{response variable}: \texttt{V785} (i.e., numbers to predict: 0, 1, \ldots{}, 9)
  \item
    \textbf{features}: 784
  \item
    \textbf{observations}: 60,000 (train) / 10,000 (test)
  \item
    \textbf{objective}: use attributes about the ``darkness'' of each of the 784 pixels in images of handwritten numbers to predict if the number is 0, 1, \ldots{}, or 9.
  \item
    \textbf{access}: provided by the \texttt{dslabs} package \citep{R-dslabs}
  \item
    \textbf{more details}: See \texttt{?dslabs::read\_mnist()} and \href{http://yann.lecun.com/exdb/mnist/}{online MNIST documentation}
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#access data}
\NormalTok{mnist <-}\StringTok{ }\NormalTok{dslabs}\OperatorTok{::}\KeywordTok{read_mnist}\NormalTok{()}
\KeywordTok{names}\NormalTok{(mnist)}
\CommentTok{## [1] "train" "test"}

\CommentTok{# initial feature dimensions}
\KeywordTok{dim}\NormalTok{(mnist}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{images)}
\CommentTok{## [1] 60000   784}

\CommentTok{# response variable}
\KeywordTok{head}\NormalTok{(mnist}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{labels)}
\CommentTok{## [1] 5 0 4 1 9 2}
\end{Highlighting}
\end{Shaded}
\item
  Grocery items and quantities purchased. Each observation represents a single basket of goods that were purchased together.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Problem type}: unsupervised basket analysis
  \item
    \textbf{response variable}: NA
  \item
    \textbf{features}: 42
  \item
    \textbf{observations}: 2,000
  \item
    \textbf{objective}: use attributes of each basket to identify common groupings of items purchased together.
  \item
    \textbf{access}: available via additional online material
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# access data}
\NormalTok{my_basket <-}\StringTok{ }\NormalTok{readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"data/my_basket.csv"}\NormalTok{)}

\CommentTok{# initial dimension}
\KeywordTok{dim}\NormalTok{(my_basket)}
\CommentTok{## [1] 2000   42}

\CommentTok{# response variable}
\NormalTok{my_basket}
\CommentTok{## # A tibble: 2,000 x 42}
\CommentTok{##    `7up` lasagna pepsi   yop `red-wine` cheese   bbq}
\CommentTok{##    <dbl>   <dbl> <dbl> <dbl>      <dbl>  <dbl> <dbl>}
\CommentTok{##  1     0       0     0     0          0      0     0}
\CommentTok{##  2     0       0     0     0          0      0     0}
\CommentTok{##  3     0       0     0     0          0      0     0}
\CommentTok{##  4     0       0     0     2          1      0     0}
\CommentTok{##  5     0       0     0     0          0      0     0}
\CommentTok{##  6     0       0     0     0          0      0     0}
\CommentTok{##  7     1       1     0     0          0      0     1}
\CommentTok{##  8     0       0     0     0          0      0     0}
\CommentTok{##  9     0       1     0     0          0      0     0}
\CommentTok{## 10     0       0     0     0          0      0     0}
\CommentTok{## # ... with 1,990 more rows, and 35 more variables:}
\CommentTok{## #   bulmers <dbl>, mayonnaise <dbl>, horlics <dbl>,}
\CommentTok{## #   `chicken-tikka` <dbl>, milk <dbl>, mars <dbl>,}
\CommentTok{## #   coke <dbl>, lottery <dbl>, bread <dbl>,}
\CommentTok{## #   pizza <dbl>, `sunny-delight` <dbl>, ham <dbl>,}
\CommentTok{## #   lettuce <dbl>, kronenbourg <dbl>, leeks <dbl>,}
\CommentTok{## #   fanta <dbl>, tea <dbl>, whiskey <dbl>, peas <dbl>,}
\CommentTok{## #   newspaper <dbl>, muesli <dbl>, `white-wine` <dbl>,}
\CommentTok{## #   carrots <dbl>, spinach <dbl>, pate <dbl>,}
\CommentTok{## #   `instant-coffee` <dbl>, twix <dbl>,}
\CommentTok{## #   potatoes <dbl>, fosters <dbl>, soup <dbl>,}
\CommentTok{## #   `toad-in-hole` <dbl>, `coco-pops` <dbl>,}
\CommentTok{## #   kitkat <dbl>, broccoli <dbl>, cigarettes <dbl>}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\hypertarget{process}{%
\chapter{Modeling Process}\label{process}}

Much like EDA, the ML process is very iterative and heurstic-based. With minimal knowledge of the problem or data at hand, it is difficult to know which ML method will perform best. This is known as the \emph{no free lunch}\index{no free lunch} theorem for ML \citep{wolpert1996lack}. Consequently, it is common for many ML approaches to be applied, evaluated, and modified before a final, optimal model can be determined. Performing this process correctly provides great confidence in our outcomes. If not, the results will be useless and, potentially, damaging \footnote{See \url{https://www.fatml.org/resources/relevant-scholarship} for many discussions regarding implications of poorly applied and interpreted ML.}.

Approaching ML modeling correctly means approaching it strategically by spending our data wisely on learning and validation procedures, properly pre-processing the feature and target variables, minimizing \emph{data leakage}\index{data leakage} (Section \ref{data-leakage}), tuning hyperparameters, and assessing model performance. Many books and courses portray the modeling process as a short sprint. A better analogy would be a marathon where many iterations of these steps are repeated before eventually finding the final optimal model. This process is illustrated in Figure \ref{fig:02-modeling-process}. Before introducing specific algorithms, this chapter, and the next, introduce concepts that are fundamental to the ML modeling process and that you'll see briskly covered in future modeling chapters.

\begin{note}
Although the discussions in this chapter focuses on supervised ML
modeling, many of the topics also apply to unsupervised methods.
\end{note}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth,height=0.9\textheight]{images/modeling_process} 

}

\caption{General predictive machine learning process.}\label{fig:02-modeling-process}
\end{figure}

\hypertarget{prerequisites}{%
\section{Prerequisites}\label{prerequisites}}

This chapter leverages the following packages.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Helper packages}
\KeywordTok{library}\NormalTok{(dplyr)     }\CommentTok{# for data manipulation}
\KeywordTok{library}\NormalTok{(ggplot2)   }\CommentTok{# for awesome graphics}

\CommentTok{# Modeling process packages}
\KeywordTok{library}\NormalTok{(rsample)   }\CommentTok{# for resampling procedures}
\KeywordTok{library}\NormalTok{(caret)     }\CommentTok{# for resampling and model training}
\KeywordTok{library}\NormalTok{(h2o)       }\CommentTok{# for resampling and model training}

\CommentTok{# h2o set-up }
\KeywordTok{h2o.no_progress}\NormalTok{()  }\CommentTok{# turn off h2o progress bars}
\KeywordTok{h2o.init}\NormalTok{()         }\CommentTok{# launch h2o}
\CommentTok{##  Connection successful!}
\CommentTok{## }
\CommentTok{## R is connected to the H2O cluster: }
\CommentTok{##     H2O cluster uptime:         17 minutes 19 seconds }
\CommentTok{##     H2O cluster timezone:       America/New_York }
\CommentTok{##     H2O data parsing timezone:  UTC }
\CommentTok{##     H2O cluster version:        3.22.1.1 }
\CommentTok{##     H2O cluster version age:    5 months and 28 days !!! }
\CommentTok{##     H2O cluster name:           H2O_started_from_R_b294776_vmp196 }
\CommentTok{##     H2O cluster total nodes:    1 }
\CommentTok{##     H2O cluster total memory:   3.28 GB }
\CommentTok{##     H2O cluster total cores:    8 }
\CommentTok{##     H2O cluster allowed cores:  8 }
\CommentTok{##     H2O cluster healthy:        TRUE }
\CommentTok{##     H2O Connection ip:          localhost }
\CommentTok{##     H2O Connection port:        54321 }
\CommentTok{##     H2O Connection proxy:       NA }
\CommentTok{##     H2O Internal Security:      FALSE }
\CommentTok{##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 }
\CommentTok{##     R Version:                  R version 3.6.0 (2019-04-26)}
\end{Highlighting}
\end{Shaded}

To illustrate some of the concepts, we'll use the Ames Housing and employee attrition data sets introduced in Chapter \ref{intro}. Throughout this book, we'll demonstrate approaches with ordinary R data frames. However, since many of the supervised machine learning chapters leverage the \textbf{h2o} package, we'll also show how to do some of the tasks with H2O objects. You can convert any R data frame to an H2O object (i.e., import it to the H2O cloud) easily with \texttt{as.h2o(\textless{}my-data-frame\textgreater{})}.

\begin{warning}
If you try to convert the original \texttt{rsample::attrition} data set
to an H2O object an error will occur. This is because several variables
are \emph{ordered factors} and H2O has no way of handling this data
type. Consequently, you must convert any ordered factors to unordered;
see \texttt{?base::ordered} for details.
\end{warning}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ames data}
\NormalTok{ames <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{()}
\NormalTok{ames.h2o <-}\StringTok{ }\KeywordTok{as.h2o}\NormalTok{(ames)}

\CommentTok{# attrition data}
\NormalTok{churn <-}\StringTok{ }\NormalTok{rsample}\OperatorTok{::}\NormalTok{attrition }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate_if}\NormalTok{(is.ordered, factor, }\DataTypeTok{ordered =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{churn.h2o <-}\StringTok{ }\KeywordTok{as.h2o}\NormalTok{(churn)}
\end{Highlighting}
\end{Shaded}

\hypertarget{splitting}{%
\section{Data splitting}\label{splitting}}

A major goal of the machine learning process is to find an algorithm \(f\left(X\right)\) that most accurately predicts future values (\(\hat{Y}\)) based on a set of features (\(X\)). In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the \textbf{\emph{generalizability}}\index{generalizability} of our algorithm. How we ``spend'' our data will help us understand how well our algorithm generalizes to unseen data.

To provide an accurate understanding of the generalizability of our final optimal model, we can split our data into training and test data sets:

\begin{itemize}
\tightlist
\item
  \textbf{Training set}: these data are used to develop feature sets, train our algorithms, tune hyperparameters, compare models, and all of the other activities required to choose a final model (e.g., the model we want to put into production).
\item
  \textbf{Test set}: having chosen a final model, these data are used to estimate an unbiased assessment of the model's performance, which we refer to as the \emph{generalization error}.
\end{itemize}

\begin{warning}
It is critical that the test set not be used prior to selecting your
final model. Assessing results on the test set prior to final model
selection biases the model selection process since the testing data will
have become part of the model development process.
\end{warning}

\begin{figure}

{\centering \includegraphics[width=3.12in]{images/data_split} 

}

\caption{Splitting data into training and test sets.}\label{fig:02-split}
\end{figure}

Given a fixed amount of data, typical recommendations for splitting your data into training-test splits include 60\% (training)--40\% (testing), 70\%--30\%, or 80\%--20\%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep the following points in mind:

\begin{itemize}
\tightlist
\item
  Spending too much in training (e.g., \(>80\%\)) won't allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (\emph{overfitting}).
\item
  Sometimes too much spent in testing (\(>40\%\)) won't allow us to get a good assessment of model parameters.
\end{itemize}

Other factors should also influence the allocation proportions. For example, very large training sets (e.g., \(n > 100\texttt{K}\)) often result in only marginal gains compared to smaller sample sizes. Consequently, you may use a smaller training sample to increase computation speed (e.g., models built on larger training sets often take longer to score new data sets in production). In contrast, as \(p \geq n\) (where \(p\) represents the number of features), larger samples sizes are often required to identify consistent signals in the features.

The two most common ways of splitting data include \textbf{\emph{simple random sampling}}\index{simple random sampling} and \textbf{\emph{stratified sampling}}\index{stratified sampling}.

\hypertarget{simple-random-sampling}{%
\subsection{Simple random sampling}\label{simple-random-sampling}}

The simplest way to split the data into training and test sets is to take a simple random sample. This does not control for any data attributes, such as the distribution your response variable (\(Y\)). There are multiple ways to split our data in R. Here we show four options to produce a 70--30 split in the Ames housing data:

\begin{note}
Sampling is a random process so setting the random number generator with
a common seed allows for reproducible results. Throughout this book
we'11 often use the seed \texttt{123} for reproducibility but the number
itself has no special meaning.
\end{note}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# base R}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{index_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(ames), }\KeywordTok{round}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(ames) }\OperatorTok{*}\StringTok{ }\FloatTok{0.7}\NormalTok{))}
\NormalTok{train_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\NormalTok{ames[index_}\DecValTok{1}\NormalTok{, ]}
\NormalTok{test_}\DecValTok{1}\NormalTok{  <-}\StringTok{ }\NormalTok{ames[}\OperatorTok{-}\NormalTok{index_}\DecValTok{1}\NormalTok{, ]}

\CommentTok{# caret package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{index_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Sale_Price, }\DataTypeTok{p =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{train_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\NormalTok{ames[index_}\DecValTok{2}\NormalTok{, ]}
\NormalTok{test_}\DecValTok{2}\NormalTok{  <-}\StringTok{ }\NormalTok{ames[}\OperatorTok{-}\NormalTok{index_}\DecValTok{2}\NormalTok{, ]}

\CommentTok{# rsample package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split_}\DecValTok{1}\NormalTok{  <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(ames, }\DataTypeTok{prop =} \FloatTok{0.7}\NormalTok{)}
\NormalTok{train_}\DecValTok{3}\NormalTok{  <-}\StringTok{ }\KeywordTok{training}\NormalTok{(split_}\DecValTok{1}\NormalTok{)}
\NormalTok{test_}\DecValTok{3}\NormalTok{   <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(split_}\DecValTok{1}\NormalTok{)}

\CommentTok{# h2o package}
\NormalTok{split_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{h2o.splitFrame}\NormalTok{(ames.h2o, }\DataTypeTok{ratios =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{seed =} \DecValTok{123}\NormalTok{)}
\NormalTok{train_}\DecValTok{4}\NormalTok{ <-}\StringTok{ }\NormalTok{split_}\DecValTok{2}\NormalTok{[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{test_}\DecValTok{4}\NormalTok{  <-}\StringTok{ }\NormalTok{split_}\DecValTok{2}\NormalTok{[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

With sufficient sample size, this sampling approach will typically result in a similar distribution of \(Y\) (e.g., \texttt{Sale\_Price} in the \texttt{ames} data) between your training and test sets, as illustrated below.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/distributions-1} 

}

\caption{Training (black) vs. test (red) response distribution.}\label{fig:distributions}
\end{figure}

\hypertarget{stratified}{%
\subsection{Stratified sampling}\label{stratified}}

If we want to explicitly control the sampling so that our training and test sets have similar \(Y\) distributions, we can use stratified sampling. This is more common with classification problems where the response variable may be severely imbalanced (e.g., 90\% of observations with response ``Yes'' and 10\% with response ``No''). However, we can also apply stratified sampling to regression problems for data sets that have a small sample size and where the response variable deviates strongly from normality (i.e., positively skewed like \texttt{Sale\_Price}). With a continuous response variable, stratified sampling will segment \(Y\) into quantiles and randomly sample from each. Consequently, this will help ensure a balanced representation of the response distribution in both the training and test sets.

The easiest way to perform stratified sampling on a response variable is to use the \textbf{rsample} package, where you specify the response variable to \texttt{strata}fy. The following illustrates that in our original employee attrition data we have an imbalanced response (No: 84\%, Yes: 16\%). By enforcing stratified sampling, both our training and testing sets have approximately equal response distributions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# orginal response distribution}
\KeywordTok{table}\NormalTok{(churn}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{prop.table}\NormalTok{()}
\CommentTok{## }
\CommentTok{##     No    Yes }
\CommentTok{## 0.8388 0.1612}

\CommentTok{# stratified sampling with the rsample package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split_strat  <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(churn, }\DataTypeTok{prop =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Attrition"}\NormalTok{)}
\NormalTok{train_strat  <-}\StringTok{ }\KeywordTok{training}\NormalTok{(split_strat)}
\NormalTok{test_strat   <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(split_strat)}

\CommentTok{# consistent response ratio between train & test}
\KeywordTok{table}\NormalTok{(train_strat}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{prop.table}\NormalTok{()}
\CommentTok{## }
\CommentTok{##     No    Yes }
\CommentTok{## 0.8388 0.1612}
\KeywordTok{table}\NormalTok{(test_strat}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{prop.table}\NormalTok{()}
\CommentTok{## }
\CommentTok{##     No    Yes }
\CommentTok{## 0.8386 0.1614}
\end{Highlighting}
\end{Shaded}

\hypertarget{class-imbalances}{%
\subsection{Class imbalances}\label{class-imbalances}}

Imbalanced data can have a significant impact on model predictions and performance \citep{apm}. Most often this involves classification problems where one class has a very small proportion of observations (e.g., defaults - 5\% versus nondefaults - 95\%). Several sampling methods have been developed to help remedy class imbalance and most of them can be categorized as either \emph{up-sampling}\index{up-sampling} or \emph{down-sampling}\index{down-sampling}.

Down-sampling balances the dataset by reducing the size of the abundant class(es) to match the frequencies in the least prevalent class. This method is used when the quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modeling. Furthermore, the reduced sample size reduces the computation burden imposed by further steps in the ML process.

On the contrary, up-sampling is used when the quantity of data is insufficient. It tries to balance the dataset by increasing the size of rarer samples. Rather than getting rid of abundant samples, new rare samples are generated by using repetition or bootstrapping (described further in Section \ref{bootstrapping}).

Note that there is no absolute advantage of one sampling method over another. Application of these two methods depends on the use case it applies to and the data set itself. A combination of over- and under-sampling is often successful and a common approach is known as Synthetic Minority Over-Sampling Technique, or SMOTE \citep{chawla2002smote}. This alternative sampling approach, as well as others, can be implemented in R (see the \texttt{sampling} argument in \texttt{?caret::trainControl()}). Furthermore, many ML algorithms implemented in R have class weighting schemes to remedy imbalances internally (e.g., most \textbf{h2o} algorithms have a \texttt{weights\_column} and \texttt{balance\_classes} argument).

\hypertarget{creating-models-in-r}{%
\section{Creating models in R}\label{creating-models-in-r}}

The R ecosystem provides a wide variety of ML algorithm implementations. This makes many powerful algorithms available at your fingertips. Moreover, there are almost always more than one package to perform each algorithm (e.g., there are over 20 packages for fitting random forests). There are pros and cons to this wide selection; some implementations may be more computationally efficient while others may be more flexible (i.e., have more hyperparameter tuning options). Future chapters will expose you to many of the packages and algorithms that perform and scale best to the kinds of tabular data and problems encountered by most organizations.

However, this also has resulted in some drawbacks as there are inconsistencies in how algorithms allow you to define the formula of interest and how the results and predictions are supplied. In addition to illustrating the more popular and powerful packages, we'll also show you how to use implementations that provide more consistency.

\hypertarget{many-formula-interfaces}{%
\subsection{Many formula interfaces}\label{many-formula-interfaces}}

To fit a model to our data, the model terms must be specified. Historically, there are two main interfaces for doing this. The formula interface using R formula rules to specify a symbolic representation of the terms. For example, \texttt{Y\ \textasciitilde{}\ X} where we say ``Y is a function of X''. To illustrate, suppose we have some generic modeling function called \texttt{model\_fn()} which accepts an R formula, as in the following examples:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sale price as a function of neighborhood and year sold}
\KeywordTok{model_fn}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Neighborhood }\OperatorTok{+}\StringTok{ }\NormalTok{Year_Sold, }\DataTypeTok{data =}\NormalTok{ ames)}

\CommentTok{# Variables + interactions}
\KeywordTok{model_fn}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Neighborhood }\OperatorTok{+}\StringTok{ }\NormalTok{Year_Sold }\OperatorTok{+}\StringTok{ }
\StringTok{           }\NormalTok{Neighborhood}\OperatorTok{:}\NormalTok{Year_Sold, }\DataTypeTok{data =}\NormalTok{ ames)}

\CommentTok{# Shorthand for all predictors}
\KeywordTok{model_fn}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames)}

\CommentTok{# Inline functions / transformations}
\KeywordTok{model_fn}\NormalTok{(}\KeywordTok{log10}\NormalTok{(Sale_Price) }\OperatorTok{~}\StringTok{ }\KeywordTok{ns}\NormalTok{(Longitude, }\DataTypeTok{df =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{           }\KeywordTok{ns}\NormalTok{(Latitude, }\DataTypeTok{df =} \DecValTok{3}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ ames)}
\end{Highlighting}
\end{Shaded}

This is very convenient but it has some disadvantages. For example:

\begin{itemize}
\tightlist
\item
  You can't nest in-line functions such as performing principal components analysis on the feature set prior to executing the model (\texttt{model\_fn(y\ \textasciitilde{}\ pca(scale(x1),\ scale(x2),\ scale(x3)),\ data\ =\ df)}).
\item
  All the model matrix calculations happen at once and can't be recycled when used in a model function.
\item
  For very wide data sets, the formula method can be extremely inefficient \citep{kuhnFormula}.
\item
  There are limited roles that variables can take which has led to several re-implementations of formulas.
\item
  Specifying multivariate outcomes is clunky and inelegant.
\item
  Not all modeling functions have a formula method (lack of consistency!).
\end{itemize}

Some modeling functions have a non-formula (XY) interface. These functions have separate arguments for the predictors and the outcome(s):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# use separate inputs for X and Y}
\NormalTok{features <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Year_Sold"}\NormalTok{, }\StringTok{"Longitude"}\NormalTok{, }\StringTok{"Latitude"}\NormalTok{)}
\KeywordTok{model_fn}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ ames[, features], }\DataTypeTok{y =}\NormalTok{ ames}\OperatorTok{$}\NormalTok{Sale_Price)}
\end{Highlighting}
\end{Shaded}

This provides more efficient calculations but can be inconvenient if you have transformations, factor variables, interactions, or any other operations to apply to the data prior to modeling.

Overall, it is difficult to determine if a package has one or both of these interfaces. For example, the \texttt{lm()} function, which performs linear regression, only has the formula method. Consequently, until you are familiar with a particular implementation you will need to continue referencing the corresponding help documentation.

A third interface, is to use \emph{variable name specification} where we provide all the data combined in one training frame but we specify the features and response with character strings. This is the interface used by the \textbf{h2o} package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{model_fn}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"Year_Sold"}\NormalTok{, }\StringTok{"Longitude"}\NormalTok{, }\StringTok{"Latitude"}\NormalTok{),}
  \DataTypeTok{y =} \StringTok{"Sale_Price"}\NormalTok{,}
  \DataTypeTok{data =}\NormalTok{ ames.h2o}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

One approach to get around these inconsistencies is to use a meta engine, which we discuss next.

\hypertarget{many-engines}{%
\subsection{Many engines}\label{many-engines}}

Although there are many individual ML packages available, there is also an abundance of meta engines that can be used to help provide consistency. For example, the following all produce the same linear regression model output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm_lm    <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames)}
\NormalTok{lm_glm   <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames, }\DataTypeTok{family =}\NormalTok{ gaussian)}
\NormalTok{lm_caret <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames, }\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{lm()} and \texttt{glm()} are two different algorithm engines that can be used to fit the linear model and \texttt{caret::train()} is a meta engine (aggregator) that allows you to apply almost any direct engine with \texttt{method\ =\ "\textless{}method-name\textgreater{}"}. There are trade-offs to consider when using direct versus meta engines. For example, using direct engines can allow for extreme flexibility but also requires you to familiarize yourself with the unique differences of each implementation. For example, the following highlights the various syntax nuances required to compute and extract predicted class probabilities across different direct engines.\footnote{This table was modified from \citet{kuhnMLtraining2019}}

\begin{longtable}[]{@{}lll@{}}
\caption{Table 1: Syntax for computing predicted class probabilities with direct engines.}\tabularnewline
\toprule
Algorithm & Package & Code\tabularnewline
\midrule
\endfirsthead
\toprule
Algorithm & Package & Code\tabularnewline
\midrule
\endhead
Linear discriminant analysis & \textbf{MASS} & \texttt{predict(obj)}\tabularnewline
Generalized linear model & \textbf{stats} & \texttt{predict(obj,\ type\ =\ "response")}\tabularnewline
Mixture discriminant analysis & \textbf{mda} & \texttt{predict(obj,\ type\ =\ "posterior")}\tabularnewline
Decision tree & \textbf{rpart} & \texttt{predict(obj,\ type\ =\ "prob")}\tabularnewline
Random Forest & \textbf{ranger} & \texttt{predict(obj)\$predictions}\tabularnewline
Gradient boosting machine & \textbf{gbm} & \texttt{predict(obj,\ type\ =\ "response",\ n.trees)}\tabularnewline
\bottomrule
\end{longtable}

Meta engines provide you with more consistency in how you specify inputs and extract outputs but can be less flexible than direct engines. Future chapters will illustrate both approaches. For meta engines, we'll focus on the \textbf{caret} package in the hardcopy of the book while also demonstrating the newer \textbf{parsnip} package in the additional online resources.\footnote{The \textbf{caret} package has been the preferred meta engine over the years; however, the author is now transitioning to fulltime development on \textbf{parsnip}, which is designed to be a more robust and tidy meta engine.}

\hypertarget{resampling}{%
\section{Resampling methods}\label{resampling}}

In section \ref{splitting} we split our data into training and testing sets. Furthermore, we were very explicit about the fact that we \textbf{\emph{do not}} use the test set to assess model performance during the training phase. So how do we assess the generalization performance of the model?

One option is to assess an error metric based on the training data. Unfortunately, this leads to biased results as some models can perform very well on the training data but not generalize well to a new data set (we'll illustrate this in Section \ref{bias-var}).

A second method is to use a \emph{validation}\index{validation} approach, which involves splitting the training set further to create two parts (as in Section \ref{splitting}): a training set and a validation set (or \emph{holdout set}). We can then train our model(s) on the new training set and estimate the performance on the validation set. Unfortunately, validation using a single holdout set can be highly variable and unreliable unless you are working with very large data sets \citep{molinaro2005prediction, hawkins2003assessing}. As the size of your data set reduces, this concern increases.

\begin{note}
Although we stick to our definitions of test, validation, and holdout
sets, these terms are sometimes used interchangeably in other literature
and software. What's important to remember is to always put a portion of
the data under lock and key until a final model has been selected (we
refer to this as the test data, but others refer to it as the holdout
set).
\end{note}

\textbf{Resampling methods}\index{resampling methods} provide an alternative approach by allowing us to repeatedly fit a model of interest to parts of the training data and testing the performance on other parts. The two most commonly used resampling methods include \emph{k-fold cross validation}\index{k-fold cross validation} and \emph{bootstrapping}\index{bootstrapping}.

\hypertarget{k-fold-cross-validation}{%
\subsection{\texorpdfstring{\emph{k}-fold cross validation}{k-fold cross validation}}\label{k-fold-cross-validation}}

\emph{k}-fold cross-validation (aka \emph{k}-fold CV) is a resampling method that randomly divides the training data into \emph{k} groups (aka folds) of approximately equal size. The model is fit on \(k-1\) folds and then the remaining fold is used to compute model performance. This procedure is repeated \emph{k} times; each time, a different fold is treated as the validation set. This process results in \emph{k} estimates of the generalization error (say \(\epsilon_1, \epsilon_2, \dots, \epsilon_k\)). Thus, the \emph{k}-fold CV estimate is computed by averaging the \emph{k} test errors, providing us with an approximation of the error we might expect on unseen data.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth,height=0.9\textheight]{images/cv} 

}

\caption{Illustration of the k-fold cross validation process.}\label{fig:02-cv}
\end{figure}

Consequently, with \emph{k}-fold CV, every observation in the training data will be held out one time to be included in the test set as illustrated in Figure \ref{fig:crossv}. In practice, one typically uses \(k = 5\) or \(k = 10\). There is no formal rule as to the size of \emph{k}; however, as \emph{k} gets larger, the difference between the estimated performance and the true performance to be seen on the test set will decrease. On the other hand, using too large of \emph{k} can introduce computational burdens. Moreover, \citet{molinaro2005prediction} found that \(k=10\) performed similarly to leave-one-out cross validation (LOOCV) which is the most extreme approach (i.e., setting \(k = n\)).

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/crossv-1} 

}

\caption{10-fold cross validation on 32 observations. Each observation is used once for validation and nine times for training.}\label{fig:crossv}
\end{figure}

Although using \(k \geq 10\) helps to minimize the variability in the estimated performance, \emph{k}-fold CV still tends to have higher variability than bootstrapping (discussed next). \citet{kim2009estimating} showed that repeating \emph{k}-fold CV can help to increase the precision of the estimated generalization error. Consequently, for smaller data sets (say \(n < 10,000\)), 10-fold CV repeated 5 or 10 times will improve the accuracy of your estimated performance and also provide an estimate of its variability.

Throughout this book we'll cover multiple ways to incorporate CV as you can often perform CV directly within certain ML functions:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# example in h2o}
\NormalTok{h2o.cv <-}\StringTok{ }\KeywordTok{h2o.glm}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ x, }
  \DataTypeTok{y =}\NormalTok{ y, }
  \DataTypeTok{training_frame =}\NormalTok{ ames.h2o,}
  \DataTypeTok{nfolds =} \DecValTok{10}  \CommentTok{# perform 10-fold CV}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Or externally as in the below chunk\footnote{\texttt{rsample::vfold\_cv()} results in a nested data frame where each element in \texttt{splits} is a list containing the training data frame and the observation IDs that will be used for training the model vs.~model validation.}. When applying it externally to an ML algorithm as below, we'll need a process to apply the ML model to each resample, which we'll also cover.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vfold_cv}\NormalTok{(ames, }\DataTypeTok{v =} \DecValTok{10}\NormalTok{)}
\CommentTok{## #  10-fold cross-validation }
\CommentTok{## # A tibble: 10 x 2}
\CommentTok{##    splits             id    }
\CommentTok{##    <list>             <chr> }
\CommentTok{##  1 <split [2.6K/293]> Fold01}
\CommentTok{##  2 <split [2.6K/293]> Fold02}
\CommentTok{##  3 <split [2.6K/293]> Fold03}
\CommentTok{##  4 <split [2.6K/293]> Fold04}
\CommentTok{##  5 <split [2.6K/293]> Fold05}
\CommentTok{##  6 <split [2.6K/293]> Fold06}
\CommentTok{##  7 <split [2.6K/293]> Fold07}
\CommentTok{##  8 <split [2.6K/293]> Fold08}
\CommentTok{##  9 <split [2.6K/293]> Fold09}
\CommentTok{## 10 <split [2.6K/293]> Fold10}
\end{Highlighting}
\end{Shaded}

\hypertarget{bootstrapping}{%
\subsection{Bootstrapping}\label{bootstrapping}}

A bootstrap sample is a random sample of the data taken \emph{with replacement} \citep{efron1986bootstrap}. This means that, after a data point is selected for inclusion in the subset, it's still available for further selection. A bootstrap sample is the same size as the original data set from which it was constructed. Figure \ref{fig:bootstrapscheme} provides a schematic of bootstrap sampling where each bootstrap sample contains 12 observations just as in the original data set. Furthermore, bootstrap sampling will contain approximately the same distribution of values (represented by colors) as the original data set.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth,height=0.7\textheight]{images/bootstrap-scheme} 

}

\caption{Illustration of the bootstrapping process.}\label{fig:bootstrapscheme}
\end{figure}

Since samples are drawn with replacement, each bootstrap sample is likely to contain duplicate values. In fact, on average, \(\approx 63.21\)\% of the original sample ends up in any particular bootstrap sample. The original observations not contained in a particular bootstrap sample are considered \emph{out-of-bag} (OOB). When bootstrapping, a model can be built on the selected samples and validated on the OOB samples; this is often done, for example, in random forests (\ref{random-forest}).

Since observations are replicated in bootstrapping, there tends to be less variability in the error measure compared with \emph{k}-fold CV \citep{efron1983estimating}. However, this can also increase the bias of your error estimate. This can be problematic with smaller data sets; however, for most average-to-large data sets (say \(n \geq 1,000\)) this concern is often negligable.

Figure \ref{fig:sampling-comparison} compares bootstrapping to 10-fold CV on a small data set with \(n = 32\) observations. A thorough introduction to the bootstrap and its use in R is provided in \citet{davison1997bootstrap}.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/sampling-comparison-1} 

}

\caption{Bootstrap sampling (left) versus 10-fold cross validation (right) on 32 observations. For bootstrap sampling, the observations that have zero replications (white) are the out-of-bag observations used for validation.}\label{fig:sampling-comparison}
\end{figure}

We can create bootstrap samples easily with \texttt{rsample::bootstraps()};

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{bootstraps}\NormalTok{(ames, }\DataTypeTok{times =} \DecValTok{10}\NormalTok{)}
\CommentTok{## # Bootstrap sampling }
\CommentTok{## # A tibble: 10 x 2}
\CommentTok{##    splits              id         }
\CommentTok{##    <list>              <chr>      }
\CommentTok{##  1 <split [2.9K/1.1K]> Bootstrap01}
\CommentTok{##  2 <split [2.9K/1.1K]> Bootstrap02}
\CommentTok{##  3 <split [2.9K/1.1K]> Bootstrap03}
\CommentTok{##  4 <split [2.9K/1K]>   Bootstrap04}
\CommentTok{##  5 <split [2.9K/1.1K]> Bootstrap05}
\CommentTok{##  6 <split [2.9K/1.1K]> Bootstrap06}
\CommentTok{##  7 <split [2.9K/1.1K]> Bootstrap07}
\CommentTok{##  8 <split [2.9K/1.1K]> Bootstrap08}
\CommentTok{##  9 <split [2.9K/1.1K]> Bootstrap09}
\CommentTok{## 10 <split [2.9K/1K]>   Bootstrap10}
\end{Highlighting}
\end{Shaded}

Bootstrapping is, typically, more of an internal resampling procedure that is naturally built into certain ML algorithms. This will become more apparent in the bagging and random forest chapters (\ref{bagging}-\ref{random-forest}).

\hypertarget{alternatives}{%
\subsection{Alternatives}\label{alternatives}}

Its important to note that there are other useful resampling procedures. If you're working with time-series specific data then you will want to incorporate rolling origin and other time series resampling procedures. \citet{hyndman2018forecasting} is the dominant, R-focused, time series resource\footnote{See their open source book at \url{https://www.otexts.org/fpp2}}.

Additionally, \citet{efron1983estimating} developed the ``632 method'' and \citet{efron1997improvements} discuss the ``632+ method''; both approaches seek to minimize biases experienced with bootstrapping on smaller data sets and are available via \textbf{caret} (see \texttt{?caret::trainControl} for details).

\hypertarget{bias-var}{%
\section{Bias variance trade-off}\label{bias-var}}

Prediction errors can be decomposed into two important subcomponents: error due to ``bias'' and error due to ``variance''. There is often a tradeoff between a model's ability to minimize bias and variance. Understanding how different sources of error lead to bias and variance helps us improve the data fitting process resulting in more accurate models.\index{bias variance trade-off}

\hypertarget{bias}{%
\subsection{Bias}\label{bias}}

\emph{Bias}\index{bias} is the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. It measures how far off in general a model's predictions are from the correct value, which provides a sense of how well a model can conform to the underlying structure of the data. Figure \ref{fig:bias-model} illustrates an example where the polynomial model does not capture the underlying structure well. Linear models are classical examples of high bias models as they are less flexible and rarely capture non-linear, non-monotonic relationships.

We also need to think of bias-variance in relation to resampling. Models with high bias are rarely effected by the noise introduced by resampling. If a model has high bias, it will have consistency in its resampling performance as illustrated by Figure \ref{fig:bias-model}.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/bias-model-1} 

}

\caption{A biased polynomial model fit to a single data set does not capture the underlying non-linear, non-monotonic data structure (left).  Models fit to 25 bootstrapped replicates of the data are underterred by the noise and generates similar, yet still biased, predictions (right).}\label{fig:bias-model}
\end{figure}

\hypertarget{variance}{%
\subsection{Variance}\label{variance}}

On the other hand, error due to \emph{variance}\index{variance} is defined as the variability of a model prediction for a given data point. Many models (e.g., \emph{k}-nearest neighbor, decision trees, gradient boosting machines) are very adaptable and offer extreme flexibility in the patterns that they can fit to. However, these models offer their own problems as they run the risk of overfitting to the training data. Although you may achieve very good performance on your training data, the model will not automatically generalize well to unseen data.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/variance-model-1} 

}

\caption{A high variance k-nearest neighbor model fit to a single data set captures the underlying non-linear, non-monotonic data structure well but also overfits to individual data points (left).  Models fit to 25 bootstrapped replicates of the data are deterred by the noise and generate highly variable predictions (right).}\label{fig:variance-model}
\end{figure}

Since high variance models are more prone to overfitting, using resampling procedures are critical to reduce this risk. Moreover, many algorithms that are capable of achieving high generalization performance have lots of \emph{hyperparameters}\index{hyperparameters} that control the level of model complexity (i.e., the tradeoff between bias and variance).

\hypertarget{tune-overfit}{%
\subsection{Hyperparameter tuning}\label{tune-overfit}}

Hyperparameters (aka \emph{tuning parameters}) are the ``knobs to twiddle''\footnote{This phrase comes from Brad Efron's comments in \citet{breiman2001statistical}} to control the complexity of machine learning algorithms and, therefore, the bias-variance trade-off. Not all algorithms have hyperparameters (e.g., ordinary least squares\footnote{At least in the ordinary sense. You could think of polynomial regression as having a single hyperparamer: the degree of the polynomial.}); however, most have at least one or more.

The proper setting of these hyperparameters are often dependent on the data and problem at hand and cannot always be estimated by the training data alone. Consequently, we need a method of identifying the optimal setting. For example, in the high variance example in the previous section, we illustrated a high variance \emph{k}-nearest neighbor model (we'll discuss \emph{k}-nearest neighbor in Chapter \ref{knn}). \emph{k}-nearest neighbor models have a single hyperparameter (\emph{k}) that determines the predicted value to be made based on the \emph{k} nearest observations in the training data to the one being predicted. If \emph{k} is small (e.g., \(k=3\)), the model will make a prediction for a given observation based on the average of the response values for the 3 observations in the training data most similar to the observation being predicted. This often results in highly variable predicted values because we are basing the prediction (in this case, an average) on a very small subset of the training data. As \emph{k} gets bigger, we base our predictions on an average of a larger subset of the training data, which naturally reduces the variance in our predicted values (remember this for later, averaging reduces variance!). Figure \ref{fig:knn-options} illustrates this point. Smaller \emph{k} values (e.g., 2, 5, or 10) lead to high variance (but lower bias) and larger values (e.g., 150) lead to high bias (but lower variance). The optimal \emph{k} value might exist somewhere between 20--50, but how do we know which value of \emph{k} to use?

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/knn-options-1} 

}

\caption{k-nearest neighbor model with differing values for k.}\label{fig:knn-options}
\end{figure}

One way to perform hyperparameter tuning is to fiddle with hyperparameters manually until you find a great combination of hyperparameter values that result in high predictive accuracy (as measured using \emph{k}-fold CV, for instance). However, this can be very tedious work depending on the number of hyperparameters. An alternative approach is to perform a \emph{grid search}\index{grid search}. A grid search is an automated approach to searching across many combinations of hyperparameter values.

For our \emph{k}-nearest neighbor example, a grid search would predefine a candidate set of values for \emph{k} (e.g., \(k = 1, 2, \dots, j\)) and perform a resampling method (e.g., \emph{k}-fold CV) to estimate which \emph{k} value generalizes the best to unseen data. Figure \ref{fig:knn-tune} illustrates the results from a grid search to assess \(k = 2, 12, 14, \dots, 150\) using repeated 10-fold CV. The error rate displayed represents the average error for each value of \emph{k} across all the repeated CV folds. On average, \(k=46\) was the optimal hyperparameter value to minimize error (in this case, RMSE which is discussed in Section \ref{model-eval})) on unseen data.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/knn-tune-1} 

}

\caption{Results from a grid search for a k-nearest neighbor model assessing values for k ranging from 2-150.  We see high error values due to high model variance when k is small and we also see high errors values due to high model bias when k is large.  The optimal model is found at k = 46.}\label{fig:knn-tune}
\end{figure}

Throughout this book you'll be exposed to different approaches to performing grid searches. In the above example, we used a \emph{full cartesian grid search}, which assesses every hyperparameter value manually defined. However, as models get more complex and offer more hyperparameters, this approach can become computationally burdensome and requires you to define the optimal hyperparameter grid settings to explore. Additional approaches we'll illustrate include \emph{random grid searches} \citep{bergstra2012random} which explores randomly selected hyperparameter values from a range of possible values, \emph{early stopping} which allows you to stop a grid search once reduction in the error stops marginally improving, \emph{adaptive resampling} via futility analysis \citep{kuhn2014futility} which adaptively resamples candidate hyperparameter values based on approximately optimal performance, and more.

\hypertarget{model-eval}{%
\section{Model evaluation}\label{model-eval}}

Historically, the performance of statistical models was largely based on goodness-of-fit tests and assessment of residuals. Unfortunately, misleading conclusions may follow from predictive models that pass these kinds of assessments \citep{breiman2001statistical}. Today, it has become widely accepted that a more sound approach to assessing model performance is to assess the predictive accuracy via \emph{loss functions}\index{loss functions}. Loss functions are metrics that compare the predicted values to the actual value (the outpuf of a loss function is often referred to as the \emph{error} or pseudo \emph{residual}). When performing resampling methods, we assess the predicted values for a validation set compared to the actual target value. For example, in regression, one way to measure error is to take the difference between the actual and predicted value for a given observation (this is the usual definition of a residual in ordinary linear regression). The overall validation error of the model is computed by aggregating the errors across the entire validation data set.

There are many loss functions to choose when assessing the performance of a predictive model; each providing a unique understanding of the predictive accuracy and differing between regression and classification models. Furthermore, the way a loss function is computed will tend to emphasize certain types of errors over others and can lead to drastic differences in how we interpret the ``optimal model''. Its important to consider the problem context when identifying the preferred performance metric to use. And when comparing multiple models, we need to compare them across the same metric.

\hypertarget{regression-models}{%
\subsection{Regression models}\label{regression-models}}

\begin{itemize}
\item
  \textbf{MSE}: Mean squared error\index{mean squared error} is the average of the squared error (\(MSE = \frac{1}{n} \sum^n_{i=1}(Y_i - \hat Y_i)^2\))\footnote{This deviates slightly from the usual definition of MSE in ordinary linear regression, where we divide by \(n-p\) (to adjust for bias) as opposed to \(n\).}. The squared component results in larger errors having larger penalties. This (along with RMSE) is the most common error metric to use. \textbf{Objective: minimize}
\item
  \textbf{RMSE}: Root mean squared error\index{root mean squared error}. This simply takes the square root of the MSE metric (\(RMSE = \sqrt{\frac{1}{n} \sum^n_{i=1}(y_i - \hat y_i)^2}\)) so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. \textbf{Objective: minimize}
\item
  \textbf{Deviance}: Short for mean residual deviance\index{deviance}. In essence, it provides a degree to which a model explains the variation in a set of data when using maximum likelihood estimation. Essentially this computes a saturated model (i.e.~fully featured model) to an unsaturated model (i.e.~intercept only or average). If the response variable distribution is Gaussian, then it will be approximately equal to MSE. When not, it usually gives a more useful estimate of error. Deviance is often used with classification models. \footnote{See this StackExchange thread (\url{http://bit.ly/what-is-deviance}) for a good overview of deviance for different models and in the context of regression versus classification.} \textbf{Objective: minimize}
\item
  \textbf{MAE}: Mean absolute error\index{mean absolute error}. Similar to MSE but rather than squaring, it just takes the mean absolute difference between the actual and predicted values (\(MAE = \frac{1}{n} \sum^n_{i=1}(\vert y_i - \hat y_i \vert)\)). This results in less emphasis on larger errors than MSE. \textbf{Objective: minimize}
\item
  \textbf{RMSLE}: Root mean squared logarithmic error\index{root mean squared logarithmic error}. Similiar to RMSE but it performs a \texttt{log()} on the actual and predicted values prior to computing the difference (\(RMSLE = \sqrt{\frac{1}{n} \sum^n_{i=1}(log(y_i + 1) - log(\hat y_i + 1))^2}\)). When your response variable has a wide range of values, large repsonse values with large errors can dominate the MSE/RMSE metric. RMSLE minimizes this impact so that small response values with large errors can have just as meaningful of an impact as large response values with large errors. \textbf{Objective: minimize}
\item
  \textbf{\(R^2\)}\index{R squared}: This is a popular metric that represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Unfortunately, it has several limitations. For example, two models built from two different data sets could have the exact same RMSE but if one has less variability in the response variable then it would have a lower \(R^2\) than the other. You should not place too much emphasis on this metric. \textbf{Objective: maximize}
\end{itemize}

Most models we assess in this book will report most, if not all, of these metrics. We will emphasize MSE and RMSE but its important to realize that certain situations warrant emphasis on some metrics more than others.

\hypertarget{classification-models}{%
\subsection{Classification models}\label{classification-models}}

\begin{itemize}
\item
  \textbf{Misclassification}\index{misclassification}: This is the overall error. For example, say you are predicting 3 classes ( \emph{high}, \emph{medium}, \emph{low} ) and each class has 25, 30, 35 observations respectively (90 observations total). If you misclassify 3 observations of class \emph{high}, 6 of class \emph{medium}, and 4 of class \emph{low}, then you misclassified 13 out of 90 observations resulting in a 14\% misclassification rate. \textbf{Objective: minimize}
\item
  \textbf{Mean per class error}\index{mean per class error}: This is the average error rate for each class. For the above example, this would be the mean of \(\frac{3}{25}, \frac{6}{30}, \frac{4}{35}\), which is 12\%. If your classes are balanced this will be identical to misclassification. \textbf{Objective: minimize}
\item
  \textbf{MSE}: Mean squared error. Computes the distance from 1.0 to the probability suggested. So, say we have three classes, A, B, and C, and your model predicts a probability of 0.91 for A, 0.07 for B, and 0.02 for C. If the correct answer was A the \(MSE = 0.09^2 = 0.0081\), if it is B \(MSE = 0.93^2 = 0.8649\), if it is C \(MSE = 0.98^2 = 0.9604\). The squared component results in large differences in probabilities for the true class having larger penalties. \textbf{Objective: minimize}
\item
  \textbf{Cross-entropy (aka Log Loss or Deviance)}\index{cross-entropy}: Similar to MSE but it incorporates a log of the predicted probability multiplied by the true class. Consequently, this metric disproportionately punishes predictions where we predict a small probability for the true class, which is another way of saying having high confidence in the wrong answer is really bad. \textbf{Objective: minimize}
\item
  \textbf{Gini index}\index{Gini index}: Mainly used with tree-based methods and commonly referred to as a measure of \emph{purity} where a small value indicates that a node contains predominantly observations from a single class. \textbf{Objective: minimize}
\end{itemize}

When applying classification models, we often use a \emph{confusion matrix}\index{confusion matrix} to evaluate certain performance measures. A confusion matrix is simply a matrix that compares actual categorical levels (or events) to the predicted categorical levels. When we predict the right level, we refer to this as a \emph{true positive}. However, if we predict a level or event that did not happen this is called a \emph{false positive} (i.e.~we predicted a customer would redeem a coupon and they did not). Alternatively, when we do not predict a level or event and it does happen that this is called a \emph{false negative} (i.e.~a customer that we did not predict to redeem a coupon does).

\begin{figure}

{\centering \includegraphics[width=1\linewidth,height=1\textheight]{images/confusion-matrix} 

}

\caption{Confusion matrix and relationships to terms such as true-positive and false-negative.}\label{fig:confusion-matrix}
\end{figure}

We can extract different levels of performance for binary classifiers. For example, given the classification (or confusion) matrix illustrated in Figure \ref{fig:confusion-matrix2} we can assess the following:

\begin{itemize}
\item
  \textbf{Accuracy}\index{accuracy}: Overall, how often is the classifier correct? Opposite of misclassification above. Example: \(\frac{TP + TN}{total} = \frac{100+50}{165} = 0.91\). \textbf{Objective: maximize}
\item
  \textbf{Precision}\index{precision}: How accurately does the classifier predict events? This metric is concerned with maximizing the true positives to false positive ratio. In other words, for the number of predictions that we made, how many were correct? Example: \(\frac{TP}{TP + FP} = \frac{100}{100+10} = 0.91\). \textbf{Objective: maximize}
\item
  \textbf{Sensitivity\index{sensitivity} (aka recall)}: How accurately does the classifier classify actual events? This metric is concerned with maximizing the true positives to false negatives ratio. In other words, for the events that occurred, how many did we predict? Example: \(\frac{TP}{TP + FN} = \frac{100}{100+5} = 0.95\). \textbf{Objective: maximize}
\item
  \textbf{Specificity}\index{specificity}: How accurately does the classifier classify actual non-events? Example: \(\frac{TN}{TN + FP} = \frac{50}{50+10} = 0.83\). \textbf{Objective: maximize}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth,height=0.5\textheight]{images/confusion-matrix2} 

}

\caption{Example confusion matrix.}\label{fig:confusion-matrix2}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{AUC}: Area under the curve\index{area under the curve}. A good binary classifier will have high precision and sensitivity. This means the classifier does well when it predicts an event will and will not occur, which minimizes false positives and false negatives. To capture this balance, we often use a ROC curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis. A line that is diagonal from the lower left corner to the upper right corner represents a random guess. The higher the line is in the upper left-hand corner, the better. AUC computes the area under this curve. \textbf{Objective: maximize}
\end{itemize}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/roc-1} 

}

\caption{ROC curve.}\label{fig:roc}
\end{figure}

\hypertarget{put-process-together}{%
\section{Putting the processes together}\label{put-process-together}}

To illustrate how this process works together via R code, let's do a simple assessment on the \texttt{ames} housing data. First, we perform stratified sampling as illustrated in Section \ref{stratified} to break our data into training vs.~test data while ensuring we have consistent distributions between the training and test sets.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# stratified sampling with the rsample package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split  <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(ames, }\DataTypeTok{prop =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Sale_Price"}\NormalTok{)}
\NormalTok{ames_train  <-}\StringTok{ }\KeywordTok{training}\NormalTok{(split)}
\NormalTok{ames_test   <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(split)}
\end{Highlighting}
\end{Shaded}

Next, we're going to apply a \emph{k}-nearest neighbor regressor to our data. To do so, we'll use \textbf{caret}, which is a meta-engine to simplify the resampling, grid search, and model application processes. The following defines:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Resampling method}: we use 10-fold CV repeated 5 times.
\item
  \textbf{Grid search}: we specify the hyperparameter values to assess (\(k = 2, 4, 6, \dots, 25\)).
\item
  \textbf{Model training \& Validation}: we train a \emph{k}-nearest neighbor (\texttt{method\ =\ "knn"}) model using our pre-specified resampling procedure (\texttt{trControl\ =\ cv}), grid search (\texttt{tuneGrid\ =\ hyper\_grid}), and preferred loss function (\texttt{metric\ =\ "RMSE"}).
\end{enumerate}

\begin{warning}
This grid search takes approximately 3.5 minutes
\end{warning}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a resampling method}
\NormalTok{cv <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}
  \DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
  \DataTypeTok{number =} \DecValTok{10}\NormalTok{, }
  \DataTypeTok{repeats =} \DecValTok{5}
\NormalTok{  )}

\CommentTok{# create a hyperparameter grid search}
\NormalTok{hyper_grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{k =} \KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DataTypeTok{by =} \DecValTok{1}\NormalTok{))}

\CommentTok{# fit knn model and perform grid search}
\NormalTok{knn_fit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ ames_train, }
  \DataTypeTok{method =} \StringTok{"knn"}\NormalTok{, }
  \DataTypeTok{trControl =}\NormalTok{ cv, }
  \DataTypeTok{tuneGrid =}\NormalTok{ hyper_grid,}
  \DataTypeTok{metric =} \StringTok{"RMSE"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Looking at our results we see that the best model coincided with \(k=\) 5, which resulted in an RMSE of 44738. This implies that, on average, our model mispredicts the expected sale price of a home by \$44,738. Figure \ref{fig:example-process-assess} illustrates the cross-validated error rate across the spectrum of hyperparameter values that we specified.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# print model results}
\NormalTok{knn_fit}
\CommentTok{## k-Nearest Neighbors }
\CommentTok{## }
\CommentTok{## 2054 samples}
\CommentTok{##   80 predictor}
\CommentTok{## }
\CommentTok{## No pre-processing}
\CommentTok{## Resampling: Cross-Validated (10 fold, repeated 5 times) }
\CommentTok{## Summary of sample sizes: 1849, 1848, 1848, 1849, 1849, 1847, ... }
\CommentTok{## Resampling results across tuning parameters:}
\CommentTok{## }
\CommentTok{##   k   RMSE   Rsquared  MAE  }
\CommentTok{##    2  47138  0.6592    30432}
\CommentTok{##    3  45374  0.6806    29403}
\CommentTok{##    4  45055  0.6847    29194}
\CommentTok{##    5  44738  0.6898    28966}
\CommentTok{##    6  44773  0.6908    28926}
\CommentTok{##    7  44816  0.6918    28970}
\CommentTok{##    8  44911  0.6921    29022}
\CommentTok{##    9  45012  0.6929    29047}
\CommentTok{##   10  45058  0.6945    28972}
\CommentTok{##   11  45057  0.6967    28908}
\CommentTok{##   12  45229  0.6962    28952}
\CommentTok{##   13  45339  0.6961    29031}
\CommentTok{##   14  45492  0.6958    29124}
\CommentTok{##   15  45584  0.6961    29188}
\CommentTok{##   16  45668  0.6964    29277}
\CommentTok{##   17  45822  0.6959    29410}
\CommentTok{##   18  46000  0.6943    29543}
\CommentTok{##   19  46206  0.6927    29722}
\CommentTok{##   20  46417  0.6911    29845}
\CommentTok{##   21  46612  0.6895    29955}
\CommentTok{##   22  46824  0.6877    30120}
\CommentTok{##   23  47009  0.6863    30257}
\CommentTok{##   24  47256  0.6837    30413}
\CommentTok{##   25  47454  0.6819    30555}
\CommentTok{## }
\CommentTok{## RMSE was used to select the optimal model using}
\CommentTok{##  the smallest value.}
\CommentTok{## The final value used for the model was k = 5.}

\CommentTok{# plot cross validation results}
\KeywordTok{ggplot}\NormalTok{(knn_fit)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/example-process-assess-1} 

}

\caption{Results from a grid search for a k-nearest neighbor model on the Ames housing data assessing values for k ranging from 2-25.}\label{fig:example-process-assess}
\end{figure}

The question remains: ``Is this the best predictive model we can find?'' We may have identified the optimal \emph{k}-nearest neighbor model for our given data set, but this doesn't mean we've found the best possible overall model. Nor have we considered potential feature and target engineering options. The remainder of this book will walk you through the journey of identifying alternative solutions and, hopefully, a much more optimal model.

\hypertarget{engineering}{%
\chapter{Feature \& Target Engineering}\label{engineering}}

Data pre-processing and engineering techniques generally refer to the addition, deletion, or transformation of data. The time spent on identifying data engineering needs can be significant and requires you to spend substantial time understanding your data\ldots{}or as Leo Breiman said ``live with your data before you plunge into modeling'' \citep[ p.~201]{breiman2001statistical}. Although this book primarily focuses on applying machine learning algorithms, feature engineering can make or break an algorithm's predictive ability and deserves your continued focus and education.

We will not cover all the potential ways of implementing feature engineering; however, we'll cover several fundamental pre-processing tasks that can potentially significantly improve modeling performance. Moreover, different models have different sensitivities to the type of target and feature values in the model and we will try to highlight some of these concerns. For more indepth coverage of feature engineering, please refer to \citet{kuhn2019feature} and \citet{zheng2018feature}.

\hypertarget{prerequisites-1}{%
\section{Prerequisites}\label{prerequisites-1}}

This chapter leverages the following packages:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Helper packages}
\KeywordTok{library}\NormalTok{(dplyr)    }\CommentTok{# for data manipulation}
\KeywordTok{library}\NormalTok{(ggplot2)  }\CommentTok{# for awesome graphics}
\KeywordTok{library}\NormalTok{(visdat)   }\CommentTok{# for additional visualizations}

\CommentTok{# Feature engineering packages}
\KeywordTok{library}\NormalTok{(caret)    }\CommentTok{# for various feature engineering tasks}
\KeywordTok{library}\NormalTok{(recipes)  }\CommentTok{# for various feature engineering tasks}
\end{Highlighting}
\end{Shaded}

We'll also continue working with the \texttt{ames\_train} data set created in Section \ref{put-process-together}:

\hypertarget{target-engineering}{%
\section{Target engineering}\label{target-engineering}}

Although not always a requirement, transforming the response variable can lead to predictive improvement, especially with parametric models (where require that certain assumptions about the model be met). For instance, ordinary linear regression models assume that the prediction errors (and hence the response) are normally distributed. This is usually fine, except when the prediction target has heavy tails (i.e., \emph{outliers}) or is skewed in one direction or the other. In these cases, the normality assumption likely does not hold. For example, as we saw in the data splitting section (\ref{splitting}), the response variable for the Ames housing data (\texttt{Sale\_Price}) is right (or positively) skewed as illustrated in Figure \ref{fig:distributions} (ranging from \$12,789 to \$755,000). A simple linear model, say \(\text{Sale\_Price}=\beta_{0} + \beta_{1} \text{Year\_Built} + \epsilon\), often assumes the error term \(\epsilon\) (and hence \texttt{Sale\_Price}) is normally distributed; fortunately, a simple log (or similar) transformation of the response can often help alleviate this concern as Figure \ref{fig:skewed-residuals} illustrates.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/skewed-residuals-1} 

}

\caption{Transforming the response variable to minimize skewness can resolve concerns with non-normally distributed errors.}\label{fig:skewed-residuals}
\end{figure}

Furthermore, using a log (or other) transformation to minimize the response skewness can be used for shaping the business problem as well. For example, in the House Prices: Advanced Regression Techniques Kaggle competition\footnote{\url{https://www.kaggle.com/c/house-prices-advanced-regression-techniques}}, which used the Ames housing data, the competition focused on using a log transformed Sale Price response because ``\ldots{}taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.'' This would be an alternative to using the root mean squared logarithmic error (RMSLE) loss function as discussed in Section \ref{model-eval}.

There are two main approaches to help correct for postively skewed target variables:

\textbf{Option 1}: normalize with a log transformation\index{log transformation}. This will transform most right skewed distributions to be approximately normal. One way to do this is to simply log transform the training and test set in a manual, single step manner similar to:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{transformed_response <-}\StringTok{ }\KeywordTok{log}\NormalTok{(ames_train}\OperatorTok{$}\NormalTok{Sale_Price)}
\end{Highlighting}
\end{Shaded}

However, we should think of the pre-processing as creating a blueprint to be re-applied strategically. For this, you can use the \textbf{recipe} package or something similar (e.g., \texttt{caret::preProcess()}). This will not return the actual log transformed values but, rather, a blueprint to be applied later.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# log transformation}
\NormalTok{ames_recipe <-}\StringTok{ }\KeywordTok{recipe}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_log}\NormalTok{(}\KeywordTok{all_outcomes}\NormalTok{())}

\NormalTok{ames_recipe}
\CommentTok{## Data Recipe}
\CommentTok{## }
\CommentTok{## Inputs:}
\CommentTok{## }
\CommentTok{##       role #variables}
\CommentTok{##    outcome          1}
\CommentTok{##  predictor         80}
\CommentTok{## }
\CommentTok{## Operations:}
\CommentTok{## }
\CommentTok{## Log transformation on all_outcomes()}
\end{Highlighting}
\end{Shaded}

If your reponse has negative values or zeros then a log transformation will produce \texttt{NaN}s and \texttt{-Inf}s, respectively (you cannot take the logarithm of a negative number). If the nonpositive response values values are small (say between -0.99 and 0) then you can apply a small offset such as in \texttt{log1p()} which adds 1 to the value prior to applying a log transformation (you can do the same within \texttt{step\_log()} by using the \texttt{offset} argument). If your data consists of values \(\le -1\), use the Yeo-Johnson transformation mentioned next.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{log}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5}\NormalTok{)}
\CommentTok{## [1] NaN}
\KeywordTok{log1p}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5}\NormalTok{)}
\CommentTok{## [1] -0.6931}
\end{Highlighting}
\end{Shaded}

\textbf{Option 2}: use a \emph{Box Cox transformation}\index{Box Cox transformation}. A Box Cox transformation is more flexible than (but also includes as a special case) the log transformation and will find an appropriate transformation from a family of power transforms that will transform the variable as close as possible to a normal distribution \citep{box1964analysis, carroll1981prediction}. At the core of the Box Cox transformation is an exponent, lambda (\(\lambda\)), which varies from -5 to 5. All values of \(\lambda\) are considered and the optimal value for the given data is estimated from the training data; The ``optimal value'' is the one which results in the best transofrmation to an approximate normal distribution. The transformation of the response \(Y\) has the form:

\begin{equation} 
 y(\lambda) =
\begin{cases}
   \frac{Y^\lambda-1}{\lambda}, & \text{if}\ \lambda \neq 0 \\
   \log\left(Y\right), & \text{if}\ \lambda = 0.
\end{cases}
\end{equation}

\begin{warning}
Be sure to compute the \texttt{lambda} on the training set and apply
that same \texttt{lambda} to both the training and test set to minimize
\emph{data leakage}. The \textbf{recipes} package automates this process
for you.

If your response has negative values, the Yeo-Johnson transformation is
very similar to the Box-Cox but does not require the input variables to
be strictly positive. To apply, use \texttt{step\_YeoJohnson()}.
\end{warning}

Figure \ref{fig:distribution-comparison} illustrates that the log transformation and Box Cox transformation both do about equally well in transforming \texttt{Sale\_Price} to look more normally distributed.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/distribution-comparison-1} 

}

\caption{Response variable transformations.}\label{fig:distribution-comparison}
\end{figure}

Note that when you model with a transformed response variable, your predictions will also be on the transformed scale. You will likely want to re-transform your predicted values back to their normal scale so that decision-makers can more easily interpret the results. This is illustrated in the following code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# log transform a value}
\NormalTok{y <-}\StringTok{ }\KeywordTok{log}\NormalTok{(}\DecValTok{10}\NormalTok{)}

\CommentTok{# re-transforming the log-transformed value}
\KeywordTok{exp}\NormalTok{(y)}
\CommentTok{## [1] 10}

\CommentTok{# Box Cox transform a value}
\NormalTok{y <-}\StringTok{ }\NormalTok{forecast}\OperatorTok{::}\KeywordTok{BoxCox}\NormalTok{(}\DecValTok{10}\NormalTok{, lambda)}

\CommentTok{# Inverse Box Cox function}
\NormalTok{inv_box_cox <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, lambda) \{}
  \CommentTok{# for Box-Cox, lambda = 0 is equivalent to log transform}
  \ControlFlowTok{if}\NormalTok{ (lambda }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\KeywordTok{exp}\NormalTok{(x) }\ControlFlowTok{else}\NormalTok{ (lambda}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}\OperatorTok{^}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{lambda) }
\NormalTok{\}}

\CommentTok{# re-transforming the Box Cox-transformed value}
\KeywordTok{inv_box_cox}\NormalTok{(y, lambda)}
\CommentTok{## [1] 10}
\CommentTok{## attr(,"lambda")}
\CommentTok{## [1] 0.0526}
\end{Highlighting}
\end{Shaded}

\hypertarget{dealing-with-missingness}{%
\section{Dealing with missingness}\label{dealing-with-missingness}}

Data quality is an important issue for any project involving analyzing data. Data quality issues deserve an entire book in their own right, and a good reference is the The Quartz guide to bad data.\footnote{\url{https://github.com/Quartz/bad-data-guide}} One of the most common data quality concerns you will run into is missing values.

Data can be missing for many different reasons; however, these reasons are usually lumped into two categories: \emph{informative missingess}\index{informative missingess} \citep{apm} and \emph{missingness at random}\index{missingness at random} \citep{little2014statistical}. Informative missinginess implies a structural cause for the missing value that can provide insight in its own right; whether this be definciencies in how the data was collected or abnormalities in the oberservational environment. Missingness at random implies that missing values occur independent of the data collection process\footnote{\citet{little2014statistical} discuss two different kinds of missingness at random; however, we combine them for simplicity as their nuanced differences are distinguished between the two in practice.}.

The category that drives missing values will determine how you handle them. For example, we may give values that are driven by informative missingness their own category (e.g., \texttt{"None"}) as their unique value may affect predictive performance. Whereas values that are missing at random may deserve deletion\footnote{If your data set is large, deleting missing observations that have missing values at random rarely impacts predictive performance. However, as your data sets get smaller, preserving observations is critical and alternative solutions should be explored.} or imputation.

Furthermore, different machine learning models handle missingness differently. Most algorithms cannot handle missingness (e.g., generalized linear models and their cousins, neural networks, and support vector machines) and, therefore, require them to be dealt with before hand. A few models (mainly tree-based), have built-in procedures to deal with missing values. However, since the modeling process involves comparing and contrasting multiple models to identify the optimal one, you will want to handle missing values prior to applying any models so that your algorithms are based on the same data quality assumptions.

\hypertarget{visualizing-missing-values}{%
\subsection{Visualizing missing values}\label{visualizing-missing-values}}

It is important to understand the distribution of missing values (i.e., \texttt{NA}) in any data set. So far, we have been using a pre-processed version of the Ames housing data set (via the \texttt{AmesHousing::make\_ames()} function). However, if we use the raw Ames housing data (via \texttt{AmesHousing::ames\_raw}), there are actually 13,997 missing values---there is at least one missing values in each row of the original data!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(AmesHousing}\OperatorTok{::}\NormalTok{ames_raw))}
\CommentTok{## [1] 13997}
\end{Highlighting}
\end{Shaded}

It is important to understand the distribution of missing values in a data set in order to determine the best approach for pre-processing. Heat maps are an efficient way to visualize the distribution of missing values for small- to medium-sized data sets. The code \texttt{is.na(\textless{}data-frame-name\textgreater{})} will return a matrix of the same dimension as the given data frame, but each cell will contain either \texttt{TRUE} (if the corresponding value is missing) or \texttt{FALSE} (if the corresponding value is not missing). To construct such a plot, we can use R's built-in \texttt{heatmap()} or \texttt{image()} functions, or \textbf{ggplot2}'s \texttt{geom\_raster()} function, among others; Figure \ref{fig:heat-map-missingness} illustrates \texttt{geom\_raster()}. This allows us to easily see where the majority of missing values occur (i.e., in the variables \texttt{Alley}, \texttt{Fireplace\ Qual}, \texttt{Pool\ QC}, \texttt{Fence}, and \texttt{Misc\ Feature}). Due to their high frequency of missingness, these variables would likely need to be removed prior to statiscial analysis, or imputed. We can also spot obvious patterns of missingness. For example, missing values appear to occur within the same observations across all garage variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AmesHousing}\OperatorTok{::}\NormalTok{ames_raw }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{is.na}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\NormalTok{reshape2}\OperatorTok{::}\KeywordTok{melt}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Var2, Var1, }\DataTypeTok{fill=}\NormalTok{value)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_raster}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_y_continuous}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{expand =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_grey}\NormalTok{(}\DataTypeTok{name =} \StringTok{""}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Present"}\NormalTok{, }\StringTok{"Missing"}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Observation"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.y  =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{bookdown_files/figure-latex/heat-map-missingness-1} 

}

\caption{Heat map of missing values in the raw Ames housing data.}\label{fig:heat-map-missingness}
\end{figure}

Digging a little deeper into these variables, we might notice that \texttt{Garage\_Cars} and \texttt{Garage\_Area} contain the value \texttt{0} whenever the other \texttt{Garage\_xx} variables have missing values (i.e.~a value of \texttt{NA}). This might be because they did not have a way to identify houses with no garages when the data were originally collected, and therefore, all houses with no garage were identified by including nothing. Since this missingness is informative, it would be appropriate to impute \texttt{NA} with a new category level (e.g., \texttt{"None"}) for these garage variables. Circumstances like this tend to only become apparent upon careful descriptive and visual examination of the data!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AmesHousing}\OperatorTok{::}\NormalTok{ames_raw }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(}\StringTok{`}\DataTypeTok{Garage Type}\StringTok{`}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\StringTok{`}\DataTypeTok{Garage Type}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{Garage Cars}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{Garage Area}\StringTok{`}\NormalTok{)}
\CommentTok{## # A tibble: 157 x 3}
\CommentTok{##    `Garage Type` `Garage Cars` `Garage Area`}
\CommentTok{##    <chr>                 <int>         <int>}
\CommentTok{##  1 <NA>                      0             0}
\CommentTok{##  2 <NA>                      0             0}
\CommentTok{##  3 <NA>                      0             0}
\CommentTok{##  4 <NA>                      0             0}
\CommentTok{##  5 <NA>                      0             0}
\CommentTok{##  6 <NA>                      0             0}
\CommentTok{##  7 <NA>                      0             0}
\CommentTok{##  8 <NA>                      0             0}
\CommentTok{##  9 <NA>                      0             0}
\CommentTok{## 10 <NA>                      0             0}
\CommentTok{## # ... with 147 more rows}
\end{Highlighting}
\end{Shaded}

The \texttt{vis\_miss()} function in R package \texttt{visdat} \citep{R-visdat} also allows for easy visualization of missing data patterns (with sorting and clustering options). We illustrate this functionality below using the raw Ames housing data (Figure \ref{fig:missingness-visna}). The columns of the heat map represent the 82 variables of the raw data and the rows represent the observations. Missing values (i.e., \texttt{NA}) are inidcated via a black cell. The variables and \texttt{NA} patterns have been clustered by rows (i.e., \texttt{cluster\ =\ TRUE}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vis_miss}\NormalTok{(AmesHousing}\OperatorTok{::}\NormalTok{ames_raw, }\DataTypeTok{cluster =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/missingness-visna-1} 

}

\caption{Visualizing missing patterns in the raw Ames housing data.}\label{fig:missingness-visna}
\end{figure}

Data can be missing for different reasons. Perhaps the values were never recoded (or lost in translation), or it was recorded in error (a common feature of data enetered by hand). Regardless, it is important to identify and attempt to understand how missing values are distributed across a data set as it can provide insight into how to deal with these observations.

\hypertarget{impute}{%
\subsection{Imputation}\label{impute}}

\emph{Imputation}\index{imputation} is the process of replacing a missing value with a substituted, ``best guess'' value. Imputation should be one of the first feature engineering steps yo take as it will effect any downstream pre-processing\footnote{For example, standardizing numeric features will include the imputed numeric values in the calculation and one-hot encoding will include the imputated categorical value.}.

\hypertarget{estimated-statistic}{%
\subsubsection{Estimated statistic}\label{estimated-statistic}}

An elementary approach to imputing missing values for a feature is to compute descriptive statistics such as the mean, median, or mode (for categorical) and use that value to replace \texttt{NA}s. Although computationally efficient, this approach does not consider any other attributes for a given observation when imputing (e.g., a female patient that is 63 inches tall may have her weight imputed as 175 lbs since that is the average weight across all observations which contains 65\% males that average a height of 70 inches).

An alternative is to try use grouped statistics to capture expected values for observations that fall into similar groups. However, this becomes infeasable for larger data sets. Modeling imputation can automate this process for you and the two most common methods include K-nearest neighbor and tree-based imputation, which are discussed next.

However, it is important to remember that imputation should be performed \textbf{within the resampling process} and as your data set gets larger, repeated model-based imputation can compound the computational demands. Thus, you must weigh the pros and cons of the two approaches. The following would build onto our \texttt{ames\_recipe} and impute all missing values for the \texttt{Gr\_Liv\_Area} variable with the median value:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames_recipe }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_medianimpute}\NormalTok{(Gr_Liv_Area)}
\CommentTok{## Data Recipe}
\CommentTok{## }
\CommentTok{## Inputs:}
\CommentTok{## }
\CommentTok{##       role #variables}
\CommentTok{##    outcome          1}
\CommentTok{##  predictor         80}
\CommentTok{## }
\CommentTok{## Operations:}
\CommentTok{## }
\CommentTok{## Box-Cox transformation on all_outcomes()}
\CommentTok{## Median Imputation for Gr_Liv_Area}
\end{Highlighting}
\end{Shaded}

\begin{tip}
Use \texttt{step\_modeimpute()} to impute categorical features with the
most common value.
\end{tip}

\hypertarget{k-nearest-neighbor}{%
\subsubsection{\texorpdfstring{\emph{K}-nearest neighbor}{K-nearest neighbor}}\label{k-nearest-neighbor}}

\emph{K}-nearest neighbor (KNN) imputes values by identifying observations with missing values, then identifying other observations that are most similar based on the other available features, and using the values from these nearest neighbor observations to impute missing values.

We discuss KNN for predictive modeling in Chapter \ref{knn}; the imputation application works in a similar manner. In KNN imputation, the missing value for a given observation is treated as the targeted response and is predicted based on the average (for quantitative values) or the mode (for qualitative values) of the \emph{k} nearest neighbors.

As discussed in Chapter \ref{knn}, if all features are quantitative then standard Euclidean distance is commonly used as the distance metric to identify the \emph{k} neighbors and when there is a mixture of quantitative and qualitative features then Gower's distance \citep{gower1971general} can be used. KNN imputation is best used on small to moderate sized data sets as it becomes computationally burdomesome with larger data sets \citep{kuhn2019feature}.

\begin{note}
As we saw in Section 2.7, \emph{k} is a tunable hyperparameter.
Suggested values for imputation are 5--10 {[}@kuhn2019feature{]}. By
default, \texttt{step\_knnimpute()} will use 5 but can be adjusted with
the \texttt{neighbors} argument.
\end{note}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames_recipe }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_knnimpute}\NormalTok{(}\KeywordTok{all_predictors}\NormalTok{(), }\DataTypeTok{neighbors =} \DecValTok{6}\NormalTok{)}
\CommentTok{## Data Recipe}
\CommentTok{## }
\CommentTok{## Inputs:}
\CommentTok{## }
\CommentTok{##       role #variables}
\CommentTok{##    outcome          1}
\CommentTok{##  predictor         80}
\CommentTok{## }
\CommentTok{## Operations:}
\CommentTok{## }
\CommentTok{## Box-Cox transformation on all_outcomes()}
\CommentTok{## 6-nearest neighbor imputation for all_predictors()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tree-based}{%
\subsubsection{Tree-based}\label{tree-based}}

As previously discussed, several implementations of decision trees (Chapter \ref{DT}) and their derivatives can be constructed in the presence of missing values. Thus, they provide a good alternative for imputation. As discussed in Chapters \ref{DT}-\ref{random-forest}, single trees have high variance but aggregating across many trees creates a robust, low variance predictor. Random forest imputation procedures have been studied \citep{shah2014comparison, stekhoven2015missforest}; however, they require significant computational demands in a resampling environment \citep{kuhn2019feature}. Bagged trees (Chapter \ref{bagging}) offer a compromise between predictive accuracy and computational burden.

Similar to KNN imputation, observations with missing values are identified and the feature containing the missing value is treated as the target and predicted using bagged decision trees.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames_recipe }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_bagimpute}\NormalTok{(}\KeywordTok{all_predictors}\NormalTok{())}
\CommentTok{## Data Recipe}
\CommentTok{## }
\CommentTok{## Inputs:}
\CommentTok{## }
\CommentTok{##       role #variables}
\CommentTok{##    outcome          1}
\CommentTok{##  predictor         80}
\CommentTok{## }
\CommentTok{## Operations:}
\CommentTok{## }
\CommentTok{## Box-Cox transformation on all_outcomes()}
\CommentTok{## Bagged tree imputation for all_predictors()}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:imputation-examples} illustrates the differences between mean, KNN, and tree-based imputation on the raw Ames housing data. It is apparent how descriptive statistic methods (e.g., using the mean and median) are inferior to the KNN and tree-based imputation methods.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/imputation-examples-1} 

}

\caption{Comparison of three different imputation methods. The red points represent actual values which were removed and made missing and the blue points represent the imputed values. Estimated statistic imputation methods (i.e. mean, median) merely predict the same value for each observation and can reduce the signal between a feature and the response; whereas KNN and tree-based procedures tend to maintain the feature distribution and relationship.}\label{fig:imputation-examples}
\end{figure}

\hypertarget{feature-filtering}{%
\section{Feature filtering}\label{feature-filtering}}

In many data analyses and modeling projects we end up with hundreds or even thousands of collected features. From a practical perspective, a model with more features often becomes harder to interpret and is costly to compute. Some models are more resistant to non-informative predictors (e.g., the Lasso and tree-based methods) than others as illustrated in Figure \ref{fig:accuracy-comparison}.\footnote{See \citet{apm} section 19.1 for data set generation.}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/accuracy-comparison-1} 

}

\caption{Test set RMSE profiles when non-informative predictors are added.}\label{fig:accuracy-comparison}
\end{figure}

Although the performance of some of our models are not significantly affected by non-informative predictors, the time to train these models can be negatively impacted as more features are added. Figure \ref{fig:impact-on-time} shows the increase in time to perform 10-fold CV on the exemplar data, which consists of 10,000 observations. We see that many algorithms (e.g., elastic nets, random forests, and gradient boosting machines) become extremely time intensive the more predictors we add. Consequently, filtering or reducing features prior to modeling may significantly speed up training time.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/impact-on-time-1} 

}

\caption{Impact in model training time as non-informative predictors are added.}\label{fig:impact-on-time}
\end{figure}

Zero and near-zero variance variables are low-hanging fruit to eliminate. Zero variance variables\index{zero variance}, meaning the feature only contains a single uniquen value, provides no useful information to a model. Some algorithms are unaffected by zero variance features. However, features that have near-zero variance\index{near-zero variance} also offer very little, if any, information to a model. Furthermore, they can cause problems during resampling as there is a high probability that a given sample will only contain a single unique value (the dominant value) for that feature. A rule of thumb for detecting near-zero variance features is:

\begin{itemize}
\tightlist
\item
  The fraction of unique values over the sample size is low (say \$ \leq 10\$\%).
\item
  The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (say \$ \geq 20\$\%).
\end{itemize}

If both of these criteria are true then it is often adventageous to remove the variable from the model. For the Ames data, we do not have any zero variance predictors but there are 20 features that meet the near-zero threshold.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{caret}\OperatorTok{::}\KeywordTok{nearZeroVar}\NormalTok{(ames_train, }\DataTypeTok{saveMetrics=} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{rownames_to_column}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(nzv)}
\CommentTok{##               rowname freqRatio percentUnique zeroVar}
\CommentTok{## 1              Street    255.75       0.09737   FALSE}
\CommentTok{## 2               Alley     24.69       0.14606   FALSE}
\CommentTok{## 3        Land_Contour     21.88       0.19474   FALSE}
\CommentTok{## 4           Utilities   2052.00       0.14606   FALSE}
\CommentTok{## 5          Land_Slope     21.68       0.14606   FALSE}
\CommentTok{## 6         Condition_2    184.73       0.34080   FALSE}
\CommentTok{## 7           Roof_Matl    112.50       0.29211   FALSE}
\CommentTok{## 8           Bsmt_Cond     24.03       0.29211   FALSE}
\CommentTok{## 9      BsmtFin_Type_2     23.13       0.34080   FALSE}
\CommentTok{## 10            Heating     91.82       0.29211   FALSE}
\CommentTok{## 11    Low_Qual_Fin_SF    674.33       1.41188   FALSE}
\CommentTok{## 12      Kitchen_AbvGr     22.85       0.19474   FALSE}
\CommentTok{## 13         Functional     35.30       0.38948   FALSE}
\CommentTok{## 14     Enclosed_Porch    109.06       7.20545   FALSE}
\CommentTok{## 15 Three_season_porch    674.67       1.26582   FALSE}
\CommentTok{## 16       Screen_Porch    186.90       4.81986   FALSE}
\CommentTok{## 17          Pool_Area   2046.00       0.43817   FALSE}
\CommentTok{## 18            Pool_QC    682.00       0.24343   FALSE}
\CommentTok{## 19       Misc_Feature     29.58       0.29211   FALSE}
\CommentTok{## 20           Misc_Val    124.00       1.26582   FALSE}
\CommentTok{##     nzv}
\CommentTok{## 1  TRUE}
\CommentTok{## 2  TRUE}
\CommentTok{## 3  TRUE}
\CommentTok{## 4  TRUE}
\CommentTok{## 5  TRUE}
\CommentTok{## 6  TRUE}
\CommentTok{## 7  TRUE}
\CommentTok{## 8  TRUE}
\CommentTok{## 9  TRUE}
\CommentTok{## 10 TRUE}
\CommentTok{## 11 TRUE}
\CommentTok{## 12 TRUE}
\CommentTok{## 13 TRUE}
\CommentTok{## 14 TRUE}
\CommentTok{## 15 TRUE}
\CommentTok{## 16 TRUE}
\CommentTok{## 17 TRUE}
\CommentTok{## 18 TRUE}
\CommentTok{## 19 TRUE}
\CommentTok{## 20 TRUE}
\end{Highlighting}
\end{Shaded}

\begin{note}
We can add \texttt{step\_zv()} and \texttt{step\_nzv()} to our
\texttt{ames\_recipe} to remove zero or near-zero variance features.
\end{note}

Other feature filtering methods exist; see \citet{saeys2007review} for a thorough review. Furthermore, several wrapper methods exist that evaluate multiple models using procedures that add or remove predictors to find the optimal combination of features that maximizes model performance (see, for example, \citet{kursa2010feature}, \citet{granitto2006recursive}, \citet{maldonado2009wrapper}). However, this topic is beyond the scope of this book.

\hypertarget{numeric-feature-engineering}{%
\section{Numeric feature engineering}\label{numeric-feature-engineering}}

Numeric features can create a host of problems for certain models when their distributions are skewed, contain outliers, or have a wide range in magnitudes. Tree-based models are quite immune to these types of problems in the feature space, but many other models (e.g., GLMs, regularized regression, KNN, support vector machines, neural networks) can be greatly hampered by these issues. Normalizing and standardizing heavily skewed features can help minimize these concerns.

\hypertarget{skewness}{%
\subsection{Skewness}\label{skewness}}

Similar to the process discussed to normalize target variables, parametric models that have distributional assumptions (e.g., GLMs, and regularized models) can benefit from minimizing the skewness of numeric features. When normalizing many variables, its best to use the Box-Cox (when feature values are strictly positive) or Yeo-Johnson (when feature values are not strictly positive) procedures as these methods will identify if a transformation is required and what the optimal transformation will be.

\begin{note}
Non-parametric models are rarely affected by skewed features; however,
normalizing features will not have a negative affect on these models'
performance. For example, normalizing features will only shift the
optimal split points in tree-based algorithms. Consequently, when in
doubt, normalize.
\end{note}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# we can normalize all numeric features, including the response }
\CommentTok{# at the same time}
\KeywordTok{recipe}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_YeoJohnson}\NormalTok{(}\KeywordTok{all_numeric}\NormalTok{())                 }
\CommentTok{## Data Recipe}
\CommentTok{## }
\CommentTok{## Inputs:}
\CommentTok{## }
\CommentTok{##       role #variables}
\CommentTok{##    outcome          1}
\CommentTok{##  predictor         80}
\CommentTok{## }
\CommentTok{## Operations:}
\CommentTok{## }
\CommentTok{## Yeo-Johnson transformation on all_numeric()}
\end{Highlighting}
\end{Shaded}

\hypertarget{standardization}{%
\subsection{Standardization}\label{standardization}}

We must also consider the scale on which the individual features are measured. What are the largest and smallest values across all features and do they span several orders of magnitude? Models that incorporate smooth functions of input features are sensitive to the scale of the inputs. For example, \(5X+2\) is a simple linear function of the input \emph{X}, and the scale of its output depends directly on the scale of the input. Many algorithms use linear functions within their algorithms, some more obvious (e.g., GLMs and regularized regression) than others (e.g., neural networks, support vector machines, and principal components analysis). Other examples include algorithms that use distance measures such as the Euclidean distance (e.g., \emph{k} nearest neighbor, \emph{k}-means clustering, and hierarchical clustering).

For these models and modeling components, it is often a good idea to \emph{standardize}\index{standardize} the features. Standardizing features includes \emph{centering} and \emph{scaling} so that numeric variables have zero mean and unit variance, which provides a common comparable unit of measure across all the variables.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/standardizing-1} 

}

\caption{Standardizing features allows all features to be compared on a common value scale regardless of their real value differences.}\label{fig:standardizing}
\end{figure}

Some packages (e.g., \textbf{glmnet}, and \textbf{caret}) have built-in options to standardize and some do not (e.g., \textbf{keras} for neural networks). However, you should standardize your variables within the recipe blueprint so that both training and test data standardization are based on the same mean and variance. This helps to minimize data leakage.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames_recipe }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_center}\NormalTok{(}\KeywordTok{all_numeric}\NormalTok{(), }\OperatorTok{-}\KeywordTok{all_outcomes}\NormalTok{()) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_scale}\NormalTok{(}\KeywordTok{all_numeric}\NormalTok{(), }\OperatorTok{-}\KeywordTok{all_outcomes}\NormalTok{())}
\CommentTok{## Data Recipe}
\CommentTok{## }
\CommentTok{## Inputs:}
\CommentTok{## }
\CommentTok{##       role #variables}
\CommentTok{##    outcome          1}
\CommentTok{##  predictor         80}
\CommentTok{## }
\CommentTok{## Operations:}
\CommentTok{## }
\CommentTok{## Box-Cox transformation on all_outcomes()}
\CommentTok{## Centering for 2 items}
\CommentTok{## Scaling for 2 items}
\end{Highlighting}
\end{Shaded}

\hypertarget{categorical-feature-engineering}{%
\section{Categorical feature engineering}\label{categorical-feature-engineering}}

Most models require that the predictors take numeric form. There are exceptions, for example, tree-based models naturally handle numeric or categorical features. However, even tree-based models can benefit from pre-processing categorical features. The following sections will discuss a few of the more common approaches to engineer categorical features.

\hypertarget{lumping}{%
\subsection{Lumping}\label{lumping}}

Sometimes features will contain levels that have very few observations. For example, there are 28 unique neighborhoods represented in the Ames housing data but several of them only have a few observations.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(ames_train, Neighborhood) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{arrange}\NormalTok{(n)}
\CommentTok{## # A tibble: 28 x 2}
\CommentTok{##    Neighborhood            n}
\CommentTok{##    <fct>               <int>}
\CommentTok{##  1 Green_Hills             1}
\CommentTok{##  2 Landmark                1}
\CommentTok{##  3 Blueste                 5}
\CommentTok{##  4 Greens                  7}
\CommentTok{##  5 Veenker                16}
\CommentTok{##  6 Northpark_Villa        17}
\CommentTok{##  7 Briardale              22}
\CommentTok{##  8 Bloomington_Heights    23}
\CommentTok{##  9 Meadow_Village         27}
\CommentTok{## 10 Clear_Creek            30}
\CommentTok{## # ... with 18 more rows}
\end{Highlighting}
\end{Shaded}

Even numeric features can have similar distributions. For example, \texttt{Screen\_Porch} has 92\% values recorded as zero (zero square footage meaning no screen porch) and the remaining 8\% have unique dispersed values.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(ames_train, Screen_Porch) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{arrange}\NormalTok{(n)}
\CommentTok{## # A tibble: 99 x 2}
\CommentTok{##    Screen_Porch     n}
\CommentTok{##           <int> <int>}
\CommentTok{##  1           40     1}
\CommentTok{##  2           53     1}
\CommentTok{##  3           60     1}
\CommentTok{##  4           63     1}
\CommentTok{##  5           80     1}
\CommentTok{##  6           84     1}
\CommentTok{##  7           88     1}
\CommentTok{##  8           92     1}
\CommentTok{##  9           94     1}
\CommentTok{## 10           95     1}
\CommentTok{## # ... with 89 more rows}
\end{Highlighting}
\end{Shaded}

Sometimes we can benefit from collapsing, or ``lumping'' these into a lesser number of categories. In the above examples, we may want to collapse all levels that are observed in less than 10\% of the training sample into an ``other'' category. We can use \texttt{step\_other()} to do so. However, lumping should be used sparingly as there is often a loss in model performance \citep{apm}.

\begin{tip}
Tree-based models often perform exceptionally well with high cardinality
features and are not as impacted by levels with small representation.
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# lump levels for two features}
\NormalTok{lumping <-}\StringTok{ }\KeywordTok{recipe}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_other}\NormalTok{(Neighborhood, }\DataTypeTok{threshold =} \FloatTok{.01}\NormalTok{, }\DataTypeTok{other =} \StringTok{"other"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{   }\KeywordTok{step_other}\NormalTok{(Screen_Porch, }\DataTypeTok{threshold =} \FloatTok{.1}\NormalTok{, }\DataTypeTok{other =} \StringTok{">0"}\NormalTok{)}

\CommentTok{# apply this blue print --> you will learn about this at }
\CommentTok{# the end of the chapter}
\NormalTok{apply_}\DecValTok{2}\NormalTok{_training <-}\StringTok{ }\KeywordTok{prep}\NormalTok{(lumping, }\DataTypeTok{training =}\NormalTok{ ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{bake}\NormalTok{(ames_train)}

\CommentTok{# new distribution of Neighborhood}
\KeywordTok{count}\NormalTok{(apply_}\DecValTok{2}\NormalTok{_training, Neighborhood) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{arrange}\NormalTok{(n)}
\CommentTok{## # A tibble: 23 x 2}
\CommentTok{##    Neighborhood                                n}
\CommentTok{##    <fct>                                   <int>}
\CommentTok{##  1 Briardale                                  22}
\CommentTok{##  2 Bloomington_Heights                        23}
\CommentTok{##  3 Meadow_Village                             27}
\CommentTok{##  4 Clear_Creek                                30}
\CommentTok{##  5 South_and_West_of_Iowa_State_University    33}
\CommentTok{##  6 Stone_Brook                                36}
\CommentTok{##  7 Timberland                                 47}
\CommentTok{##  8 other                                      47}
\CommentTok{##  9 Northridge                                 55}
\CommentTok{## 10 Iowa_DOT_and_Rail_Road                     59}
\CommentTok{## # ... with 13 more rows}

\CommentTok{# new distribution of Screen_Porch}
\KeywordTok{count}\NormalTok{(apply_}\DecValTok{2}\NormalTok{_training, Screen_Porch) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{arrange}\NormalTok{(n)}
\CommentTok{## # A tibble: 2 x 2}
\CommentTok{##   Screen_Porch     n}
\CommentTok{##   <fct>        <int>}
\CommentTok{## 1 >0             185}
\CommentTok{## 2 0             1869}
\end{Highlighting}
\end{Shaded}

\hypertarget{one-hot-dummy-encoding}{%
\subsection{One-hot \& dummy encoding}\label{one-hot-dummy-encoding}}

Many models require that all predictor variables be numeric. Consequently, we need to intelligently transform any categorical variables into numeric representations so that these algorithms can compute. Some packages automate this process (e.g., \textbf{h2o} and \textbf{caret}) while others do not (e.g., \textbf{glmnet} and \textbf{keras}). There are many ways to re, say,code categorical variables as numeric (e.g., one-hot, ordinal, binary, sum, and Helmert).

The most common is referred to as one-hot encoding, where we transpose our categorical variables so that each level of the feature is represented as a boolean value. For example, one-hot encoding\index{one-hot encoding} \texttt{x} in the following

\begin{tabular}{r|l}
\hline
id & x\\
\hline
1 & a\\
\hline
2 & c\\
\hline
3 & a\\
\hline
4 & b\\
\hline
5 & a\\
\hline
6 & c\\
\hline
7 & c\\
\hline
8 & b\\
\hline
\end{tabular}

results in the following representation:

\begin{tabular}{r|r|r|r}
\hline
id & x.a & x.b & x.c\\
\hline
1 & 1 & 0 & 0\\
\hline
2 & 0 & 0 & 1\\
\hline
3 & 1 & 0 & 0\\
\hline
4 & 0 & 1 & 0\\
\hline
5 & 1 & 0 & 0\\
\hline
6 & 0 & 0 & 1\\
\hline
7 & 0 & 0 & 1\\
\hline
8 & 0 & 1 & 0\\
\hline
\end{tabular}

This is called less than \emph{full rank} encoding where we retain all variables for each level of \texttt{x}. However, this creates perfect collinearity which causes problems with some predictive modeling algorithms (e.g., ordinary linear regression and neural networks). Alternatively, we can create a full-rank encoding by dropping one of the levels (level \texttt{a} has been dropped). This is referred to as \emph{dummy} encoding\index{dummy encoding}.

\begin{tabular}{r|r|r}
\hline
id & x.b & x.c\\
\hline
1 & 0 & 0\\
\hline
2 & 0 & 1\\
\hline
3 & 0 & 0\\
\hline
4 & 1 & 0\\
\hline
5 & 0 & 0\\
\hline
6 & 0 & 1\\
\hline
7 & 0 & 1\\
\hline
8 & 1 & 0\\
\hline
\end{tabular}

We can one-hot or dummy encode with the same function (\texttt{step\_dummy()}). By default, \texttt{step\_dummy()} will create a full rank encoding but you can change this by setting \texttt{one\_hot\ =\ TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# lump levels for two features}
\KeywordTok{recipe}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_dummy}\NormalTok{(}\KeywordTok{all_nominal}\NormalTok{(), }\DataTypeTok{one_hot =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{## Data Recipe}
\CommentTok{## }
\CommentTok{## Inputs:}
\CommentTok{## }
\CommentTok{##       role #variables}
\CommentTok{##    outcome          1}
\CommentTok{##  predictor         80}
\CommentTok{## }
\CommentTok{## Operations:}
\CommentTok{## }
\CommentTok{## Dummy variables from all_nominal()}
\end{Highlighting}
\end{Shaded}

\begin{tip}
Since one-hot encoding adds new features it can significantly increase
the dimensionality of our data. If you have a data set with many
categorical variables and those categorical variables in turn have many
unique levels, the number of features can explode. In these cases you
may want to explore label/ordinal encoding or some other alternative.
\end{tip}

\hypertarget{label-encoding}{%
\subsection{Label encoding}\label{label-encoding}}

\emph{Label encoding}\index{label encoding} is a pure numeric conversion of the levels of a categorical variable. If a categorical variable is a factor and it has pre-specified levels then the numeric conversion will be in level order. If no levels are specified, the encoding will be based on alphabetical order. For example, the \texttt{MS\_SubClass} variable has 16 levels, which we can recode numerically with \texttt{step\_integer()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# original categories}
\KeywordTok{count}\NormalTok{(ames_train, MS_SubClass)}
\CommentTok{## # A tibble: 16 x 2}
\CommentTok{##    MS_SubClass                                   n}
\CommentTok{##    <fct>                                     <int>}
\CommentTok{##  1 One_Story_1946_and_Newer_All_Styles         749}
\CommentTok{##  2 One_Story_1945_and_Older                     97}
\CommentTok{##  3 One_Story_with_Finished_Attic_All_Ages        4}
\CommentTok{##  4 One_and_Half_Story_Unfinished_All_Ages       14}
\CommentTok{##  5 One_and_Half_Story_Finished_All_Ages        192}
\CommentTok{##  6 Two_Story_1946_and_Newer                    401}
\CommentTok{##  7 Two_Story_1945_and_Older                     94}
\CommentTok{##  8 Two_and_Half_Story_All_Ages                  16}
\CommentTok{##  9 Split_or_Multilevel                          87}
\CommentTok{## 10 Split_Foyer                                  31}
\CommentTok{## 11 Duplex_All_Styles_and_Ages                   73}
\CommentTok{## 12 One_Story_PUD_1946_and_Newer                147}
\CommentTok{## 13 One_and_Half_Story_PUD_All_Ages               1}
\CommentTok{## 14 Two_Story_PUD_1946_and_Newer                 94}
\CommentTok{## 15 PUD_Multilevel_Split_Level_Foyer             12}
\CommentTok{## 16 Two_Family_conversion_All_Styles_and_Ages    42}

\CommentTok{# label encoded}
\KeywordTok{recipe}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_integer}\NormalTok{(MS_SubClass) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{prep}\NormalTok{(ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{bake}\NormalTok{(ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(MS_SubClass)}
\CommentTok{## # A tibble: 16 x 2}
\CommentTok{##    MS_SubClass     n}
\CommentTok{##          <dbl> <int>}
\CommentTok{##  1           1   749}
\CommentTok{##  2           2    97}
\CommentTok{##  3           3     4}
\CommentTok{##  4           4    14}
\CommentTok{##  5           5   192}
\CommentTok{##  6           6   401}
\CommentTok{##  7           7    94}
\CommentTok{##  8           8    16}
\CommentTok{##  9           9    87}
\CommentTok{## 10          10    31}
\CommentTok{## 11          11    73}
\CommentTok{## 12          12   147}
\CommentTok{## 13          13     1}
\CommentTok{## 14          14    94}
\CommentTok{## 15          15    12}
\CommentTok{## 16          16    42}
\end{Highlighting}
\end{Shaded}

We should be careful with label encoding unordered categorical features because most models will treat them as ordered numeric features. If a categorical feature is naturally ordered then label encoding is a natural choice (most commonly referred to as ordinal encoding). For example, the various quality features in the Ames housing data are ordinal in nature (ranging from \texttt{Very\_Poor} to \texttt{Very\_Excellent}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames_train }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\KeywordTok{contains}\NormalTok{(}\StringTok{"Qual"}\NormalTok{))}
\CommentTok{## # A tibble: 2,054 x 6}
\CommentTok{##    Overall_Qual Exter_Qual Bsmt_Qual Low_Qual_Fin_SF}
\CommentTok{##    <fct>        <fct>      <fct>               <int>}
\CommentTok{##  1 Above_Avera~ Typical    Typical                 0}
\CommentTok{##  2 Average      Typical    Typical                 0}
\CommentTok{##  3 Above_Avera~ Typical    Typical                 0}
\CommentTok{##  4 Good         Good       Typical                 0}
\CommentTok{##  5 Above_Avera~ Typical    Typical                 0}
\CommentTok{##  6 Very_Good    Good       Good                    0}
\CommentTok{##  7 Very_Good    Good       Good                    0}
\CommentTok{##  8 Good         Typical    Typical                 0}
\CommentTok{##  9 Above_Avera~ Typical    Good                    0}
\CommentTok{## 10 Above_Avera~ Typical    Good                    0}
\CommentTok{## # ... with 2,044 more rows, and 2 more variables:}
\CommentTok{## #   Kitchen_Qual <fct>, Garage_Qual <fct>}
\end{Highlighting}
\end{Shaded}

Ordinal encoding\index{ordinal encoding} these features provides a natural and intuitive interpretation and can logically be applied to all models.

\begin{tip}
The various \texttt{xxx\_Qual} features in the Ames housing are not
ordered factors. For ordered factors you could also use
\texttt{step\_ordinalscore()}.
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# original categories}
\KeywordTok{count}\NormalTok{(ames_train, Overall_Qual)}
\CommentTok{## # A tibble: 10 x 2}
\CommentTok{##    Overall_Qual       n}
\CommentTok{##    <fct>          <int>}
\CommentTok{##  1 Very_Poor          4}
\CommentTok{##  2 Poor               8}
\CommentTok{##  3 Fair              23}
\CommentTok{##  4 Below_Average    169}
\CommentTok{##  5 Average          582}
\CommentTok{##  6 Above_Average    497}
\CommentTok{##  7 Good             425}
\CommentTok{##  8 Very_Good        249}
\CommentTok{##  9 Excellent         75}
\CommentTok{## 10 Very_Excellent    22}

\CommentTok{# label encoded}
\KeywordTok{recipe}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_integer}\NormalTok{(Overall_Qual) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{prep}\NormalTok{(ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{bake}\NormalTok{(ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(Overall_Qual)}
\CommentTok{## # A tibble: 10 x 2}
\CommentTok{##    Overall_Qual     n}
\CommentTok{##           <dbl> <int>}
\CommentTok{##  1            1     4}
\CommentTok{##  2            2     8}
\CommentTok{##  3            3    23}
\CommentTok{##  4            4   169}
\CommentTok{##  5            5   582}
\CommentTok{##  6            6   497}
\CommentTok{##  7            7   425}
\CommentTok{##  8            8   249}
\CommentTok{##  9            9    75}
\CommentTok{## 10           10    22}
\end{Highlighting}
\end{Shaded}

\hypertarget{alternatives-1}{%
\subsection{Alternatives}\label{alternatives-1}}

There are several alternative categorical encodings that are implemented in various R machine learning engines and are worth exploring. For example, target encoding\index{target encoding} is the process of replacing a categorical value with the mean (regression) or proportion (classification) of the target variable. For example, target encoding the \texttt{Neighborhood} feature would change \texttt{North\_Ames} to 144617.

\begin{tabular}{l|r}
\hline
Neighborhood & Avg Sale\_Price\\
\hline
North\_Ames & 147040\\
\hline
College\_Creek & 202438\\
\hline
Old\_Town & 121815\\
\hline
Edwards & 124297\\
\hline
Somerset & 232394\\
\hline
Northridge\_Heights & 320174\\
\hline
Gilbert & 191095\\
\hline
Sawyer & 137405\\
\hline
Northwest\_Ames & 186082\\
\hline
Sawyer\_West & 183062\\
\hline
\end{tabular}

Target encoding runs the risk of \emph{data leakage} since you are using the reponse variable to encode a feature. An alternative to this is to change the feature value to represent the proportion a particular level represents for a given feature. In this case, \texttt{North\_Ames} would be changed to 0.153.

\begin{note}
In Chapter 9, we discuss how tree-based models use this approach to
order categorical features when choosing a split point.
\end{note}

\begin{tabular}{l|r}
\hline
Neighborhood & Proportion\\
\hline
North\_Ames & 0.1543\\
\hline
College\_Creek & 0.0930\\
\hline
Old\_Town & 0.0808\\
\hline
Edwards & 0.0638\\
\hline
Somerset & 0.0618\\
\hline
Northridge\_Heights & 0.0579\\
\hline
Gilbert & 0.0560\\
\hline
Sawyer & 0.0521\\
\hline
Northwest\_Ames & 0.0399\\
\hline
Sawyer\_West & 0.0448\\
\hline
\end{tabular}

Several alternative approaches include effect or likelihood encoding \citep{micci2001preprocessing, zumel2016vtreat}, empirical Bayes methods \citep{west2014linear}, word and entity embeddings \citep{guo2016entity, chollet2018deep}, and more. For more indepth coverage of categorical encodings we highly recommend \citet{kuhn2019feature}.

\hypertarget{feature-reduction}{%
\section{Dimension reduction}\label{feature-reduction}}

Dimension reduction is an alternative approach to filter out non-informative features without manually removing them. We discuss dimension reduction topics in depth later in the book (Chapters \ref{pca}-\ref{autoencoders}) so please refer to those chapters for details.

However, we wanted to highlight that it is very common to include these types of dimension reduction approaches during the feature engineering process. For example, we may wish to reduce the dimension of our features with principal components analysis\index{principal components analysis} (Chapter \ref{pca}) and retain the number of components required to explain, say, 95\% of the variance and use these components as features in downstream modeling.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{recipe}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_center}\NormalTok{(}\KeywordTok{all_numeric}\NormalTok{()) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_scale}\NormalTok{(}\KeywordTok{all_numeric}\NormalTok{()) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_pca}\NormalTok{(}\KeywordTok{all_numeric}\NormalTok{(), }\DataTypeTok{threshold =} \FloatTok{.95}\NormalTok{)}
\CommentTok{## Data Recipe}
\CommentTok{## }
\CommentTok{## Inputs:}
\CommentTok{## }
\CommentTok{##       role #variables}
\CommentTok{##    outcome          1}
\CommentTok{##  predictor         80}
\CommentTok{## }
\CommentTok{## Operations:}
\CommentTok{## }
\CommentTok{## Centering for all_numeric()}
\CommentTok{## Scaling for all_numeric()}
\CommentTok{## PCA extraction with all_numeric()}
\end{Highlighting}
\end{Shaded}

\hypertarget{proper-implementation}{%
\section{Proper implementation}\label{proper-implementation}}

We stated at the beginning of this chapter that we should think of feature engineering as creating a blueprint rather than manually performing each task individually. This helps us in two ways: (1) thinking sequentially and (2) to apply appropriately within the resampling process.

\hypertarget{sequential-steps}{%
\subsection{Sequential steps}\label{sequential-steps}}

Thinking of feature engineering as a blueprint forces us to think of the ordering of our pre-processing steps. Although each particular problem requires you to think of the effects of sequential pre-processing, there are some general suggestions that you should consider:

\begin{itemize}
\tightlist
\item
  If using a log or Box-Cox transformation, don't center the data first or do any operations that might make the data non-positive. Alternatively, use the Yeo-Johnson transformation so you don't have to worry about this.
\item
  One-hot or dummy encoding typically results in sparse data which many algorithms can operate efficiently on. If you standardize sparse data you will create dense data and you loose the computational efficiency. Consequently, its often preferred to standardize your numeric features and then one-hot/dummy encode.
\item
  If you are lumping infrequently categories together, do so before one-hot/dummy encoding.
\item
  Although you can perform dimension reduction procedures on categorical features, it is common to primarily do so on numeric features when doing so for feature engineering purposes.
\end{itemize}

While your project's needs may vary, here is a suggested order of potential steps that should work for most problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Filter out zero or near-zero variance features.
\item
  Perform imputation if required.
\item
  Normalize to resolve numeric feature skewness.
\item
  Standardize (center and scale) numeric features.
\item
  Perform dimension reduction (e.g., PCA) on numeric features.
\item
  One-hot or dummy encode categorical features.
\end{enumerate}

\hypertarget{data-leakage}{%
\subsection{Data leakage}\label{data-leakage}}

\emph{Data leakage}\index{data leakage} is when information from outside the training data set is used to create the model. Data leakage often occurs during the data preprocessing period. To minimize this, feature engineering should be done in isolation of each resampling iteration. Recall that resampling allows us to estimate the generalizable prediction error. Therefore, we should apply our feature engineering blueprint to each resample independently as illustrated in Figure \ref{fig:minimize-leakage}. That way we are not leaking information from one data set to another (each resample is designed to act as isolated training and test data).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/minimize-leakage} 

}

\caption{Performing feature engineering pre-processing within each resample helps to minimize data leakage.}\label{fig:minimize-leakage}
\end{figure}

For example, when standardizing numeric features, each resampled training data should use its own mean and variance estimates and these specific values should be applied to the same resampled test set. This imitates how real-life prediction occurs where we only know our current data's mean and variance estimates; therefore, on new data that comes in where we need to predict we assume the feature values follow the same distribution of what we've seen in the past.

\hypertarget{engineering-process-example}{%
\subsection{Putting the process together}\label{engineering-process-example}}

To illustrate how this process works together via R code, let's do a simple re-assessment on the \texttt{ames} data set that we did at the end of the last chapter (section \ref{put-process-together}) and see if some simple feature engineering improves our prediction error. But first, we'll formally introduce the \textbf{recipes} package, which we've been implicitly illustrating throughout.

The \textbf{recipes} package allows us to develop our feature engineering blueprint in a sequential nature. The idea behind \textbf{recipes} is similar to \texttt{caret::preProcess()} where we want to create the pre-processing blueprint but apply it later and within each resample.\footnote{In fact, most of the feature engineering capabilities found in \textbf{resample} can also be found in \texttt{caret::preProcess()}.}

There are three main steps in creating and applying feature engineering with \textbf{recipes}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{recipe}: where you define your feature engineering steps to create your blueprint.
\item
  \texttt{prep}are: estimate feature engineering parameters based on training data.
\item
  \texttt{bake}: apply the blueprint to new data.
\end{enumerate}

The first step is where you define your blueprint (aka recipe). With this process, you supply the formula of interest (the target variable, features, and the data these are based on) with \texttt{recipe()} and then you sequentially add feature engineering steps with \texttt{step\_xxx()}. For example, the following defines \texttt{Sale\_Price} as the target variable and then uses all the remaining columns as features based on \texttt{ames\_train}. We then:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Remove near-zero variance features that are categorical (aka nominal).
\item
  Ordinally encode our quality-based features (which are inherently ordinal).
\item
  Center and scale (i.e., standardize) all numeric fetures.
\item
  Perform dimension reduction by applying PCA to all numeric features.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{blueprint <-}\StringTok{ }\KeywordTok{recipe}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_nzv}\NormalTok{(}\KeywordTok{all_nominal}\NormalTok{())  }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_integer}\NormalTok{(}\KeywordTok{matches}\NormalTok{(}\StringTok{"Qual|Cond|QC|Qu"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_center}\NormalTok{(}\KeywordTok{all_numeric}\NormalTok{(), }\OperatorTok{-}\KeywordTok{all_outcomes}\NormalTok{()) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_scale}\NormalTok{(}\KeywordTok{all_numeric}\NormalTok{(), }\OperatorTok{-}\KeywordTok{all_outcomes}\NormalTok{()) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_pca}\NormalTok{(}\KeywordTok{all_numeric}\NormalTok{(), }\OperatorTok{-}\KeywordTok{all_outcomes}\NormalTok{())}
  
\NormalTok{blueprint}
\CommentTok{## Data Recipe}
\CommentTok{## }
\CommentTok{## Inputs:}
\CommentTok{## }
\CommentTok{##       role #variables}
\CommentTok{##    outcome          1}
\CommentTok{##  predictor         80}
\CommentTok{## }
\CommentTok{## Operations:}
\CommentTok{## }
\CommentTok{## Sparse, unbalanced variable filter on all_nominal()}
\CommentTok{## Integer encoding for matches("Qual|Cond|QC|Qu")}
\CommentTok{## Centering for 2 items}
\CommentTok{## Scaling for 2 items}
\CommentTok{## PCA extraction with 2 items}
\end{Highlighting}
\end{Shaded}

Next, we need to train this blueprint on some training data. Remember, there are many feature engineering steps that we do not want to train on the test data (e.g., standardize and PCA) as this would create data leakage. So in this step we estimate these parameters based on the training data of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prepare <-}\StringTok{ }\KeywordTok{prep}\NormalTok{(blueprint, }\DataTypeTok{training =}\NormalTok{ ames_train)}
\NormalTok{prepare}
\CommentTok{## Data Recipe}
\CommentTok{## }
\CommentTok{## Inputs:}
\CommentTok{## }
\CommentTok{##       role #variables}
\CommentTok{##    outcome          1}
\CommentTok{##  predictor         80}
\CommentTok{## }
\CommentTok{## Training data contained 2054 data points and no missing data.}
\CommentTok{## }
\CommentTok{## Operations:}
\CommentTok{## }
\CommentTok{## Sparse, unbalanced variable filter removed Street, ... [trained]}
\CommentTok{## Integer encoding for Condition_1, ... [trained]}
\CommentTok{## Centering for Lot_Frontage, ... [trained]}
\CommentTok{## Scaling for Lot_Frontage, ... [trained]}
\CommentTok{## PCA extraction with Lot_Frontage, ... [trained]}
\end{Highlighting}
\end{Shaded}

Lastly, we can apply our blueprint to new data (e.g., the training data or future test data) with \texttt{bake()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baked_train <-}\StringTok{ }\KeywordTok{bake}\NormalTok{(prepare, }\DataTypeTok{new_data =}\NormalTok{ ames_train)}
\NormalTok{baked_test <-}\StringTok{ }\KeywordTok{bake}\NormalTok{(prepare, }\DataTypeTok{new_data =}\NormalTok{ ames_test)}
\NormalTok{baked_train}
\CommentTok{## # A tibble: 2,054 x 27}
\CommentTok{##    MS_SubClass MS_Zoning Lot_Shape Lot_Config}
\CommentTok{##    <fct>       <fct>     <fct>     <fct>     }
\CommentTok{##  1 One_Story_~ Resident~ Slightly~ Corner    }
\CommentTok{##  2 One_Story_~ Resident~ Regular   Inside    }
\CommentTok{##  3 One_Story_~ Resident~ Slightly~ Corner    }
\CommentTok{##  4 One_Story_~ Resident~ Regular   Corner    }
\CommentTok{##  5 Two_Story_~ Resident~ Slightly~ Inside    }
\CommentTok{##  6 One_Story_~ Resident~ Slightly~ Inside    }
\CommentTok{##  7 One_Story_~ Resident~ Slightly~ Inside    }
\CommentTok{##  8 Two_Story_~ Resident~ Regular   Inside    }
\CommentTok{##  9 Two_Story_~ Resident~ Slightly~ Corner    }
\CommentTok{## 10 One_Story_~ Resident~ Slightly~ Inside    }
\CommentTok{## # ... with 2,044 more rows, and 23 more variables:}
\CommentTok{## #   Neighborhood <fct>, Bldg_Type <fct>,}
\CommentTok{## #   House_Style <fct>, Roof_Style <fct>,}
\CommentTok{## #   Exterior_1st <fct>, Exterior_2nd <fct>,}
\CommentTok{## #   Mas_Vnr_Type <fct>, Foundation <fct>,}
\CommentTok{## #   Bsmt_Exposure <fct>, BsmtFin_Type_1 <fct>,}
\CommentTok{## #   Central_Air <fct>, Electrical <fct>,}
\CommentTok{## #   Garage_Type <fct>, Garage_Finish <fct>,}
\CommentTok{## #   Paved_Drive <fct>, Fence <fct>, Sale_Type <fct>,}
\CommentTok{## #   Sale_Price <int>, PC1 <dbl>, PC2 <dbl>, PC3 <dbl>,}
\CommentTok{## #   PC4 <dbl>, PC5 <dbl>}
\end{Highlighting}
\end{Shaded}

Consequently, the goal is to develop our blueprint, then within each resample iteration we want to apply \texttt{prep()} and \texttt{bake()} to our resample training and validation data. Luckily, the \textbf{caret} package simplifies this process. We only need to specify the blueprint and \textbf{caret} will automatically prepare and bake within each resample. We illustrate with the \texttt{ames} housing example.

First, we create our feature engineering blueprint to perform the following tasks:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Filter out near-zero variance features for categorical features.
\item
  Ordinally encode all quality features, which are on a 1--10 likert scale.
\item
  Standardize (center and scale) all numeric features.
\item
  One-hot encode our remaining categorical features.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{blueprint <-}\StringTok{ }\KeywordTok{recipe}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_train) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_nzv}\NormalTok{(}\KeywordTok{all_nominal}\NormalTok{()) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_integer}\NormalTok{(}\KeywordTok{matches}\NormalTok{(}\StringTok{"Qual|Cond|QC|Qu"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_center}\NormalTok{(}\KeywordTok{all_numeric}\NormalTok{(), }\OperatorTok{-}\KeywordTok{all_outcomes}\NormalTok{()) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_scale}\NormalTok{(}\KeywordTok{all_numeric}\NormalTok{(), }\OperatorTok{-}\KeywordTok{all_outcomes}\NormalTok{()) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{step_dummy}\NormalTok{(}\KeywordTok{all_nominal}\NormalTok{(), }\OperatorTok{-}\KeywordTok{all_outcomes}\NormalTok{(), }\DataTypeTok{one_hot =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, we apply the same resampling method and hyperparameter search grid as we did in Section \ref{put-process-together}. The only difference is when we train our resample models with \texttt{train()}, we supply our blueprint as the first argument and then \textbf{caret} takes care of the rest.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a resampling method}
\NormalTok{cv <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}
  \DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
  \DataTypeTok{number =} \DecValTok{10}\NormalTok{, }
  \DataTypeTok{repeats =} \DecValTok{5}
\NormalTok{  )}

\CommentTok{# create a hyperparameter grid search}
\NormalTok{hyper_grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{k =} \KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DataTypeTok{by =} \DecValTok{1}\NormalTok{))}

\CommentTok{# fit knn model and perform grid search}
\NormalTok{knn_fit2 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  blueprint, }
  \DataTypeTok{data =}\NormalTok{ ames_train, }
  \DataTypeTok{method =} \StringTok{"knn"}\NormalTok{, }
  \DataTypeTok{trControl =}\NormalTok{ cv, }
  \DataTypeTok{tuneGrid =}\NormalTok{ hyper_grid,}
  \DataTypeTok{metric =} \StringTok{"RMSE"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Looking at our results we see that the best model was associated with \(k=\) 12, which resulted in a cross-validated RMSE of 32,991. Figure \ref{fig:knn-with-blueprint-assess} illustrates the cross-validated error rate across the spectrum of hyperparameter values that we specified.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# print model results}
\NormalTok{knn_fit2}
\CommentTok{## k-Nearest Neighbors }
\CommentTok{## }
\CommentTok{## 2054 samples}
\CommentTok{##   80 predictor}
\CommentTok{## }
\CommentTok{## Recipe steps: nzv, integer, center, scale, dummy }
\CommentTok{## Resampling: Cross-Validated (10 fold, repeated 5 times) }
\CommentTok{## Summary of sample sizes: 1848, 1850, 1848, 1850, 1850, 1848, ... }
\CommentTok{## Resampling results across tuning parameters:}
\CommentTok{## }
\CommentTok{##   k   RMSE   Rsquared  MAE  }
\CommentTok{##    2  36949  0.7931    22711}
\CommentTok{##    3  35724  0.8071    21985}
\CommentTok{##    4  34731  0.8194    21332}
\CommentTok{##    5  34127  0.8271    20948}
\CommentTok{##    6  33910  0.8314    20777}
\CommentTok{##    7  33414  0.8389    20670}
\CommentTok{##    8  33237  0.8428    20568}
\CommentTok{##    9  33145  0.8452    20541}
\CommentTok{##   10  33043  0.8476    20499}
\CommentTok{##   11  33006  0.8485    20508}
\CommentTok{##   12  32991  0.8495    20541}
\CommentTok{##   13  33095  0.8491    20610}
\CommentTok{##   14  33116  0.8496    20631}
\CommentTok{##   15  33075  0.8508    20657}
\CommentTok{##   16  33105  0.8510    20696}
\CommentTok{##   17  33194  0.8509    20738}
\CommentTok{##   18  33289  0.8504    20828}
\CommentTok{##   19  33373  0.8500    20889}
\CommentTok{##   20  33468  0.8497    20963}
\CommentTok{##   21  33529  0.8498    21020}
\CommentTok{##   22  33581  0.8498    21065}
\CommentTok{##   23  33652  0.8496    21133}
\CommentTok{##   24  33702  0.8498    21202}
\CommentTok{##   25  33710  0.8504    21236}
\CommentTok{## }
\CommentTok{## RMSE was used to select the optimal model using}
\CommentTok{##  the smallest value.}
\CommentTok{## The final value used for the model was k = 12.}

\CommentTok{# plot cross validation results}
\KeywordTok{ggplot}\NormalTok{(knn_fit2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/knn-with-blueprint-assess-1} 

}

\caption{Results from the same grid search performed in Section 2.7 but with feature engineering performed within each resample.}\label{fig:knn-with-blueprint-assess}
\end{figure}

By applying a handful of the preprocessing techniques discussed throughout this chapter, we were able to reduce our prediction error by over \$10,000. The chapters that follow will look to see if we can continue reducing our error by applying different algorithms and feature engineering blueprints.

\hypertarget{part-supervised-learning}{%
\part{Supervised Learning}\label{part-supervised-learning}}

\hypertarget{linear-regression}{%
\chapter{Linear Regression}\label{linear-regression}}

\emph{Linear regression}\index{linear regression}, a staple of classical statistical modeling, is one of the simplest algorithms for doing supervised learning. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described in later chapters, linear regression is still a useful and widely applied statistical learning method. Moreover, it serves as a good starting point for more advanced approaches; as we will see in later chapters, many of the more sophisticated statistical learning approaches can be seen as generalizations to or extensions of ordinary linear regression. Consequently, it is important to have a good understanding of linear regression before studying more complex learning methods. This chapter introduces linear regression with an emphasis on prediction, rather than inference. An excellent and comprehensive overview of linear regression is provided in \citet{kutner-2005-applied}. See \citet{faraway-2016-linear} for a discussion of linear regression in R (the book's website also provides Python scripts).

\hypertarget{prerequisites-2}{%
\section{Prerequisites}\label{prerequisites-2}}

This chapter leverages the following packages:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Helper packages}
\KeywordTok{library}\NormalTok{(dplyr)    }\CommentTok{# for data manipulation}
\KeywordTok{library}\NormalTok{(ggplot2)  }\CommentTok{# for awesome graphics}

\CommentTok{# Modeling packages}
\KeywordTok{library}\NormalTok{(caret)    }\CommentTok{# for logistic regression modeling}

\CommentTok{# Model interpretability packages}
\KeywordTok{library}\NormalTok{(vip)      }\CommentTok{# variable importance}
\end{Highlighting}
\end{Shaded}

We'll also continue working with the \texttt{ames\_train} data set created in Section \ref{put-process-together}.

\hypertarget{simple-linear-regression}{%
\section{Simple linear regression}\label{simple-linear-regression}}

Pearson's correlation coefficient is often used to quantify the strength of the linear association between two continuous variables. In this section, we seek to fully characterize that linear relationship. \emph{Simple linear regression} (SLR) assumes that the statistical relationship between two continuous variables (say \(X\) and \(Y\)) is (at least approximately) linear:

\begin{equation}
\label{eq:lm}
  Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, \quad \text{for } i = 1, 2, \dots, n,
\end{equation}

where \(Y_i\) represents the \emph{i}-th response value, \(X_i\) represents the \emph{i}-th feature value, \(\beta_0\) and \(\beta_1\) are fixed, but unknown constants (commonly referred to as coefficients or parameters) that represent the intercept and slope of the regression line, respectively, and \(\epsilon_i\) represents noise or random error. In this Chapter, we'll assume that the errors are normally distirbuted with mean zero and constant variance \(\sigma^2\), denoted \(\stackrel{iid}{\sim} \left(0, \sigma^2\right)\). Since the random errors are centered around zero (i.e., \(E\left(\epsilon\right) = 0\)), linear regression is really a problem of estimating a \emph{conditional mean}:

\begin{equation}
  E\left(Y_i | X_i\right) = \beta_0 + \beta_1 X_i.
\end{equation}

For brevity, we often drop the conditional piece and write \(E\left(Y | X\right) = E\left(Y\right)\). Consequently, the interpretation of the coefficients are in terms of the average, or mean response. For example, the intercept \(\beta_0\) represents the average response value when \(X = 0\) (it is often not meaningful or of interest and is is sometimes referred to as a \emph{bias term}). The slope \(\beta_1\) represents the increase in the average response per one-unit increase in \(X\) (i.e., it is a \emph{rate of change}).

\hypertarget{estimation}{%
\subsection{Estimation}\label{estimation}}

Ideally, we want estimates of \(\beta_0\) and \(\beta_1\) that give us the ``best fitting'' line. But what is meant by ``best fitting''? The most common approach is to use the method of \emph{least squares}\index{least squares} (LS) estimation; this form of linear regression is often referred to as ordinary least squares (OLS) regression. There are multiple ways to measure ``best fitting'', but the LS criterion finds the ``best fitting'' line by minimizing the \emph{residual sum of squares}\index{residual sum of squares} (RSS):

\begin{equation}
\label{eq:least-squares-simple}
  RSS\left(\beta_0, \beta_1\right) = \sum_{i=1}^n\left[Y_i - \left(\beta_0 + \beta_1 X_i\right)\right]^2 = \sum_{i=1}^n\left(Y_i - \beta_0 - \beta_1 X_i\right)^2.
\end{equation}

The LS estimates of \(\beta_0\) and \(\beta_1\) are denoted as \(\widehat{\beta}_0\) and \(\widehat{\beta}_1\), respectively. Once obtained, we can generate predicted values, say at \(X = X_{new}\), using the estimated regression equation:

\begin{equation}
  \widehat{Y}_{new} = \widehat{\beta}_0 + \widehat{\beta}_1 X_{new},
\end{equation}

where \(\widehat{Y}_{new} = \widehat{E\left(Y_{new} | X = X_{new}\right)}\) is the estimated mean response at \(X = X_{new}\).

With the Ames housing data, suppose we wanted to model a linear relationship between the total above ground living space of a home (\texttt{Gr\_Liv\_Area}) and sale price (\texttt{Sale\_Price}). To perform an OLS regression model in R we can use the \texttt{lm()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Gr_Liv_Area, }\DataTypeTok{data =}\NormalTok{ ames_train)}
\end{Highlighting}
\end{Shaded}

The fitted model (\texttt{model1}) is displayed in the left plot in Figure \ref{fig:04-visualize-model1} where the points represent the values of \texttt{Sale\_Price} in the training data. In the right plot of Figure \ref{fig:04-visualize-model1}, the vertical lines represent the individual errors, called \emph{residuals}\index{residuals}, associated with each observation. The OLS criterion \eqref{eq:least-squares-simple} identifies the ``best fitting'' line that minimizes the sum of squares of these residuals.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/04-visualize-model1-1} 

}

\caption{The least squares fit from regressing sale price on living space for the the Ames housing data. Left: Fitted regresison line. Right: Fitted regression line with vertical grey bars representing the residuals.}\label{fig:04-visualize-model1}
\end{figure}

The \texttt{coef()} function extracts the estimated coefficients from the model. We can also use \texttt{summary()} to get a more detailed report of the model results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(model1)}
\CommentTok{## }
\CommentTok{## Call:}
\CommentTok{## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_train)}
\CommentTok{## }
\CommentTok{## Residuals:}
\CommentTok{##     Min      1Q  Median      3Q     Max }
\CommentTok{## -413052  -30218   -1612   23383  330421 }
\CommentTok{## }
\CommentTok{## Coefficients:}
\CommentTok{##             Estimate Std. Error t value}
\CommentTok{## (Intercept)  7989.35    3892.40    2.05}
\CommentTok{## Gr_Liv_Area   115.59       2.46   46.90}
\CommentTok{##                        Pr(>|t|)    }
\CommentTok{## (Intercept)                0.04 *  }
\CommentTok{## Gr_Liv_Area <0.0000000000000002 ***}
\CommentTok{## ---}
\CommentTok{## Signif. codes:  }
\CommentTok{## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{## }
\CommentTok{## Residual standard error: 55800 on 2052 degrees of freedom}
\CommentTok{## Multiple R-squared:  0.517,  Adjusted R-squared:  0.517 }
\CommentTok{## F-statistic: 2.2e+03 on 1 and 2052 DF,  p-value: <0.0000000000000002}
\end{Highlighting}
\end{Shaded}

The estimated coefficients from our model are \(\widehat{\beta}_0 =\) 7989.35 and \(\widehat{\beta}_1 =\) 115.59. To interpret, we estimate that the mean selling price increases by 115.59 for each additional one square foot of above ground living space. This simple description of the relationship between the sale price and square footage using a single number (i.e., the slope) is what makes linear regression such an intuitive and popular modeling tool.

One drawback of the LS procedure in linear regression is that it only provides estimates of the coefficents; it does not provide an estimate of the error variance \(\sigma^2\)! LS also makes no assumptions about the random errors. These assumptions are important for inference and in estimating the error variance which we're assuming is a constant value \(\sigma^2\). One way to estimate \(\sigma^2\) (which is required for characterizing the variability of our fitted model), is to use the method of \emph{maximum likelihood}\index{maximum likelihood} (ML) estimation (see \citet{kutner-2005-applied} sec 1.7 for details). The ML procedure requires that we assume a particular distribution for the random errors. Most often, we assume the errors to be normally distributed. In practice, under the usual assumptions stated above, an unbiased estimate of the error variance is given as the sum of the squared residuals divided by \(n - p\) (where \(p\) is the number of regression coefficients or parameters in the model):

\begin{equation}
  \widehat{\sigma}^2 = \frac{1}{n - p}\sum_{i = 1} ^ n r_i ^ 2,
\end{equation}

where \(r_i = \left(Y_i - \widehat{Y}_i\right)\) is referred to as the \emph{i}-th residual (i.e., the difference between the \emph{i}-th observed and predicted response value). The quantity \(\widehat{\sigma}^2\) is also referred to as the \emph{mean square error}\index{mean square error} (MSE) and it's square root is denoted RMSE (see Section \ref{model-eval} for discussion on these metrics). In R, the RMSE of a linear model can be extracted using the \texttt{sigma()} function:

\begin{note}
Typically, these error metrics are computed on a separate validation set
or using cross-validation as discussed in Section 2.4; however, they can
also be computed on the same training data the model was trained on as
illustrated here.
\end{note}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sigma}\NormalTok{(model1)    }\CommentTok{# RMSE}
\CommentTok{## [1] 55753}
\KeywordTok{sigma}\NormalTok{(model1)}\OperatorTok{^}\DecValTok{2}  \CommentTok{# MSE}
\CommentTok{## [1] 3108447546}
\end{Highlighting}
\end{Shaded}

Note that the RMSE is also reported as the \texttt{Residual\ standard\ error} in the output from \texttt{summary()}.

\hypertarget{inference}{%
\subsection{Inference}\label{inference}}

How accurate are the LS of \(\beta_0\) and \(\beta_1\)? Point estimates by themselves are not very useful. It is often desirable to associate some measure of an estimates variability. The variability of an estimate is often measured by its \emph{standard error}\index{standard error} (SE)---the square root of its variance. If we assume that the errors in the linear regression model are \(\stackrel{iid}{\sim} \left(0, \sigma^2\right)\), then simple expressions for the SEs of the estimated coefficients exist and are displayed in the column labeled \texttt{Std.\ Error} in the output from \texttt{summary()}. From this, we can also derive simple \(t\)-tests to understand if the individual coefficients are statistically significant from zero. The \emph{t}-statistics for such a test are nothing more than the estimated coefficients divided by their corresponding estimated standard errors (i.e., in the output from \texttt{summary()}, \texttt{t\ value} = \texttt{Estimate} / \texttt{Std.\ Error}). The reported \emph{t}-statistics measure the number of standard deviations each coefficient is away from 0. Thus, large \emph{t}-statistics (greater than two in absolute value, say) roughly indicate statistical significance at the \(\alpha = 0.05\) level. The \emph{p}-values for these tests are also reported by \texttt{summary()} in the column labeled \texttt{Pr(\textgreater{}\textbar{}t\textbar{})}.

Under the same assumptions, we can also derive confidence intervals for the coefficients. The formula for the traditional \(100\left(1 - \alpha\right)\)\% confidence interval for \(\beta_j\) is

\begin{equation}
  \widehat{\beta}_j \pm t_{1 - \alpha / 2, n - p} \widehat{SE}\left(\widehat{\beta}_j\right),
  \label{eq:conf-int}
\end{equation}

In R, we can construct such (one-at-a-time) confidence intervals for each coefficient using \texttt{confint()}. For example, a 95\% confidence intervals for the coefficients in our SLR example can be computed using

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(model1, }\DataTypeTok{level =} \FloatTok{0.95}\NormalTok{)}
\CommentTok{##             2.5 %  97.5 %}
\CommentTok{## (Intercept) 355.9 15622.8}
\CommentTok{## Gr_Liv_Area 110.8   120.4}
\end{Highlighting}
\end{Shaded}

To interpret, we estimate with 95\% confidence that the mean selling price increases between 110.75 and 120.42 for each additional one square foot of above ground living space. We can also conclude that the slope \(\beta_1\) is significantly different from zero (or any other pre-specified value not included in the interval) at the \(\alpha = 0.05\) level. This is also supported by the output from \texttt{summary()}.

\BeginKnitrBlock{note}
Most statistical software, including R, will include estimated standard errors, \emph{t}-statistics, etc. as part of its regression output. However, it is important to remember that such quantities depend on three major assumptions of the linear regresion model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independent observations
\item
  The random errors have mean zero, and constant variance
\item
  The random errors are normally distributed
\end{enumerate}

If any or all of these assumptions are violated, then remdial measures need to be taken. For instance, \emph{weighted least squares} (and other procedures) can be used when the constant variance assumption is violated. Transformations (of both the response and features) can also help to correct departures from these assumptions. The residuals are extremely useful in helping to identify how parametric models depart from such assumptions.
\EndKnitrBlock{note}

\hypertarget{multi-lm}{%
\section{Multiple linear regression}\label{multi-lm}}

In practice, we often have more than one predictor. For example, with the Ames housing data, we may wish to understand if above ground square footage (\texttt{Gr\_Liv\_Area}) and the year the house was built (\texttt{Year\_Built}) are (linearly) related to sale price (\texttt{Sale\_Price}). We can extend the SLR model so that it can directly accommodate multiple predictors; this is referred to as the \emph{multiple linear regression} (MLR) model. With two predictors, the MLR model becomes:

\begin{equation}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon,
\end{equation}

where \(X_1\) and \(X_2\) are features of interest. In our Ames housing example, \(X_1\) represents \texttt{Gr\_Liv\_Area} and \(X_2\) represents \texttt{Year\_Built}.

In R, multiple linear regression models can be fit by separating all the features of interest with a \texttt{+}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(model2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Gr_Liv_Area }\OperatorTok{+}\StringTok{ }\NormalTok{Year_Built, }\DataTypeTok{data =}\NormalTok{ ames_train))}
\CommentTok{## }
\CommentTok{## Call:}
\CommentTok{## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)}
\CommentTok{## }
\CommentTok{## Coefficients:}
\CommentTok{## (Intercept)  Gr_Liv_Area   Year_Built  }
\CommentTok{##  -2071358.4         99.2       1067.1}
\end{Highlighting}
\end{Shaded}

Alternatively, we can use \texttt{update()} to update the model formula used in \texttt{model1}. The new formula can use a \texttt{.} as short hand for keep everything on either the left or right hand side of the formula, and a \texttt{+} or \texttt{-} can be used to add or remove terms from the original model, respectively. In the case of adding \texttt{Year\_Built} to to \texttt{model1}, we could've used:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(model2 <-}\StringTok{ }\KeywordTok{update}\NormalTok{(model1, . }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{+}\StringTok{ }\NormalTok{Year_Built))}
\CommentTok{## }
\CommentTok{## Call:}
\CommentTok{## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)}
\CommentTok{## }
\CommentTok{## Coefficients:}
\CommentTok{## (Intercept)  Gr_Liv_Area   Year_Built  }
\CommentTok{##  -2071358.4         99.2       1067.1}
\end{Highlighting}
\end{Shaded}

The LS estimates of the regression coefficients are \(\widehat{\beta}_1 =\) 99.169 and \(\widehat{\beta}_2 =\) 1067.108 (the estimated intercept is -2071358.426. In other words, every one square foot increase to above ground square footage is associated with an additional \$99.17 in \textbf{mean selling price} when holding the year the house was built constant. Likewise, for every year newer a home is there is approximately an increase of \$1,067.11 in selling price when holding the above ground square footage constant.

A contour plot of the fitted regression surface is displayed in the left side of Figure \ref{fig:04-mlr-fit} below. Note how the fitted regression surface is flat (i.e., it does not twist or bend). This is true for all linear models that include only \emph{main effects} (i.e., terms involving only a single predictor). One way to model curvature is to include \emph{interaction effects}. An interaction occurs when the effect of one predictor on the response depends on the values of other predictors. In linear regression, interactions can be captured via products of features (i.e., \(X1 \times X_2\)). A model with two main effects can also include a two-way interaction. For example, to include an interaction between \(X_1 =\) \texttt{Gr\_Liv\_Area} and \(X_2 =\) \texttt{Year\_Built}, we introduce an additional product term:

\begin{equation}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon.
\end{equation}

Note that in R, we use the \texttt{:} operator to include an interaction (technically, we could use \texttt{*} as well, but \texttt{x1\ *\ x2} is shorthand for \texttt{x1\ +\ x2\ +\ x1:x2} so is slightly redundant):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Gr_Liv_Area }\OperatorTok{+}\StringTok{ }\NormalTok{Year_Built }\OperatorTok{+}\StringTok{ }\NormalTok{Gr_Liv_Area }\OperatorTok{:}\StringTok{ }\NormalTok{Year_Built, }
   \DataTypeTok{data =}\NormalTok{ ames_train)}
\CommentTok{## }
\CommentTok{## Call:}
\CommentTok{## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, }
\CommentTok{##     data = ames_train)}
\CommentTok{## }
\CommentTok{## Coefficients:}
\CommentTok{##            (Intercept)             Gr_Liv_Area  }
\CommentTok{##              30353.414               -1243.040  }
\CommentTok{##             Year_Built  Gr_Liv_Area:Year_Built  }
\CommentTok{##                  0.149                   0.681}
\end{Highlighting}
\end{Shaded}

A contour plot of the fitted regression surface with interaction is displayed in the right side of Figure \ref{fig:04-mlr-fit}. Note the curvature in the contour lines.

\begin{note}
Interaction effects are quite prevelant in predictive modeling. Since
linear models are an example of parametric modeling, it is up to the
analyst to decide if and when to include interaction effects. In later
chapters, we'll discuss algorithms that can automatically detect and
incorporate interaction effects (albeit in different ways). It is also
important to understand a concept called the \emph{hierarchy
principle}---which demands that all lower-order terms corresponding to
an interaction be retained in the model---when concidering interaction
effects in linear regression models.
\end{note}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/04-mlr-fit-1} 

}

\caption{In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The 'best-fit' plane minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (plane).}\label{fig:04-mlr-fit}
\end{figure}

In general, we can include as many predictors as we want, as long as we have more rows than parameters! The general multiple linear regression model with \emph{p} distinct predictors is

\begin{equation}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon,
\end{equation}

where \(X_i\) for \(i = 1, 2, \dots, p\) are the predictors of interest. Note some of these may represent interactions (e.g., \(X_3 = X_1 \times X_2\)) between or transformations\footnote{Transformations of the features serve a number of purposes (e.g., modeling nonlinear relationships or alleviating departures from common regression assumptions). See \citet{kutner-2005-applied} for details.} (e.g., \(X_4 = \sqrt{X_1}\)) of the original features. Unfortunately, visualizing beyond three dimensions is not practical as our best-fit plane becomes a hyperplane. However, the motivation remains the same where the best-fit hyperplane is identified by minimizing the RSS. The code below creates a third model where we use all features in our data set as main effects (i.e., no interaction terms) to predict \texttt{Sale\_Price}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# include all possible main effects}
\NormalTok{model3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_train) }

\CommentTok{# print estimated coefficients in a tidy data frame}
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(model3)  }
\CommentTok{## # A tibble: 292 x 5}
\CommentTok{##    term            estimate std.error statistic p.value}
\CommentTok{##    <chr>              <dbl>     <dbl>     <dbl>   <dbl>}
\CommentTok{##  1 (Intercept)      -1.20e7 10949313.    -1.09    0.274}
\CommentTok{##  2 MS_SubClassOne~   3.37e3     3655.     0.921   0.357}
\CommentTok{##  3 MS_SubClassOne~   1.21e4    11926.     1.02    0.309}
\CommentTok{##  4 MS_SubClassOne~   1.16e4    12833.     0.902   0.367}
\CommentTok{##  5 MS_SubClassOne~   6.67e3     6552.     1.02    0.309}
\CommentTok{##  6 MS_SubClassTwo~  -1.81e3     6018.    -0.301   0.763}
\CommentTok{##  7 MS_SubClassTwo~   1.02e4     6612.     1.54    0.124}
\CommentTok{##  8 MS_SubClassTwo~  -1.62e4    10468.    -1.54    0.123}
\CommentTok{##  9 MS_SubClassSpl~  -1.03e4    11585.    -0.888   0.375}
\CommentTok{## 10 MS_SubClassSpl~  -3.13e3     7577.    -0.413   0.680}
\CommentTok{## # ... with 282 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{assessing-model-accuracy}{%
\section{Assessing model accuracy}\label{assessing-model-accuracy}}

We've fit three main effects models to the Ames housing data: a single predictor, two predictors, and all possible predictors. But the question remains, which model is ``best''? To answer this question we have to define what we mean by ``best''. In our case, we'll use the RMSE metric and cross-validation (Section \ref{resampling}) to determine the ``best'' model. We can use the \texttt{caret::train()} function to train a linear model (i.e., \texttt{method\ =\ "lm"}) using cross-validation (or a variety of other validation methods). In practice, a number of factors should be considered in determining a ``best'' model (e.g., time constraints, model production cost, predictive accuracy, etc.). The benefit of \textbf{caret} is that it provides built-in cross-validation capabilities, whereas the \texttt{lm()} function does not\footnote{Although general cross-validation is not avilable in \texttt{lm()} alone, a simple metric called the \emph{PRESS} statistic, for \textbf{PRE}dictive \textbf{S}um of \textbf{S}quare, (equivalent to a \emph{leave-one-out} cross-validated RMSE) can be computed by summing the PRESS residuals which are available using \texttt{rstandard(\textless{}lm-model-name\textgreater{},\ type\ =\ "predictive")}. See \texttt{?rstandard} for details.}. The following code chunk uses \texttt{caret::train()} to refit \texttt{model1} using 10-fold cross-validation:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Use caret package to train model using 10-fold cross-validation}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)  }\CommentTok{# for reproducibility}
\NormalTok{(cv_model1 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
  \DataTypeTok{form =}\NormalTok{ Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Gr_Liv_Area, }
  \DataTypeTok{data =}\NormalTok{ ames_train, }
  \DataTypeTok{method =} \StringTok{"lm"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{))}
\CommentTok{## Linear Regression }
\CommentTok{## }
\CommentTok{## 2054 samples}
\CommentTok{##    1 predictor}
\CommentTok{## }
\CommentTok{## No pre-processing}
\CommentTok{## Resampling: Cross-Validated (10 fold) }
\CommentTok{## Summary of sample sizes: 1848, 1847, 1848, 1849, 1848, 1850, ... }
\CommentTok{## Resampling results:}
\CommentTok{## }
\CommentTok{##   RMSE   Rsquared  MAE  }
\CommentTok{##   55670  0.5214    38380}
\CommentTok{## }
\CommentTok{## Tuning parameter 'intercept' was held constant at}
\CommentTok{##  a value of TRUE}
\end{Highlighting}
\end{Shaded}

The resulting cross-validated RMSE is \$55,670.37 (this is the average RMSE across the 10 CV folds). How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about \$55,670.37 off from the actual sale price.

We can perform cross-validation on the other two models in a similar fashion, which we do in the code chunk below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# model 2 CV}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model2 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Gr_Liv_Area }\OperatorTok{+}\StringTok{ }\NormalTok{Year_Built, }
  \DataTypeTok{data =}\NormalTok{ ames_train, }
  \DataTypeTok{method =} \StringTok{"lm"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{)}

\CommentTok{# model 3 CV}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model3 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ ames_train, }
  \DataTypeTok{method =} \StringTok{"lm"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{)}

\CommentTok{# Extract out of sample performance measures}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{model1 =}\NormalTok{ cv_model1, }
  \DataTypeTok{model2 =}\NormalTok{ cv_model2, }
  \DataTypeTok{model3 =}\NormalTok{ cv_model3}
\NormalTok{)))}
\CommentTok{## }
\CommentTok{## Call:}
\CommentTok{## summary.resamples(object = resamples(list(model1}
\CommentTok{##  = cv_model1, model2 = cv_model2, model3 = cv_model3)))}
\CommentTok{## }
\CommentTok{## Models: model1, model2, model3 }
\CommentTok{## Number of resamples: 10 }
\CommentTok{## }
\CommentTok{## MAE }
\CommentTok{##         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{## model1 36295   36806  37005 38380   40034 42096    0}
\CommentTok{## model2 28076   30690  31325 31479   32620 34536    0}
\CommentTok{## model3 14257   15855  16131 17080   16689 25677    0}
\CommentTok{## }
\CommentTok{## RMSE }
\CommentTok{##         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{## model1 50003   52413  54193 55670   60344 62415    0}
\CommentTok{## model2 40456   42957  45597 46133   49114 53745    0}
\CommentTok{## model3 20945   25674  33769 37304   42967 80339    0}
\CommentTok{## }
\CommentTok{## Rsquared }
\CommentTok{##          Min. 1st Qu. Median   Mean 3rd Qu.   Max.}
\CommentTok{## model1 0.3805  0.4832 0.5277 0.5214  0.5883 0.6403}
\CommentTok{## model2 0.5304  0.6666 0.6860 0.6716  0.7084 0.7371}
\CommentTok{## model3 0.4204  0.7462 0.8287 0.7967  0.9099 0.9224}
\CommentTok{##        NA's}
\CommentTok{## model1    0}
\CommentTok{## model2    0}
\CommentTok{## model3    0}
\end{Highlighting}
\end{Shaded}

Extracting the results for each model, we see that by adding more information via more predictors, we are able to improve the out-of-sample cross validation performance metrics. Specifically, our cross-validated RMSE reduces from \$46,132.74 (the model with two predictors) down to \$37,304.33 (for our full model). In this case, the model with all possible main effects performs the ``best'' (compared with the other two).

\hypertarget{lm-residuals}{%
\section{Model concerns}\label{lm-residuals}}

As previously stated, linear regression has been a popular modeling tool due to the ease of interpreting the coefficients. However, linear regression makes several strong assumptions that are often violated as we include more predictors in our model. Violation of these assumptions can lead to flawed interpretation of the coefficients and prediction results.

\textbf{1. Linear relationship:} Linear regression assumes a linear relationship between the predictor and the response variable. However, as discussed in Chapter \ref{engineering}, non-linear relationships can be made linear (or near-linear) by applying transformations to the response and/or predictors. For example, Figure \ref{fig:04-linear-relationship} illustrates the relationship between sale price and the year a home was built. The left plot illustrates the non-linear relationship that exists. However, we can achieve a near-linear relationship by log transforming sale price; although some non-linearity still exists for older homes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames_train, }\KeywordTok{aes}\NormalTok{(Year_Built, Sale_Price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\StringTok{"Sale price"}\NormalTok{, }\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{dollar) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Year built"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Non-transformed variables with a }\CharTok{\textbackslash{}n}\StringTok{non-linear relationship."}\NormalTok{)}

\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames_train, }\KeywordTok{aes}\NormalTok{(Year_Built, Sale_Price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_log10}\NormalTok{(}\StringTok{"Sale price"}\NormalTok{, }\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{dollar, }
                \DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{400000}\NormalTok{, }\DataTypeTok{by =} \DecValTok{100000}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Year built"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Transforming variables can provide a }\CharTok{\textbackslash{}n}\StringTok{near-linear relationship."}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/04-linear-relationship-1} 

}

\caption{Linear regression assumes a linear relationship between the predictor(s) and the response variable; however, non-linear relationships can often be altered to be near-linear by appying a transformation to the variable(s).}\label{fig:04-linear-relationship}
\end{figure}

\textbf{2. Constant variance among residuals:}\index{constant variance} Linear regression assumes the variance among error terms (\(\epsilon_1, \epsilon_2, \dots, \epsilon_p\)) are constant (this assumption is referred to as homoscedasticity). If the error variance is not constant, the \emph{p}-values and confidence intervals for the coefficients will be invalid. Similar to the linear relationship assumption, non-constant variance can often be resolved with variable transformations or by including additional predictors. For example, Figure \ref{fig:04-homoskedasticity} shows the residuals vs.~predicted values for \texttt{model1} and \texttt{model3}. \texttt{model1} displays a classic violation of constant variance as indicated by the cone-shaped pattern. However, \texttt{model3} appears to have near-constant variance.

\begin{tip}
The \texttt{broom::augment} function is an easy way to add model results
to each observation (i.e.~predicted values, residuals).
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{df1 <-}\StringTok{ }\NormalTok{broom}\OperatorTok{::}\KeywordTok{augment}\NormalTok{(cv_model1}\OperatorTok{$}\NormalTok{finalModel, }\DataTypeTok{data =}\NormalTok{ ames_train)}

\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(df1, }\KeywordTok{aes}\NormalTok{(.fitted, .resid)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Predicted values"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Residuals"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Model 1"}\NormalTok{, }\DataTypeTok{subtitle =} \StringTok{"Sale_Price ~ Gr_Liv_Area"}\NormalTok{)}

\NormalTok{df2 <-}\StringTok{ }\NormalTok{broom}\OperatorTok{::}\KeywordTok{augment}\NormalTok{(cv_model3}\OperatorTok{$}\NormalTok{finalModel, }\DataTypeTok{data =}\NormalTok{ ames_train)}

\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(df2, }\KeywordTok{aes}\NormalTok{(.fitted, .resid)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{)  }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Predicted values"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Residuals"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Model 3"}\NormalTok{, }\DataTypeTok{subtitle =} \StringTok{"Sale_Price ~ ."}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/04-homoskedasticity-1} 

}

\caption{Linear regression assumes constant variance among the residuals. `model1` (left) shows definitive signs of heteroskedasticity whereas `model3` (right) appears to have constant variance.}\label{fig:04-homoskedasticity}
\end{figure}

\textbf{3. No autocorrelation:}\index{autocorrelation} Linear regression assumes the errors are independent and uncorrelated. If in fact, there is correlation among the errors, then the estimated standard errors of the coefficients will be biased leading to prediction intervals being narrower than they should be. For example, the left plot in Figure \ref{fig:04-autocorrelation} displays the residuals (\(y\)-axis) vs.~the observation ID (\(x\)-axis) for \texttt{model1}. A clear pattern exists suggesting that information about \(\epsilon_1\) provides information about \(\epsilon_2\).

This pattern is a result of the data being ordered by neighborhood, which we have not accounted for in this model. Consequently, the residuals for homes in the same neighborhood are correlated (homes within a neighborhood are typically the same size and can often contain similar features). Since the \texttt{Neighborhood} predictor is included in \texttt{model3} (right plot), the correlation in the errors is reduced.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{df1 <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(df1, }\DataTypeTok{id =} \KeywordTok{row_number}\NormalTok{())}
\NormalTok{df2 <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(df2, }\DataTypeTok{id =} \KeywordTok{row_number}\NormalTok{())}

\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(df1, }\KeywordTok{aes}\NormalTok{(id, .resid)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Row ID"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Residuals"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Model 1"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Correlated residuals."}\NormalTok{)}

\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(df2, }\KeywordTok{aes}\NormalTok{(id, .resid)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Row ID"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Residuals"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Model 3"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Uncorrelated residuals."}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/04-autocorrelation-1} 

}

\caption{Linear regression assumes uncorrelated errors. The residuals in `model1` (left) have a distinct pattern suggesting that information about $\epsilon_1$ provides information about $\epsilon_2$. Whereas `model3` has no signs of autocorrelation.}\label{fig:04-autocorrelation}
\end{figure}

\textbf{4. More observations than predictors:} Although not an issue with the Ames housing data, when the number of features exceeds the number of observations (\(p > n\)), the OLS estimates are not obtainable. To resolve this issue an analyst can remove variables one-at-a-time until \(p < n\). Although pre-processing tools can be used to guide this manual approach \citep[43-47]{apm}, it can be cumbersome and prone to errors. In Chapter \ref{regularized-regression} we'll introduce regularized regression which provides an alternative to OLS that can be used when \(p > n\).

\textbf{5. No or little multicollinearity:}\index{multicollinearity} \emph{Collinearity}\index{collinearity} refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the OLS, since it can be difficult to separate out the individual effects of collinear variables on the response. In fact, collinearity can cause predictor variables to appear as statistically insignificant when in fact they are significant. This obviously leads to an inaccurate interpretation of coefficients and makes it difficult to identify influential predictors.

In \texttt{ames}, for example, \texttt{Garage\_Area} and \texttt{Garage\_Cars} are two variables that have a correlation of 0.89 and both variables are strongly related to our response variable (\texttt{Sale\_Price}). Looking at our full model where both of these variables are included, we see that \texttt{Garage\_Cars} is found to be statistically significant but \texttt{Garage\_Area} is not:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit with two strongly correlated variables}
\KeywordTok{summary}\NormalTok{(cv_model3) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(term }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Garage_Area"}\NormalTok{, }\StringTok{"Garage_Cars"}\NormalTok{))}
\CommentTok{## # A tibble: 2 x 5}
\CommentTok{##   term        estimate std.error statistic p.value}
\CommentTok{##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>}
\CommentTok{## 1 Garage_Cars  4962.     1803.        2.75 0.00599}
\CommentTok{## 2 Garage_Area     9.47      5.97      1.58 0.113}
\end{Highlighting}
\end{Shaded}

However, if we refit the full model without \texttt{Garage\_Cars}, the coefficient estimate for \texttt{Garage\_Area} increases two fold and becomes statistically significant.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# model without Garage_Area}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{mod_wo_Garage_Cars <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =} \KeywordTok{select}\NormalTok{(ames_train, }\OperatorTok{-}\NormalTok{Garage_Cars), }
  \DataTypeTok{method =} \StringTok{"lm"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{)}

\KeywordTok{summary}\NormalTok{(mod_wo_Garage_Cars) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(term }\OperatorTok{==}\StringTok{ "Garage_Area"}\NormalTok{)}
\CommentTok{## # A tibble: 1 x 5}
\CommentTok{##   term        estimate std.error statistic      p.value}
\CommentTok{##   <chr>          <dbl>     <dbl>     <dbl>        <dbl>}
\CommentTok{## 1 Garage_Area     21.6      4.02      5.38 0.0000000846}
\end{Highlighting}
\end{Shaded}

This reflects the instability in the linear regression model caused by between-predictor relationships; this instability also gets propagated directly to the model predictions. Considering 16 of our 34 numeric predictors have a medium to strong correlation (Section \ref{pca}), the biased coefficients of these predictors are likely restricting the predictive accuracy of our model. How can we control for this problem? One option is to manually remove the offending predictors (one-at-a-time) until all pariwise correlations are below some pre-determined threshold. However, when the number of predictors is large such as in our case, this becomes tedious. Moreover, multicollinearity can arise when one feature is linearly related to two or more features (which is more difficult to detect\footnote{In such cases we can use a statistic called the \emph{variance inflation factor} which tries to capture how strongly each feature is linearly related to all the others predictors in a model.}). In these cases, manual removal of specific predictors may not be possible. Consequently, the following sections offers two simple extensions of linear regression where dimension reduction is applied prior to performing linear regression. Chapter \ref{regularized-regression} offers a modified regression approach that helps to deal with the problem. And future chapters provide alternative methods that are less effected by multicollinearity.

\hypertarget{PCR}{%
\section{Principal component regression}\label{PCR}}

As mentioned in Section \ref{feature-reduction} and fully discussed in Chapter \ref{pca}, principal components analysis can be used to represent correlated variables with a smaller number of uncorrelated features (called principle components) and the resulting components can be used as predictors in a linear regression model. This two-step process is known as \emph{principal component regression}\index{principal component regression} (PCR) \citep{massy1965principal} and is illustrated in Figure \ref{fig:pcr-steps}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth,height=0.7\textheight]{images/pcr-steps} 

}

\caption{A depiction of the steps involved in performing principal component regression.}\label{fig:pcr-steps}
\end{figure}

Performing PCR with \textbf{caret} is an easy extension from our previous model. We simply specify \texttt{method\ =\ "pcr"} within \texttt{train()} to perform PCA on all our numeric predictors prior to fitting the model. Often, we can greatly improve performance by only using a small subset of all principal components as predictors. Consequently, you can think of the number of principal components as a tuning parameter (see Section \ref{tune-overfit}). The following performs cross-validated PCR with \(1, 2, \dots, 20\) principal components, and Figure \ref{fig:pcr-regression} illustrates the cross-validated RMSE. You can see a significant drop in prediction error from our previous linear models using just five principal components followed by a gradual decrease thereafter. Using 17 principal components corresponds to the lowest RMSE (see \texttt{cv\_model\_pcr} for a comparison of the cross-validated results).

\begin{note}
Note in the below example we use \texttt{preProcess} to remove near-zero
variance features and center/scale the numeric features. We then use
\texttt{method\ =\ "pcr"}. This is equivalent to creating a blueprint as
illustrated in Section 3.8.3 to remove near-zero variance features,
center/scale the numeric features, perform PCA on the nuemric features,
then feeding that blueprint into \texttt{train()} with
\texttt{method\ =\ "lm"}.
\end{note}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# perform 10-fold cross validation on a PCR model tuning the }
\CommentTok{# number of principal components to use as predictors from 1-20}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model_pcr <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ ames_train, }
  \DataTypeTok{method =} \StringTok{"pcr"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{tuneLength =} \DecValTok{20}
\NormalTok{  )}

\CommentTok{# model with lowest RMSE}
\NormalTok{cv_model_pcr}\OperatorTok{$}\NormalTok{bestTune}
\CommentTok{##    ncomp}
\CommentTok{## 19    19}

\CommentTok{# plot cross-validated RMSE}
\KeywordTok{ggplot}\NormalTok{(cv_model_pcr)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/pcr-regression-1} 

}

\caption{The 10-fold cross valdation RMSE obtained using PCR with 1-20 principal components.}\label{fig:pcr-regression}
\end{figure}

By controlling for multicollinearity with PCR, we can experience significant improvement in our predictive accuary compared to the previously obtained linear models (reducing the cross-validated RMSE from about \$37,000 to below \$35,000); however, we still do not improve upon the \emph{k}-nearest neighbor model illustrated in Section \ref{engineering-process-example}. It's important to note that since PCR is a two step process, the PCA step does not consider any aspects of the response when it selects the components. Consequently, the new predictors produced by the PCA step are not designed to maximize the relationship with the response. Instead, it simply seeks to reduce the variability present throughout the predictor space. If that variability happens to be related to the response variability, then PCR has a good chance to identify a predictive relationship, as in our case. If, however, the variability in the predictor space is not related to the variability of the response, then PCR can have difficulty identifying a predictive relationship when one might actually exists (i.e., we may actually experience a decrease in our predictive accuracy). An alternative approach to reduce the impact of multicollinearity is partial least squares.

\hypertarget{partial-least-squares}{%
\section{Partial least squares}\label{partial-least-squares}}

\emph{Partial least squares}\index{partial least squares} (PLS) can be viewed as a supervised dimension reduction procedure \citep{apm}. Similar to PCR, this technique also constructs a set of linear combinations of the inputs for regression, but unlike PCR it uses the response variable to aid the construction of the principal components as illustrated in Figure \ref{fig:pcr-vs-pls}\footnote{Figure \ref{fig:pcr-vs-pls} was inspired by, and modified from, Chapter 6 in \citet{apm}.}. Thus, we can think of PLS as a supervised dimension reduction procedure that finds new features that not only captures most of the information in the original features, but also are related to the response.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/pls-vs-pcr} 

}

\caption{A diagram depicting the differences between PCR (left) and PLS (right). PCR finds principal components (PCs) that maximally summarize the features independent of the response variable and then uses those PCs as predictor variables. PLS finds components that simultaneously summarize variation of the predictors while being optimally correlated with the outcome and then uses those PCs as predictors.}\label{fig:pcr-vs-pls}
\end{figure}

We illustrate PLS with some exemplar data\footnote{This is actually using the solubility data that is provided by the \textbf{AppliedPredictiveModeling} package \citep{R-apm}}. Figure \ref{fig:pls-vs-pcr-relationship} illustrates that the first two PCs when using PCR have very little relationship to the response variable; however, the first two PCs when using PLS have a much stronger association to the response.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/pls-vs-pcr-relationship-1} 

}

\caption{Illustration showing that the first two PCs when using PCR have very little relationship to the response variable (top row); however, the first two PCs when using PLS have a much stronger association to the response (bottom row).}\label{fig:pls-vs-pcr-relationship}
\end{figure}

Referring to Equation \eqref{eq:pca1} in Chapter \ref{pca}, PLS will compute the first principal (\(z_1\)) by setting each \(\phi_{j1}\) to the coefficient from a SLR model of \(y\) onto that respective \(x_j\). One can show that this coefficient is proportional to the correlation between \(y\) and \(x_j\). Hence, in computing \(z_1 = \sum^p_{j=1} \phi_{j1}x_j\), PLS places the highest weight on the variables that are most strongly related to the response.

To compute the second PC (\(z_2\)), we first regress each variable on \(z_1\). The residuals from this regression capture the remaining signal that has not been explained by the first PC. We substitute these residual values for the predictor values in Equation \eqref{eq:pca2} in Chapter \ref{pca}. This process continues until all \(m\) components have been computed and then we use OLS to regress the response on \(z_1, \dots, z_m\).

\begin{note}
See @esl and @geladi1986partial for a thorough discussion of PLS.
\end{note}

Similar to PCR, we can easily fit a PLS model by changing the \texttt{method} argument in \texttt{train()}. As with PCR, the number of principal components to use is a tuning parameter that is determined by the model that maximizes predictive accuracy (minimizes RMSE in this case). The following performs cross-validated PLS with \(1, 2, \dots, 20\) PCs, and Figure \ref{fig:pls-regression} shows the cross-validated RMSEs. You can see a greater drop in prediction error compared to PCR. Using PLS with \(m = 3\) principal components corresponded with the lowest cross-validated RMSE of \$29,970.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# perform 10-fold cross validation on a PLS model tuning the }
\CommentTok{# number of principal components to use as predictors from 1-20}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model_pls <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ ames_train, }
  \DataTypeTok{method =} \StringTok{"pls"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{tuneLength =} \DecValTok{20}
\NormalTok{)}

\CommentTok{# model with lowest RMSE}
\NormalTok{cv_model_pls}\OperatorTok{$}\NormalTok{bestTune}
\CommentTok{##   ncomp}
\CommentTok{## 3     3}

\CommentTok{# plot cross-validated RMSE}
\KeywordTok{ggplot}\NormalTok{(cv_model_pls)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/pls-regression-1} 

}

\caption{The 10-fold cross valdation RMSE obtained using PLS with 1-20 principal components.}\label{fig:pls-regression}
\end{figure}

\hypertarget{lm-model-interp}{%
\section{Feature interpretation}\label{lm-model-interp}}

Once we've found the model that minimizes the predictive accuracy, our next goal is to interpret the model structure. Linear regression models provide a very intuitive model structure as they assume a \emph{monotonic linear relationship}\index{monotonic linear relationship} between the predictor variables and the response. The \emph{linear} relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. As discussed earlier in the chapter, this constant rate of change is provided by the coefficient for a predictor. The \emph{monotonic} relationship means that a given predictor variable will always have a positive or negative relationship. But how do we determine the most influential variables?

Variable importance seeks to identify those variables that are most influential in our model. For linear regression models, this is most often measured by the absolute value of the \emph{t}-statistic for each model parameter used; though simple, the results can be hard to interpret when the model includes interaction effects and complex transformations (in Chapter \ref{iml} we'll discuss \emph{model-agnostic}\index{model-agnostic} approaches that don't have this issue). For a PLS model, variable importance can be computed using the weighted sums of the absolute regression coefficients. The weights are a function of the reduction of the RSS across the number of PLS components and are computed separately for each outcome. Therefore, the contribution of the coefficients are weighted proportionally to the reduction in the RSS.

We can use \texttt{vip::vip()} to extract and plot the most important variables. The importance measure is normalized from 100 (most important) to 0 (least important). Figure \ref{fig:pls-vip} illustrates that the top 4 most important variables are \texttt{Gr\_liv\_Area}, \texttt{First\_Flr\_SF}, \texttt{Total\_Bsmt\_SF}, and \texttt{Garage\_Cars} respectively.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vip}\NormalTok{(cv_model_pls, }\DataTypeTok{num_features =} \DecValTok{20}\NormalTok{, }\DataTypeTok{method =} \StringTok{"model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/pls-vip-1} 

}

\caption{Top 20 most important variables for the PLS model.}\label{fig:pls-vip}
\end{figure}

As stated earlier, linear regression models assume a monotonic linear relationship. To illustrate this, we can construct partial dependence plots (PDPs). PDPs plot the change in the average predicted value (\(\widehat{y}\)) as specified feature(s) vary over their marginal distribution. As you will see in later chapters, PDPs become more useful when non-linear relationships are present (we discuss PDPs and other ML interpretation techniques in Chapter \ref{iml}). However, PDPs of linear models help illustrate how a fixed change in \(x_i\) relates to a fixed linear change in \(\widehat{y}_i\) while taking into account the average effect of all the other features in the model (for linear models, the slope of the PDP is equal to the corresponding features LS coefficient).

\begin{tip}
The \textbf{pdp} package {[}@R-pdp{]} provides convenient functions for
computing and plotting PDPs. For example, the following code chunk would
plot the PDP for the \texttt{Gr\_Liv\_Area} predictor.

\texttt{pdp::partial(cv\_model\_pls,\ "Gr\_Liv\_Area",\ grid.resolution\ =\ 20,\ plot\ =\ TRUE)}
\end{tip}

All four of the most important predictors have a positive relationship with sale price; however, we see that the slope (\(\widehat{beta}_i\)) is steepest for the most important predictor and gradually decreases for lessor important variables.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/unnamed-chunk-33-1} 

}

\caption{Partial dependence plots for the first four most important variables.}\label{fig:unnamed-chunk-33}
\end{figure}

\hypertarget{final-thoughts}{%
\section{Final thoughts}\label{final-thoughts}}

Linear regression is usually the first supervised learning algorithm you will learn. The approach provides a solid fundamental understanding of the supervised learning task; however, as we've discussed there are several concerns that result from the assumptions required. Although extensions of linear regression that integrate dimension reduction steps into the algorithm can help address some of the problems with linear regression, more advanced supervised algorithms typically provide greater flexibility and improved accuracy. Nonetheless, understanding linear regression provides a foundation that will serve you well in learning these more advanced methods.

\hypertarget{logistic-regression}{%
\chapter{Logistic Regression}\label{logistic-regression}}

Linear regression is used to approximate the relationship between a continuous response variable and a set of predictor variables. However, when the response variable is binary (i.e., Yes/No), linear regression is not appropriate. Fortunately, analysts can turn to an analogous method, \emph{logistic regression}\index{logistic regression}, which is similar to linear regression in many ways. This chapter explores the use of logistic regression for binary response variables. Logistic regression can be expanded for multinomial problems (see \citet{faraway2016extending} for discussion of multinomial logistic regression in R); however, that goes beyond our intent here.

\hypertarget{prerequisites-3}{%
\section{Prerequisites}\label{prerequisites-3}}

For this section we'll use the following packages:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Helper packages}
\KeywordTok{library}\NormalTok{(dplyr)     }\CommentTok{# for data wrangling}
\KeywordTok{library}\NormalTok{(ggplot2)   }\CommentTok{# for awesome plotting}
\KeywordTok{library}\NormalTok{(rsample)   }\CommentTok{# for data splitting}

\CommentTok{# Modeling packages}
\KeywordTok{library}\NormalTok{(caret)     }\CommentTok{# for logistic regression modeling}

\CommentTok{# Model interpretability packages}
\KeywordTok{library}\NormalTok{(vip)       }\CommentTok{# variable importance}
\end{Highlighting}
\end{Shaded}

To illustrate logistic regression concepts we'll use the employee attrition data, where our intent is to predict the \texttt{Attrition} response variable (coded as \texttt{"Yes"}/\texttt{"No"}). As in the previous chapter, we'll set aside 30\% of our data as a test set to assess our generalizability error.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\NormalTok{attrition }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_if}\NormalTok{(is.ordered, factor, }\DataTypeTok{ordered =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Create training (70%) and test (30%) sets for the rsample::attrition data.}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)  }\CommentTok{# for reproducibility}
\NormalTok{churn_split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(df, }\DataTypeTok{prop =} \FloatTok{.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Attrition"}\NormalTok{)}
\NormalTok{churn_train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(churn_split)}
\NormalTok{churn_test  <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(churn_split)}
\end{Highlighting}
\end{Shaded}

\hypertarget{why-logistic-regression}{%
\section{Why logistic regression}\label{why-logistic-regression}}

To provide a clear motivation of logistic regression, assume we have credit card default data for customers and we want to understand if the current credit card balance of a customer is an indicator of whether or not they'll default on their credit card. To classify a customer as a high- vs.~low-risk defaulter based on their balance we could use linear regression; however, the left plot in Figure \ref{fig:whylogit} illustrates how linear regression would predict the probability of defaulting. Unfortunately, for balances close to zero we predict a negative probability of defaulting; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of defaulting, regardless of credit card balance, must fall between 0 and 1. These inconsistencies only increase as our data become more imbalanced and the number of outliers increase. Contrast this with the logistic regression line (right plot) that is nonlinear (sigmoidal-shaped).

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/whylogit-1} 

}

\caption{Comparing the predicted probabilities of linear regression (left) to logistic regression (right). Predicted probabilities using linear regression results in flawed logic whereas predicted values from logistic regression will always lie between 0 and 1.}\label{fig:whylogit}
\end{figure}

To avoid the inadequecies of the linear model fit on a binary response, we must model the probability of our response using a function that gives outputs between 0 and 1 for all values of \(X\). Many functions meet this description. In logistic regression, we use the logistic function, which is defined in Equation \eqref{eq:logistic} and produces the S-shaped curve in the right plot above.

\begin{equation}
\label{eq:logistic}
  p\left(X\right) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}
\end{equation}

The \(\beta_i\) parameters represent the coefficients as in linear regression and \(p\left(X\right)\) may be interpreted as the probability that the positive class (default in the above example) is present. The minimum for \(p\left(x\right)\) is obtained at \(\lim_{a \rightarrow -\infty} \left[ \frac{e^a}{1+e^a} \right] = 0\), and the maximium for \(p\left(x\right)\) is obtained at \(\lim_{a \rightarrow \infty} \left[ \frac{e^a}{1+e^a} \right] = 1\) which restricts the output probabilities to 0--1. Rearranging Equation \eqref{eq:logistic} yields the \emph{logit transformation}\index{logit transformation} (which is where logistic regression gets its name):

\begin{equation}
\label{eq:logit}
  g\left(X\right) = \ln \left[ \frac{p\left(X\right)}{1 - p\left(X\right)} \right] = \beta_0 + \beta_1 X
\end{equation}

Applying a logit transformation to \(p\left(X\right)\) results in a linear equation similar to the mean response in a simple linear regression model. Using the logit transformation also results in an intuitive interpretation for the magnitude of \(\beta_1\): the odds (e.g., of defaulting) increase multiplicatively by \(\exp\left(\beta_1\right)\) for every one-unit increase in \(X\). A similar interpretation exists if \(X\) is categorical; see \citet{agresti2003categorical}, Chapter 5, for details.

\hypertarget{simple-logistic-regression}{%
\section{Simple logistic regression}\label{simple-logistic-regression}}

We will fit two logistic regression models in order to predict the probability of an employee attriting. The first predicts the probability of attrition based on their monthly income (\texttt{MonthlyIncome}) and the second is based on whether or not the employee works overtime (\texttt{OverTime}). The \texttt{glm()} function fits generalized linear models, a class of models that includes both logistic regression and simple linear regression as special cases. The syntax of the \texttt{glm()} function is similar to that of \texttt{lm()}, except that we must pass the argument \texttt{family\ =\ "binomial"} in order to tell R to run a logistic regression rather than some other type of generalized linear model (the default is \texttt{family\ =\ "gaussian"}, which is equivalent to ordinary linear regression assuming normally distributed errors).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model1 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{MonthlyIncome, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }
              \DataTypeTok{data =}\NormalTok{ churn_train)}
\NormalTok{model2 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{OverTime, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }
              \DataTypeTok{data =}\NormalTok{ churn_train)}
\end{Highlighting}
\end{Shaded}

In the background \texttt{glm()}, uses ML estimation to estimate the unknown model parameters. The basic intuition behind using ML estimation to fit a logistic regression model is as follows: we seek estimates for \(\beta_0\) and \(\beta_1\) such that the predicted probability \(\widehat p\left(X_i\right)\) of attrition for each employee corresponds as closely as possible to the employee's observed attrition status. In other words, we try to find \(\widehat \beta_0\) and \(\widehat \beta_1\) such that plugging these estimates into the model for \(p\left(X\right)\) (Equation \eqref{eq:logistic}) yields a number close to one for all employees who attrited, and a number close to zero for all employees who did not. This intuition can be formalized using a mathematical equation called a \emph{likelihood function}\index{likelihood function}:

\begin{equation}
\label{eq:max-like} 
  \ell\left(\beta_0, \beta_1\right) = \prod_{i:y_i=1}p\left(X_i\right) \prod_{i':y_i'=0}\left[1-p\left(x_i'\right)\right]
\end{equation}

The estimates \(\widehat \beta_0\) and \(\widehat \beta_1\) are chosen to \emph{maximize} this likelihood function. What results is the predicted probability of attrition. Figure \ref{fig:glm-sigmoid} illustrates the predicted probablities for the two models.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/glm-sigmoid-1} 

}

\caption{Predicted probablilities of employee attrition based on monthly income (left) and overtime (right). As monthly income increases, `model1` predicts a decreased probability of attrition and if employees work overtime `model2` predicts an increased probability.}\label{fig:glm-sigmoid}
\end{figure}

The table below shows the coefficient estimates and related information that result from fitting a logistic regression model in order to predict the probability of \emph{Attrition = Yes} for our two models. Bear in mind that the coefficient estimates from logistic regression characterize the relationship between the predictor and response variable on a \emph{log-odds}\index{log-odds} (i.e., logit) scale.

For \texttt{model1}, the estimated coefficient for \texttt{MonthlyIncome} is \(\widehat \beta_1 =\) -0.000130, which is negative, indicating that an increase in \texttt{MonthlyIncome} is associated with a decrease in the probability of attrition. Similarly, for \texttt{model2}, employees that work \texttt{OverTime} are associated with an increased probability of attrition compared to those that do not work \texttt{OverTime}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tidy}\NormalTok{(model1)}
\CommentTok{## # A tibble: 2 x 5}
\CommentTok{##   term         estimate std.error statistic     p.value}
\CommentTok{##   <chr>           <dbl>     <dbl>     <dbl>       <dbl>}
\CommentTok{## 1 (Intercept) -0.924    0.155         -5.96     2.59e-9}
\CommentTok{## 2 MonthlyInc~ -0.000130 0.0000264     -4.93     8.36e-7}
\KeywordTok{tidy}\NormalTok{(model2)}
\CommentTok{## # A tibble: 2 x 5}
\CommentTok{##   term        estimate std.error statistic  p.value}
\CommentTok{##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>}
\CommentTok{## 1 (Intercept)    -2.18     0.122    -17.9  6.76e-72}
\CommentTok{## 2 OverTimeYes     1.41     0.176      8.00 1.20e-15}
\end{Highlighting}
\end{Shaded}

As discussed earlier, it is easier to interpret the coefficients using an \(\exp()\) transformation:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(model1))}
\CommentTok{##   (Intercept) MonthlyIncome }
\CommentTok{##        0.3971        0.9999}
\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(model2))}
\CommentTok{## (Intercept) OverTimeYes }
\CommentTok{##      0.1126      4.0812}
\end{Highlighting}
\end{Shaded}

Thus, the odds of an employee attriting in \texttt{model1} increase multiplicatively by 0.9999 for every one dollar increase in \texttt{MonthlyIncome}, whereas the odds of attriting in \texttt{model2} increase multiplicatively by 4.0812 for employees that work \texttt{OverTime} compared to those that do not.

Many aspects of the logistic regression output are similar to those discussed for linear regression. For example, we can use the estimated standard errors to get confidence intervals as we did for linear regression in Chapter \ref{linear-regression}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(model1)  }\CommentTok{# for odds, you can use `exp(confint(model1))`}
\CommentTok{##                   2.5 %      97.5 %}
\CommentTok{## (Intercept)   -1.226775 -0.61800619}
\CommentTok{## MonthlyIncome -0.000185 -0.00008108}
\KeywordTok{confint}\NormalTok{(model2)}
\CommentTok{##              2.5 % 97.5 %}
\CommentTok{## (Intercept) -2.430 -1.952}
\CommentTok{## OverTimeYes  1.063  1.753}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-logistic-regression}{%
\section{Multiple logistic regression}\label{multiple-logistic-regression}}

We can also extend our model as seen in Equation 1 so that we can predict a binary response using multiple predictors:

\begin{equation}
\label{eq:multi-logistic}
p\left(X\right) = \frac{e^{\beta_0 + \beta_1 X + \cdots + \beta_p X_p }}{1 + e^{\beta_0 + \beta_1 X + \cdots + \beta_p X_p}} 
\end{equation}

Let's go ahead and fit a model that predicts the probability of \texttt{Attrition} based on the \texttt{MonthlyIncome} and \texttt{OverTime}. Our results show that both features are statistically significant (at the 0.05 level) and Figure \ref{fig:glm-sigmoid2} illustrates common trends between \texttt{MonthlyIncome} and \texttt{Attrition}; however, working \texttt{OverTime} tends to nearly double the probability of attrition.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model3 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{MonthlyIncome }\OperatorTok{+}\StringTok{ }\NormalTok{OverTime,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }
  \DataTypeTok{data =}\NormalTok{ churn_train}
\NormalTok{  )}

\KeywordTok{tidy}\NormalTok{(model3)}
\CommentTok{## # A tibble: 3 x 5}
\CommentTok{##   term           estimate std.error statistic  p.value}
\CommentTok{##   <chr>             <dbl>     <dbl>     <dbl>    <dbl>}
\CommentTok{## 1 (Intercept)   -1.43     0.176         -8.11 5.25e-16}
\CommentTok{## 2 MonthlyIncome -0.000139 0.0000270     -5.15 2.62e- 7}
\CommentTok{## 3 OverTimeYes    1.47     0.180          8.16 3.43e-16}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/glm-sigmoid2-1} 

}

\caption{Predicted probability of attrition based on monthly income and whether or not employees work overtime.}\label{fig:glm-sigmoid2}
\end{figure}

\hypertarget{assessing-model-accuracy-1}{%
\section{Assessing model accuracy}\label{assessing-model-accuracy-1}}

With a basic understanding of logistic regression under our belt, similar to linear regression our concern now shifts to how well do our models predict. As in the last chapter, we'll use \texttt{caret::train()} and fit three 10-fold cross validated logistic regression models. Extracting the accuracy measures (in this case, classification accuracy), we see that both \texttt{cv\_model1} and \texttt{cv\_model2} had an average accuracy of 83.88\%. However, \texttt{cv\_model3} which used all predictor variables in our data achieved an average accuracy rate of 87.58\%.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model1 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{MonthlyIncome, }
  \DataTypeTok{data =}\NormalTok{ churn_train, }
  \DataTypeTok{method =} \StringTok{"glm"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model2 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{MonthlyIncome }\OperatorTok{+}\StringTok{ }\NormalTok{OverTime, }
  \DataTypeTok{data =}\NormalTok{ churn_train, }
  \DataTypeTok{method =} \StringTok{"glm"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model3 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ churn_train, }
  \DataTypeTok{method =} \StringTok{"glm"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{)}

\CommentTok{# extract out of sample performance measures}
\KeywordTok{summary}\NormalTok{(}
  \KeywordTok{resamples}\NormalTok{(}
    \KeywordTok{list}\NormalTok{(}
      \DataTypeTok{model1 =}\NormalTok{ cv_model1, }
      \DataTypeTok{model2 =}\NormalTok{ cv_model2, }
      \DataTypeTok{model3 =}\NormalTok{ cv_model3}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{)}\OperatorTok{$}\NormalTok{statistics}\OperatorTok{$}\NormalTok{Accuracy}
\CommentTok{##          Min. 1st Qu. Median   Mean 3rd Qu.   Max.}
\CommentTok{## model1 0.8350  0.8350 0.8365 0.8388  0.8431 0.8447}
\CommentTok{## model2 0.8350  0.8350 0.8365 0.8388  0.8431 0.8447}
\CommentTok{## model3 0.8365  0.8495 0.8792 0.8758  0.8908 0.9314}
\CommentTok{##        NA's}
\CommentTok{## model1    0}
\CommentTok{## model2    0}
\CommentTok{## model3    0}
\end{Highlighting}
\end{Shaded}

We can get a better understanding of our model's performance by assessing the confusion matrix (see section \ref{model-eval}). We can use \texttt{train::confusionMatrix()} to compute a confusion matrix. We need to supply our model's predicted class and the actuals from our training data. The confusion matrix provides a wealth of information. Particularly, we can see that although we do well predicting cases of non-attrition (note the high specificity), our model does particularly poor predicting actual cases of attrition (note the low sensitivity).

\begin{tip}
By default the \texttt{predict()} function predicts the response class
for a \textbf{caret} model; however, you can change the \texttt{type}
argument to predict the probabilities (see \texttt{?predict.train}).
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predict class}
\NormalTok{pred_class <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(cv_model3, churn_train)}

\CommentTok{# create confusion matrix}
\KeywordTok{confusionMatrix}\NormalTok{(}
  \DataTypeTok{data =} \KeywordTok{relevel}\NormalTok{(pred_class, }\DataTypeTok{ref =} \StringTok{"Yes"}\NormalTok{), }
  \DataTypeTok{reference =} \KeywordTok{relevel}\NormalTok{(churn_train}\OperatorTok{$}\NormalTok{Attrition, }\DataTypeTok{ref =} \StringTok{"Yes"}\NormalTok{)}
\NormalTok{)}
\CommentTok{## Confusion Matrix and Statistics}
\CommentTok{## }
\CommentTok{##           Reference}
\CommentTok{## Prediction Yes  No}
\CommentTok{##        Yes  93  25}
\CommentTok{##        No   73 839}
\CommentTok{##                                         }
\CommentTok{##                Accuracy : 0.905         }
\CommentTok{##                  95% CI : (0.885, 0.922)}
\CommentTok{##     No Information Rate : 0.839         }
\CommentTok{##     P-Value [Acc > NIR] : 0.000000000536}
\CommentTok{##                                         }
\CommentTok{##                   Kappa : 0.602         }
\CommentTok{##                                         }
\CommentTok{##  Mcnemar's Test P-Value : 0.000002057257}
\CommentTok{##                                         }
\CommentTok{##             Sensitivity : 0.5602        }
\CommentTok{##             Specificity : 0.9711        }
\CommentTok{##          Pos Pred Value : 0.7881        }
\CommentTok{##          Neg Pred Value : 0.9200        }
\CommentTok{##              Prevalence : 0.1612        }
\CommentTok{##          Detection Rate : 0.0903        }
\CommentTok{##    Detection Prevalence : 0.1146        }
\CommentTok{##       Balanced Accuracy : 0.7657        }
\CommentTok{##                                         }
\CommentTok{##        'Positive' Class : Yes           }
\CommentTok{## }
\end{Highlighting}
\end{Shaded}

One thing to point out, in the confusion matrix above you will note the metric \texttt{No\ Information\ Rate:\ 0.839}. This represents the ratio of non-attrition vs.~attrition in our training data (\texttt{table(churn\_train\$Attrition)\ \%\textgreater{}\%\ prop.table()}). Consequently, if we simply predicted \texttt{"No"} for every employee we would still get an accuracy rate of 83.9\%. Therefore, our goal is to maximize our accuracy rate over and above this no information baseline while also trying to balance sensitivity and specificity. To that end, we plot the ROC curve (section \ref{model-eval}) which is displayed in Figure \ref{fig:roc}. If we compare our simple model (\texttt{cv\_model1}) to our full model (\texttt{cv\_model3}), we see the lift achieved with the more accurate model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ROCR)}

\CommentTok{# Compute predicted probabilities}
\NormalTok{m1_prob <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(cv_model1, churn_train, }\DataTypeTok{type =} \StringTok{"prob"}\NormalTok{)}\OperatorTok{$}\NormalTok{Yes}
\NormalTok{m3_prob <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(cv_model3, churn_train, }\DataTypeTok{type =} \StringTok{"prob"}\NormalTok{)}\OperatorTok{$}\NormalTok{Yes}

\CommentTok{# Compute AUC metrics for cv_model1 and cv_model3}
\NormalTok{perf1 <-}\StringTok{ }\KeywordTok{prediction}\NormalTok{(m1_prob, churn_train}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{performance}\NormalTok{(}\DataTypeTok{measure =} \StringTok{"tpr"}\NormalTok{, }\DataTypeTok{x.measure =} \StringTok{"fpr"}\NormalTok{)}
\NormalTok{perf2 <-}\StringTok{ }\KeywordTok{prediction}\NormalTok{(m3_prob, churn_train}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{performance}\NormalTok{(}\DataTypeTok{measure =} \StringTok{"tpr"}\NormalTok{, }\DataTypeTok{x.measure =} \StringTok{"fpr"}\NormalTok{)}

\CommentTok{# Plot ROC curves for cv_model1 and cv_model3}
\KeywordTok{plot}\NormalTok{(perf1, }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(perf2, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"cv_model1"}\NormalTok{, }\StringTok{"cv_model3"}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\DataTypeTok{lty =} \DecValTok{2}\OperatorTok{:}\DecValTok{1}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/logistic-regression-roc-1} 

}

\caption{ROC curve for cross-validated models 1 and 3. The increase in the AUC represents the 'lift' that we achieve with model 3.}\label{fig:logistic-regression-roc}
\end{figure}

Similar to linear regression, we can perform a PLS logistic regression to assess if reducing the dimension of our numeric predictors helps to improve accuracy. There are 16 numeric features in our data set so the following code performs a 10-fold cross-validated PLS model while tuning the number of principal components to use from 1--16. The optimal model uses 14 principal components, which is not reducing the dimension by much. However, the mean accuracy of 0.876 is no better than the average CV accuracy of \texttt{cv\_model3} (0.876).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Perform 10-fold CV on a PLS model tuning the number of PCs to }
\CommentTok{# use as predictors}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model_pls <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ churn_train, }
  \DataTypeTok{method =} \StringTok{"pls"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{tuneLength =} \DecValTok{16}
\NormalTok{)}

\CommentTok{# Model with lowest RMSE}
\NormalTok{cv_model_pls}\OperatorTok{$}\NormalTok{bestTune}
\CommentTok{##    ncomp}
\CommentTok{## 14    14}

\CommentTok{# Plot cross-validated RMSE}
\KeywordTok{ggplot}\NormalTok{(cv_model_pls)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/pls-logistic-regression-1} 

}

\caption{The 10-fold cross validation RMSE obtained using PLS with 1--16 principal components.}\label{fig:pls-logistic-regression}
\end{figure}

\hypertarget{glm-residuals}{%
\section{Model concerns}\label{glm-residuals}}

As with linear models, it is important to check the adequacy of the logistic regression model (in fact, this should be done for all parametric models). This was discussed for linear models in Section \ref{lm-residuals} where the residuals played an important role. Although not as common, residual analysis and diagnostics are equally important to generalized linear models. The problem is that there is no obvious way to define what a residual is for more general models. For instance, how might we define a residual in logistic regression when the outcome is either 0 or 1? Nonetheless attempts have been made and a number of useful diagnostics can be constructed based on the idea of a \emph{pseduo residual}; see, for example, \citet{harrell2015regression}, Section 10.4.

More recently, \citet{dungang2018residuals} introduced the concept of \emph{surrogate residuals} that allows for residual-based diagnostic procedures and plots not unlike those in traditional linear regression (e.g., checking for outliers and misspecified link functions). For an overview with examples in R using the \textbf{sure} package, see \citet{greenwell2018residuals}.

\hypertarget{feature-interpretation}{%
\section{Feature interpretation}\label{feature-interpretation}}

Similar to linear regression, once our preferred logistic regression model is identified, we need to interpret how the features are influencing the results. As with normal linear regression models, variable importance for logistic regression models can be computed using the absolute value of the \(z\)-statistic for each coefficient (albeit with the same issues previously discussed). Using \texttt{vip::vip()} we can extract our top 20 influential variables. Figure \ref{fig:glm-vip} illustrates that \texttt{OverTime} is the most influential followed by \texttt{JobSatisfaction}, and \texttt{EnvironmentSatisfaction}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vip}\NormalTok{(cv_model3, }\DataTypeTok{num_features =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/glm-vip-1} 

}

\caption{Top 20 most important variables for the PLS model.}\label{fig:glm-vip}
\end{figure}

Similar to linear regression, logistic regression assumes a monotonic linear relationship. However, the linear relationship occurrs on the logit scale; on the probability scale, the relationship will be nonlinear. This is illustrated by the PDP in Figure \ref{fig:glm-pdp} which illustrates the functional relationship between the predicted probability of attrition and the number of companies an employee has worked for (\texttt{NumCompaniesWorked}) while taking into account the average effect of all the other predictos in the model. Employees that have experienced more employment changes tend to have a high probability of making another change in the future.

Furthermore, the PDPs for the top three categorical predictors (\texttt{OverTime}, \texttt{JobSatisfaction}, and \texttt{EnvironmentSatisfaction}) illustrate the change in predicted probability of attrition based on the employee's status for each predictor.

\begin{tip}
See the online supplemental material for the code to reproduce the plots
in Figure \ref{fig:glm-pdp}.
\end{tip}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/glm-pdp-1} 

}

\caption{Partial dependence plots for the first four most important variables.  We can see how the predicted probability of attrition changes for each value of the influential predictors.}\label{fig:glm-pdp}
\end{figure}

\hypertarget{final-thoughts-1}{%
\section{Final thoughts}\label{final-thoughts-1}}

Logistic regression provides an alternative to linear regression for binary classification problems. However, similar to linear regression, logistic regression suffers from the many assumptions involved in the algorithm (i.e.~linear relationship of the coefficient, multicollinearity). Moreover, often we have more than two classes to predict which is commonly referred to as multinomial classification. Although multinomial extensions of logistic regression exist, the assumptions made only increase and, often, the stability of the coefficient estimates (and therefore the accuracy) decrease. Future chapters will discuss more advanced algorithms that provide a more natural and trustworthy approach to binary and multinomial classification prediction.

\hypertarget{regularized-regression}{%
\chapter{Regularized Regression}\label{regularized-regression}}

Linear models (LMs) provide a simple, yet effective, approach to predictive modeling. Moreover, when certain assumptions required by LMs are met (e.g., constant variance), the estimated coefficients are unbiased and, of all linear unbiased estimates, have the lowest variance. However, in today's world, data sets being analyzed typically have a large amount of features. As the number of features grow, certain assumptions typically break down and these models tend to overfit the training data, causing our out of sample error to increase. \textbf{Regularization}\index{regularization} methods provide a means to constrain or \emph{regularize} the estimated coefficients, which can reduce the variance and decrease out of sample error.

\hypertarget{prerequisites-4}{%
\section{Prerequisites}\label{prerequisites-4}}

This chapter leverages the following packages. Most of these packages are playing a supporting role while the main emphasis will be on the \textbf{glmnet} package \citep{R-glmnet}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Helper packages}
\KeywordTok{library}\NormalTok{(recipes)  }\CommentTok{# for feature engineering}

\CommentTok{# Modeling packages}
\KeywordTok{library}\NormalTok{(glmnet)   }\CommentTok{# for implementing regularized regression}
\KeywordTok{library}\NormalTok{(caret)    }\CommentTok{# for automating the tuning process}

\CommentTok{# Model interpretability packages}
\KeywordTok{library}\NormalTok{(vip)      }\CommentTok{# for variable importance}
\end{Highlighting}
\end{Shaded}

To illustrate various regularization concepts we'll continue working with the \texttt{ames\_train} and \texttt{ames\_test} data sets created in Section \ref{put-process-together}; however, at the end of the chapter we'll also apply regularized regression to the employee attrition data.

\hypertarget{why}{%
\section{Why regularize?}\label{why}}

The easiest way to understand regularized regression is to explain how and why it is applied to ordinary least squares (OLS). The objective in OLS regression is to find the \emph{hyperplane}\index{hyperplane}\footnote{See Section \ref{hyperplanes} for more discussion on hyperplanes.} (e.g., a straight line in two dimensions) that minimizes the sum of squared errors (SSE) between the observed and predicted response values (see \ref{fig:hyperplane} below). This means identifying the hyperplane that minimizes the grey lines, which measure the vertical distance between the observed (red dots) and predicted (blue line) response values.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/hyperplane-1} 

}

\caption{Fitted regression line using Ordinary Least Squares.}\label{fig:hyperplane}
\end{figure}

More formally, the objective function being minimized can be written as:

\begin{equation}
\label{eq:ols-objective}
\text{minimize} \left( SSE = \sum^n_{i=1} \left(y_i - \hat{y}_i\right)^2 \right)
\end{equation}

As we discussed in Chapter \ref{linear-regression}, the OLS objective function performs quite well when our data adhere to a few key assumptions:

\begin{itemize}
\tightlist
\item
  Linear relationship;
\item
  There are more observations (\emph{n}) than features (\emph{p}) (\(n > p\));
\item
  No or little multicollinearity.
\end{itemize}

\BeginKnitrBlock{note}
For classicial statistical inference procedures (e.g., confidence intervals based on the classic \emph{t}-statistic) to be valid, we also need to make stronger assumptions regarding normality (of the errors) and homoscedasticy (i.e., constant error variance).
\EndKnitrBlock{note}

Many real-life data sets, like those cmmon to \emph{text mining} and \emph{genomic studies} are \emph{wide}, meaning they contain a larger number of features (\(p > n\)). As \emph{p} increases, we're more likely to violate some of the OLS assumptions and alternative approaches should be considered. This was briefly illustrated in Chapter \ref{linear-regression} where the presence of multicollinearity was diminishing the interpretability of our estimated coefficients due to inflated variance. By reducing multicollinearity, we were able to increase our model's accuracy. Of course, multicollinearity can also occur when \(n > p\).

Having a large number of features invites additional issues in using classic regression models. For one, having a large number of features makes the model much less interpretable. Additionally, when \(p > n\), there are many (in fact infinite) solutions to the OLS problem! In such cases, it is useful (and practical) to assume that a smaller subset of the features exhibit the strongest effects (something called the \emph{bet on sparsity principal} \citep[see][p.~2]{hastie2015statistical}.). For this reason, we sometimes prefer estimation techniques that incorporate \emph{feature selection}\index{feature selection}. One approach to this is called \emph{hard threshholding} feature selection, which includes many of the traditional linear model selection approaches like \emph{forward selection} and \emph{backward elimination}. These procedures, however, can be computationally inefficient, do not scale well, and they simply assume a feature as either in or out of the model (hence the name hard thresholding). In contrast, a more modern approach, called \emph{soft threshholding}, slowly pushes the effects of irrelevant features toward zero, and in some cases, will zero out entire coefficients. As will be demonstrated, this can result in more accurate models that are also easier to interpret.

With wide data (or data that exhibits multicollinearity), one alternative to OLS regression is to use regularized regression (also commonly referred to as \emph{penalized} models\index{penalized models} or \emph{shrinkage} methods\index{shrinkage methods} as in \citet{esl} and \citet{apm}) to constrain the total size of all the coefficient estimates. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model (at the expense of no longer being unbiased---a reasonable compromise).

The objective function of a regularized regression model is similar to OLS; albeit, with a penalty term \(P\).

\begin{equation}
\label{eq:penalty}
\text{minimize} \left( SSE + P \right)
\end{equation}

This penalty parameter constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the sum of squared errors (SSE).

This concept generalizes to all GLM models (e.g., logistic and Poisson regression) and even some \emph{survival models}. So far, we have been discussing OLS and the sum of squared errors loss function. However, different models within the GLM family have different loss functions (see Chapter 4 of \citet{esl}). Yet we can think of the penalty parameter all the same---it constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the model's loss function.

There are three common penalty parameters we can implement:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ridge;
\item
  LASSO, or lasso;
\item
  Elastic net, which is a combination of Ridge and Lasso.
\end{enumerate}

\hypertarget{ridge}{%
\subsection{Ridge penalty}\label{ridge}}

Ridge regression\index{ridge penalty} \citep{hoerl1970ridge} controls the estimated coefficients by adding \(\lambda \sum^p_{j=1} \beta_j^2\) to the objective function.

\begin{equation}
\label{eq:ridge-penalty}
\text{minimize } \left( SSE + \lambda \sum^p_{j=1} \beta_j^2 \right)
\end{equation}

The size of this penalty, referred to as \(L^2\) (or Euclidean) norm, can take on a wide range of values, which is controlled by the \emph{tuning parameter} \(\lambda\). When \(\lambda = 0\) there is no effect and our objective function equals the normal OLS regression objective function of simply minimizing SSE. However, as \(\lambda \rightarrow \infty\), the penalty becomes large and forces the coefficients toward zero (but not all the way). This is illustrated in Figure \ref{fig:ridge-coef-example} where exemplar coefficients have been regularized with \(\lambda\) ranging from 0 to over 8,000.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/ridge-coef-example-1} 

}

\caption{Ridge regression coefficients for 15 exemplar predictor variables as $\lambda$ grows from  $0 \rightarrow \infty$. As $\lambda$ grows larger, our coefficient magnitudes are more constrained.}\label{fig:ridge-coef-example}
\end{figure}

Although these coefficients were scaled and centered prior to the analysis, you will notice that some are quite large when \(\lambda\) is near zero. Furthermore, you'll notice that feature \texttt{x1} has a large negative parameter that fluctuates until \(\lambda \approx 7\) where it then continuously skrinks toward zero. This is indicative of multicollinearity and likely illustrates that constraining our coefficients with \(\lambda > 7\) may reduce the variance, and therefore the error, in our predictions.

In essence, the ridge regression model pushes many of the correlated features toward each other rather than allowing for one to be wildly positive and the other wildly negative. In addition, many of the less-important features also get pushed toward zero. This helps to provide clarity in identifying the important signals in our data (i.e., the labeled features in \ref{fig:ridge-coef-example}).

However, ridge regression does not perform feature selection and will retain \textbf{all} available features in the final model. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create (e.g., in smaller data sets with severe multicollinearity). If greater interpretation is necessary and feel that many of the features are redundant or irrelvant then a lasso or elastic net penalty may be preferable.

\hypertarget{lasso}{%
\subsection{Lasso penalty}\label{lasso}}

The \emph{least absolute shrinkage and selection operator} (lasso) penalty\index{Lasso penalty} \citep{tibshirani1996regression} is an alternative to the ridge penalty that requires only a small modification. The only difference is that we swap out the \(L^2\) norm for an \(L^1\) norm: \(\lambda \sum^p_{j=1} | \beta_j|\):

\begin{equation}
\label{eq:lasso-penalty}
\text{minimize } \left( SSE + \lambda \sum^p_{j=1} | \beta_j | \right)
\end{equation}

Whereas the ridge penalty pushes variables to \emph{approximately but not equal to zero}, the lasso penalty will actually push coefficients all the way to zero as illustrated in Figure \ref{fig:lasso-coef-example}. Switching to the lasso penalty not only improves the model but it also conducts automated feature selection.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/lasso-coef-example-1} 

}

\caption{Lasso regression coefficients as $\lambda$ grows from  $0 \rightarrow \infty$.}\label{fig:lasso-coef-example}
\end{figure}

In the figure above we see that when \(\lambda < 0.01\) all 15 variables are included in the model, when \(\lambda \approx 0.5\) 9 variables are retained, and when \(log\left(\lambda\right) = 1\) only 5 variables are retained. Consequently, when a data set has many features, lasso can be used to identify and extract those features with the largest (and most consistent) signal.

\hypertarget{elastic}{%
\subsection{Elastic nets}\label{elastic}}

A generalization of the ridge and lasso penalties, called the \emph{elastic net}\index{elastic net} \citep{zou2005regularization}, combines the two penalties:

\begin{equation}
\label{eq:elastic-penalty}
\text{minimize } \left( SSE + \lambda_1 \sum^p_{j=1} \beta_j^2 + \lambda_2 \sum^p_{j=1} | \beta_j | \right)
\end{equation}

Although lasso models perform feature selection, when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically handling correlated features together. Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/elastic-net-coef-example-1} 

}

\caption{Elastic net coefficients as $\lambda$ grows from  $0 \rightarrow \infty$.}\label{fig:elastic-net-coef-example}
\end{figure}

\hypertarget{implementation}{%
\section{Implementation}\label{implementation}}

First, we illustrate an implementation of regularized regression using the direct engine \textbf{glmnet}. This will provide you with a strong sense of what is happening with a regularized model. Realize there are other implementations available (e.g., \textbf{h2o}, \textbf{elasticnet}, \textbf{penalized}, some of which you can find example implementations on our online resource). Then, in the tuning section (\ref{regression-glmnet-tune}), we'll demonstrate how to apply a regularized model so we can properly compare it with our previous predictive models.

The \textbf{glmnet} package is extremely efficient and fast, even on very large data sets (mostly due to its use of Fortran to solve the lasso porblem via \emph{coordinate descent}); note, however, that it only accepts the non-formula XY interface (\ref{many-formula-interfaces}) so prior to modeling we need to separate our feature and target sets.

\begin{note}
The following uses \texttt{model.matrix} to dummy encode our feature set
(see \texttt{Matrix::sparse.model.matrix} for increased efficiency on
larger data sets). We also \(\log\) transform the response variable
which is not required; however, parametric models such as regularized
regression are sensitive to skewed response values so transforming can
often improve predictive performance.
\end{note}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create training  feature matrices}
\CommentTok{# we use model.matrix(...)[, -1] to discard the intercept}
\NormalTok{X <-}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., ames_train)[, }\DecValTok{-1}\NormalTok{]}

\CommentTok{# transform y with log transformation}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{log}\NormalTok{(ames_train}\OperatorTok{$}\NormalTok{Sale_Price)}
\end{Highlighting}
\end{Shaded}

To apply a regularized model we can use the \texttt{glmnet::glmnet()} function. The \texttt{alpha} parameter tells \textbf{glmnet} to perform a ridge (\texttt{alpha\ =\ 0}), lasso (\texttt{alpha\ =\ 1}), or elastic net (\texttt{0\ \textless{}\ alpha\ \textless{}\ 1}) model. By default, \textbf{glmnet} will do two things that you should be aware of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Since regularized methods apply a penalty to the coefficients, we need to ensure our coefficients are on a common scale. If not, then predictors with naturally larger values (e.g., total square footage) will be penalized more than predictors with naturally smaller values (e.g., total number of rooms). By default, \textbf{glmnet} automatically standardizes your features. If you standardize your predictors prior to \textbf{glmnet} you can turn this argument off with \texttt{standardize\ =\ FALSE}.
\item
  \textbf{glmnet} will fit ridge models across a wide range of \(\lambda\) values, which is illustrated in Figure \ref{fig:ridge1}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Apply Ridge regression to attrition data}
\NormalTok{ridge <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ X,}
  \DataTypeTok{y =}\NormalTok{ Y,}
  \DataTypeTok{alpha =} \DecValTok{0}
\NormalTok{)}

\KeywordTok{plot}\NormalTok{(ridge, }\DataTypeTok{xvar =} \StringTok{"lambda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/ridge1-1} 

}

\caption{Coefficients for our ridge regression model as $\lambda$ grows from  $0 \rightarrow \infty$.}\label{fig:ridge1}
\end{figure}

We can see the exact \(\lambda\) values applied with \texttt{ridge\$lambda}. Although you can specify your own \(\lambda\) values, by default \textbf{glmnet} applies 100 \(\lambda\) values that are data derived.

\begin{tip}
\textbf{glmnet} can auto-generate the appropriate \(\lambda\) values
based on the data; the vast majority of the time you will have little
need to adjust this default.
\end{tip}

We can also access the coefficients for a particular model using \texttt{coef()}. \textbf{glmnet} stores all the coefficients for each model in order of largest to smallest \(\lambda\). Here we just peak at the two largest coefficients (which correspond to \texttt{Latitude} \& \texttt{Overall\_QualVery\_Excellent}) for the largest (289.0010) and smallest (0.02791035) \(\lambda\) values. You can see how the largest \(\lambda\) value has pushed most of these coefficients to nearly 0.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# lambdas applied to penalty parameter}
\NormalTok{ridge}\OperatorTok{$}\NormalTok{lambda }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\CommentTok{## [1] 289.0 263.3 239.9 218.6 199.2 181.5}

\CommentTok{# small lambda results in large coefficients}
\KeywordTok{coef}\NormalTok{(ridge)[}\KeywordTok{c}\NormalTok{(}\StringTok{"Latitude"}\NormalTok{, }\StringTok{"Overall_QualVery_Excellent"}\NormalTok{), }\DecValTok{100}\NormalTok{]}
\CommentTok{##                   Latitude Overall_QualVery_Excellent }
\CommentTok{##                     0.3887                     0.1274}

\CommentTok{# large lambda results in small coefficients}
\KeywordTok{coef}\NormalTok{(ridge)[}\KeywordTok{c}\NormalTok{(}\StringTok{"Latitude"}\NormalTok{, }\StringTok{"Overall_QualVery_Excellent"}\NormalTok{), }\DecValTok{1}\NormalTok{]  }
\CommentTok{##                                   Latitude }
\CommentTok{## 0.0000000000000000000000000000000000067824 }
\CommentTok{##                 Overall_QualVery_Excellent }
\CommentTok{## 0.0000000000000000000000000000000000009723}
\end{Highlighting}
\end{Shaded}

At this point, we do not understand how much improvement we are experiencing in our loss function across various \(\lambda\) values.

\hypertarget{regression-glmnet-tune}{%
\section{Tuning}\label{regression-glmnet-tune}}

Recall that \(\lambda\) is a tuning parameter that helps to control our model from over-fitting to the training data. To identify the optimal \(\lambda\) value we can use \emph{k}-fold cross-validation (CV). \texttt{glmnet::cv.glmnet()} can perform \emph{k}-fold CV, and by default, performs 10-fold CV. Below we perform a CV \textbf{glmnet} model with both a ridge and lasso penalty separately:

\begin{tip}
By default, \texttt{glmnet::cv.glmnet()} uses MSE as the loss function
but you can also use mean absolute error (MAE) for continuous outcomes
by changing the \texttt{type.measure} argument; see
\texttt{?glmnet::cv.glmnet()} for more details.
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Apply CV Ridge regression to Ames data}
\NormalTok{ridge <-}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ X,}
  \DataTypeTok{y =}\NormalTok{ Y,}
  \DataTypeTok{alpha =} \DecValTok{0}
\NormalTok{)}

\CommentTok{# Apply CV Lasso regression to Ames data}
\NormalTok{lasso <-}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ X,}
  \DataTypeTok{y =}\NormalTok{ Y,}
  \DataTypeTok{alpha =} \DecValTok{1}
\NormalTok{)}

\CommentTok{# plot results}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(ridge, }\DataTypeTok{main =} \StringTok{"Ridge penalty}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(lasso, }\DataTypeTok{main =} \StringTok{"Lasso penalty}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/ridge-lasso-cv-models-1} 

}

\caption{10-fold CV MSE for a ridge and lasso model. First dotted vertical line in each plot represents the $\lambda$ with the smallest MSE and the second represents the $\lambda$ with an MSE within one standard error of the minimum MSE.}\label{fig:ridge-lasso-cv-models}
\end{figure}

Figure \ref{fig:ridge-lasso-cv-models} illustrates the 10-fold CV MSE across all the \(\lambda\) values. In both models we see a slight improvement in the MSE as our penalty \(log(\lambda)\) gets larger , suggesting that a regular OLS model likely overfits the training data. But as we constrain it further (i.e., continue to increase the penalty), our MSE starts to decrease. The numbers across the top of the plot refer to the number of features in the model. Ridge regression does not force any variables to exactly zero so all features will remain in the model but we see the number of variables retained in the lasso model decrease as the penalty increases.

The first and second vertical dashed lines represent the \(\lambda\) value with the minimum MSE and the largest \(\lambda\) value within one standard error of it. The minimum MSE for our ridge model is 0.01899 (produced when \(\lambda =\) 0.09686 whereas the minimium MSE for our lasso model is 0.02090 (produced when \(\lambda =\) 0.00400).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Ridge model}
\KeywordTok{min}\NormalTok{(ridge}\OperatorTok{$}\NormalTok{cvm)       }\CommentTok{# minimum MSE}
\CommentTok{## [1] 0.01899}
\NormalTok{ridge}\OperatorTok{$}\NormalTok{lambda.min     }\CommentTok{# lambda for this min MSE}
\CommentTok{## [1] 0.09686}

\NormalTok{ridge}\OperatorTok{$}\NormalTok{cvm[ridge}\OperatorTok{$}\NormalTok{lambda }\OperatorTok{==}\StringTok{ }\NormalTok{ridge}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se]  }\CommentTok{# 1 st.error of min MSE}
\CommentTok{## [1] 0.02121}
\NormalTok{ridge}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se  }\CommentTok{# lambda for this MSE}
\CommentTok{## [1] 0.471}

\CommentTok{# Lasso model}
\KeywordTok{min}\NormalTok{(lasso}\OperatorTok{$}\NormalTok{cvm)       }\CommentTok{# minimum MSE}
\CommentTok{## [1] 0.0209}
\NormalTok{lasso}\OperatorTok{$}\NormalTok{lambda.min     }\CommentTok{# lambda for this min MSE}
\CommentTok{## [1] 0.004002}

\NormalTok{lasso}\OperatorTok{$}\NormalTok{cvm[lasso}\OperatorTok{$}\NormalTok{lambda }\OperatorTok{==}\StringTok{ }\NormalTok{lasso}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se]  }\CommentTok{# 1 st.error of min MSE}
\CommentTok{## [1] 0.0241}
\NormalTok{lasso}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se  }\CommentTok{# lambda for this MSE}
\CommentTok{## [1] 0.01616}
\end{Highlighting}
\end{Shaded}

We can assess this visually. Figure \ref{fig:ridge-lasso-cv-viz-results} plots the estimated coefficients across the range of \(\lambda\) values. The dashed red line represents the \(\lambda\) value with the smallest MSE and the dashed blue line represents largest \(\lambda\) value that falls within one standard error of the minimum MSE. This shows you how much we can constrain the coefficients while still maximizing predictive accuracy.

\begin{tip}
Above, we saw that both ridge and lasso penalties provide similiar MSEs;
however, these plots illustrate that ridge is still using all 299
features whereas the lasso model can get a similar MSE while reducing
the feature set from 299 down to 131. However, there will be some
variability with this MSE and we can reasonably assume that we can
achieve a similar MSE with a slightly more constrained model that uses
only 63 features. Although this lasso model does not offer significant
improvement over the ridge model, we get approximately the same accuracy
by using only 63 features! If describing and interpreting the predictors
is an important component of your analysis, this may significantly aid
your endeavor.
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Ridge model}
\NormalTok{ridge_min <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ X,}
  \DataTypeTok{y =}\NormalTok{ Y,}
  \DataTypeTok{alpha =} \DecValTok{0}
\NormalTok{)}

\CommentTok{# Lasso model}
\NormalTok{lasso_min <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ X,}
  \DataTypeTok{y =}\NormalTok{ Y,}
  \DataTypeTok{alpha =} \DecValTok{1}
\NormalTok{)}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{# plot ridge model}
\KeywordTok{plot}\NormalTok{(ridge_min, }\DataTypeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Ridge penalty}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \KeywordTok{log}\NormalTok{(ridge}\OperatorTok{$}\NormalTok{lambda.min), }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{lty =} \StringTok{"dashed"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \KeywordTok{log}\NormalTok{(ridge}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se), }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lty =} \StringTok{"dashed"}\NormalTok{)}

\CommentTok{# plot lasso model}
\KeywordTok{plot}\NormalTok{(lasso_min, }\DataTypeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Lasso penalty}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \KeywordTok{log}\NormalTok{(lasso}\OperatorTok{$}\NormalTok{lambda.min), }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{lty =} \StringTok{"dashed"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \KeywordTok{log}\NormalTok{(lasso}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se), }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lty =} \StringTok{"dashed"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/ridge-lasso-cv-viz-results-1} 

}

\caption{Coefficients for our ridge and lasso models. First dotted vertical line in each plot represents the $\lambda$ with the smallest MSE and the second represents the $\lambda$ with an MSE within one standard error of the minimum MSE.}\label{fig:ridge-lasso-cv-viz-results}
\end{figure}

So far we've implemented a pure ridge and pure lasso model. However, we can implement an elastic net the same way as the ridge and lasso models, by adjusting the \texttt{alpha} parameter. Any \texttt{alpha} value between 0--1 will perform an elastic net. When \texttt{alpha\ =\ 0.5} we perform an equal combination of penalties whereas \texttt{alpha} \(< 0.5\) will have a heavier ridge penalty applied and \texttt{alpha} \(> 0.5\) will have a heavier lasso penalty.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/glmnet-elastic-comparison-1} 

}

\caption{Coefficients for various penalty parameters.}\label{fig:glmnet-elastic-comparison}
\end{figure}

Often, the optimal model contains an \texttt{alpha} somewhere between 0--1, thus we want to tune both the \(\lambda\) and the \texttt{alpha} parameters. As in Chapters \ref{linear-regression} and \ref{logistic-regression}, we can use the \textbf{caret} package to automate the tuning process. This ensures that any feature engineering is appropriately applied within each resample. The following performs a grid search over 10 values of the alpha parameter between 0--1 and ten values of the lambda parameter from the lowest to highest lambda values identified by \textbf{glmnet}.

\begin{warning}
This grid search took roughly \textbf{71 seconds} to compute.
\end{warning}

The following snippet of code shows that the model that minimized RMSE used an alpha of 0.1 and \(\lambda\) of 0.04688. The minimum RMSE of 0.1419461 (\(MSE = 0.1419461^2 = 0.0201487\)) is in line with the full ridge model produced earlier. Figure \ref{fig:glmnet-tuning-grid} illustrates how the combination of alpha values (\(x\)-axis) and \(\lambda\) values (line color) influence the RMSE.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# grid search across }
\NormalTok{cv_glmnet <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ X,}
  \DataTypeTok{y =}\NormalTok{ Y,}
  \DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{,}
  \DataTypeTok{preProc =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneLength =} \DecValTok{10}
\NormalTok{)}

\CommentTok{# model with lowest RMSE}
\NormalTok{cv_glmnet}\OperatorTok{$}\NormalTok{bestTune}
\CommentTok{##   alpha  lambda}
\CommentTok{## 8   0.1 0.04688}

\CommentTok{# plot cross-validated RMSE}
\KeywordTok{ggplot}\NormalTok{(cv_glmnet)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/glmnet-tuning-grid-1} 

}

\caption{The 10-fold cross valdation RMSE across 10 alpha values (x-axis) and 10 lambda values (line color).}\label{fig:glmnet-tuning-grid}
\end{figure}

So how does this compare to our previous best model for the Ames data set? Keep in mind that for this chapter we \(\log\) transformed the response variable (\texttt{Sale\_Price}). Consequently, to provide a fair comparison to our previously obtained PLS model's RMSE of \$29,970, we need to re-transform our predicted values. The following illustrates that our optimal regularized model achieved an RMSE of \$23,503. Introducing a penalty parameter to constrain the coefficients provided quite an improvement over our previously obtained dimension reduction approach.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predict sales price on training data}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(cv_glmnet, X)}

\CommentTok{# compute RMSE of transformed predicted}
\KeywordTok{RMSE}\NormalTok{(}\KeywordTok{exp}\NormalTok{(pred), }\KeywordTok{exp}\NormalTok{(Y))}
\CommentTok{## [1] 23503}
\end{Highlighting}
\end{Shaded}

\hypertarget{lm-features}{%
\section{Feature interpretation}\label{lm-features}}

Variable importance for regularized models provides a similar interpretation as in linear (or logistic) regression. Importance is determined by maginitude of the standardized coefficients and we can see in Figure \ref{fig:regularize-vip} some of the same features that were considered highly influential in our PLS model, albeit in differing order (i.e. \texttt{Gr\_Liv\_Area}, \texttt{Overall\_Qual}, \texttt{Total\_Bsmt\_SF}, \texttt{First\_Flr\_SF}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vip}\NormalTok{(cv_glmnet, }\DataTypeTok{num_features =} \DecValTok{20}\NormalTok{, }\DataTypeTok{bar =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/regularize-vip-1} 

}

\caption{Top 20 most important variables for the optimal regularized regression model.}\label{fig:regularize-vip}
\end{figure}

Similar to linear and logistic regression, the relationship between the features and response is monotonic linear. However, since we modeled our response with a log transformation, the estimated relationships will still be monotonic but non-linear on the original response scale. Figure \ref{fig:regularized-top4-pdp} illustrates the relationship between the top four most influential variables (i.e., largest aboslute coefficients) and the non-transformed sales price. All relationships are positive in nature, as the values in these features increase (or for \texttt{Overall\_QualExcellent} if it exists) the average predicted sales price increases.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/regularized-top4-pdp-1} 

}

\caption{Partial dependence plots for the first four most important variables.}\label{fig:regularized-top4-pdp}
\end{figure}

However, we see the \(5^{th}\) most influential variable is \texttt{Overall\_QualPoor}. When a home has an overall quality rating of poor we see that the average predicted sales price decreases versus when it has some other overall quality rating. Consequently, its important to not only look at the variable importance ranking, but also observe the positive or negative nature of the relationship.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/regularized-num5-pdp-1} 

}

\caption{Partial dependence plot for when overall quality of a home is (1) versus is not poor (0).}\label{fig:regularized-num5-pdp}
\end{figure}

\hypertarget{attrition-data}{%
\section{Attrition data}\label{attrition-data}}

We saw that regularization significantly improved our predictive accuracy for the Ames data set, but how about for the emplyee attrition example. In Chapter \ref{logistic-regression} we saw a maximum CV accuracy of 86.3\% for our logistic regression model. We see a little improvement in the following with some preprocessing; however, performing a regularized logistic regression model provides us with an additional 0.8\% improvement in accuracy (likely within the margin of error).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\NormalTok{attrition }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_if}\NormalTok{(is.ordered, factor, }\DataTypeTok{ordered =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Create training (70%) and test (30%) sets for the}
\CommentTok{# rsample::attrition data. Use set.seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{churn_split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(df, }\DataTypeTok{prop =} \FloatTok{.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Attrition"}\NormalTok{)}
\NormalTok{train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(churn_split)}
\NormalTok{test  <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(churn_split)}

\CommentTok{# train logistic regression model}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{glm_mod <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"glm"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{preProc =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{  )}

\CommentTok{# train regularized logistic regression model}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{penalized_mod <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{preProc =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneLength =} \DecValTok{10}
\NormalTok{  )}

\CommentTok{# extract out of sample performance measures}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{logistic_model =}\NormalTok{ glm_mod, }
  \DataTypeTok{penalized_model =}\NormalTok{ penalized_mod}
\NormalTok{  )))}\OperatorTok{$}\NormalTok{statistics}\OperatorTok{$}\NormalTok{Accuracy}
\CommentTok{##                   Min. 1st Qu. Median   Mean 3rd Qu.}
\CommentTok{## logistic_model  0.8365  0.8495 0.8792 0.8758  0.8908}
\CommentTok{## penalized_model 0.8447  0.8759 0.8835 0.8836  0.8915}
\CommentTok{##                   Max. NA's}
\CommentTok{## logistic_model  0.9314    0}
\CommentTok{## penalized_model 0.9412    0}
\end{Highlighting}
\end{Shaded}

\hypertarget{final-thoughts-2}{%
\section{Final thoughts}\label{final-thoughts-2}}

Regularized regression provides many great benefits over traditional GLMs when applied to large data sets with lots of features. It provides a great option for handling the \(n > p\) problem, helps minimize the impact of multicollinearity, and can perform automated feature selection. It also has relatively few hyperparamters which makes them easy to tune, are quite computationally efficient compared to other algorithms discussed in later chapters, and does not require lots of memory.

However, regularized regression does require some feature pre-processing. Notably, all inputs must be numeric; however, some packages (e.g., \textbf{caret} and \textbf{h2o}) automate this process. They cannot automatically handle missing data, which requires you to remove or impute them prior to modeling. Similar to GLMs, they are also not robust to outliers in both the feature and target. Lastly, regularized regression models still assume a monotonic linear relationship (always increasing or decreasing in a linear fashion). It is also up to the analyst whether or not to include specific interaction effects.

\hypertarget{mars}{%
\chapter{Multivariate Adaptive Regression Splines}\label{mars}}

The previous chapters discussed algorithms that are intrinsically linear. Many of these models can be adapted to nonlinear patterns in the data by manually adding nonlinear model terms (e.g., squared terms, interaction effects, and other transofmations of the original features); however, to do so you the analyst must know the specific nature of the nonlinearities and interactions \emph{a priori}. Alternatively, there are numerous algorithms that are inherently nonlinear. When using these models, the exact form of the nonlinearity does not need to be known explicitly or specified prior to model training. Rather, these algorithms will search for, and discover, nonlinearities and interactions in the data that help maximize predictive accuracy.

This chapter discusses \emph{multivariate adaptive regression splines}\index{multivariate adaptive regression splines} (MARS) \citep{friedman1991multivariate}, an algorithm that automatically creates a piecewise linear model which provides an intuitive stepping block into nonlinearity after grasping the concept of multiple linear regression. Future chapters will focus on other nonlinear algorithms.

\hypertarget{prerequisites-5}{%
\section{Prerequisites}\label{prerequisites-5}}

For this chapter we will use the following packages:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Helper packages}
\KeywordTok{library}\NormalTok{(dplyr)     }\CommentTok{# for data wrangling}
\KeywordTok{library}\NormalTok{(ggplot2)   }\CommentTok{# for awesome plotting}

\CommentTok{# Modeling packages}
\KeywordTok{library}\NormalTok{(earth)     }\CommentTok{# for fitting MARS models}
\KeywordTok{library}\NormalTok{(caret)     }\CommentTok{# for automating the tuning process}

\CommentTok{# Model interpretability packages}
\KeywordTok{library}\NormalTok{(vip)       }\CommentTok{# for variable importance}
\KeywordTok{library}\NormalTok{(pdp)       }\CommentTok{# for variable relationships}
\end{Highlighting}
\end{Shaded}

To illustrate various concepts we'll continue with the \texttt{ames\_train} and \texttt{ames\_test} data sets created in Section \ref{put-process-together}:

\hypertarget{the-basic-idea}{%
\section{The basic idea}\label{the-basic-idea}}

In the previous chapters, we focused on linear models (where the analyst has to explicitly specify any nonlinear relationships and interaction effects). We illustrated some of the advantages of linear models such as their ease and speed of computation and also the intuitive nature of interpreting their coefficients. However, linear models make a strong assumption about linearity, and this assumption is often a poor one, which can affect predictive accuracy.

We can extend linear models to capture any non-linear relationship. Typically, this is done by explicitly including polynomial terms (e.g., \(X_1^2\)) or step functions. Polynomial regression is a form of regression in which the relationship between \(X\) and \(Y\) is modeled as an \(d\)-th degree polynomial of in \(X\). For example, Equation \eqref{eq:poly} represents a polynomial regression function where \(Y\) is modeled as a \(d\)-th degree polynomial in \(X\). Generally speaking, it is unusual to use \(d\) greater than 3 or 4 as the larger \(d\) becomes, the easier the function fit becomes overly flexible and oddly shapened\ldots{}especially near the boundaries of the range of \(X\) values. Increasing \(d\) also tends to increase the presence of multicollinearity.

\begin{equation}
\label{eq:poly}
  y_i = \beta_0 + \beta_1 x_i + \beta_2 x^2_i + \beta_3 x^3_i \dots + \beta_d x^d_i + \epsilon_i,
\end{equation}

An alternative to polynomials is to use step functions. Whereas polynomial functions impose a global non-linear relationship, step functions break the range of \(X\) into bins, and fits a simple constant (e.g., the mean response) in each. This amounts to converting a continuous feature into an ordered categorical variable such that our linear regression function is converted to Equation \eqref{eq:steps}

\begin{equation}
\label{eq:steps}
  y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i) + \beta_3 C_3(x_i) \dots + \beta_d C_d(x_i) + \epsilon_i,
\end{equation}

where \(C_1(x)\) represents \(X\) values ranging from \(c_1 \leq X < c_2\), \(C_2\left(X\right)\) represents \(X\) values ranging from \(c_2 \leq X < c_3\), \(\dots\), \(C_d\left(X\right)\) represents \(X\) values ranging from \(c_{d-1} \leq X < c_d\). Figure \ref{fig:nonlinear-comparisons} contrasts linear, polynomial, and step function fits for non-linear, non-monotonic simulated data.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/nonlinear-comparisons-1} 

}

\caption{Blue line represents predicted (`y`) values as a function of `x` for alternative approaches to modeling explicit nonlinear regression patterns. (A) Traditional linear regression approach does not capture any nonlinearity unless the predictor or response is transformed (i.e. log transformation). (B) Degree-2 polynomial, (C) Degree-3 polynomial, (D) Step function fitting cutting `x` into six categorical levels.}\label{fig:nonlinear-comparisons}
\end{figure}

Although useful, the typical implementation of polynomial regression and step functions require the user to explicitly identify and incorporate which variables should have what specific degree of interaction or at what points of a variable \(X\) should cut points be made for the step functions. Considering many data sets today can easily contain 50, 100, or more features, this would require an enormous and unncessary time commitment from an analyst to determine these explicit non-linear settings.

\hypertarget{multivariate-regression-splines}{%
\subsection{Multivariate regression splines}\label{multivariate-regression-splines}}

Multivariate adaptive regression splines (MARS) provide a convenient approach to capture the nonlinearity relationships in the data by assessing cutpoints (\emph{knots}) similar to step functions. The procedure assesses each data point for each predictor as a knot and creates a linear regression model with the candidate feature(s). For example, consider our non-linear, non-monotonic data above where \(Y = f\left(X\right)\). The MARS procedure will first look for the single point across the range of \texttt{X} values where two different linear relationships between \texttt{Y} and \texttt{X} achieve the smallest error (e.g., smallest SSE). What results is known as a hinge function \(h\left(x-a\right)\), where \(a\) is the cutpoint value. For a single knot (Figure \ref{fig:examples-of-multiple-knots} (A)), our hinge function is \(h\left(\text{x}-1.183606\right)\) such that our two linear models for \texttt{Y} are

\begin{equation}
\label{eq:hinge}
  \text{y} = 
  \begin{cases}
    \beta_0 + \beta_1(1.183606 - \text{x}) & \text{x} < 1.183606, \\
    \beta_0 + \beta_1(\text{x} - 1.183606) & \text{x} > 1.183606
  \end{cases}
\end{equation}

Once the first knot has been found, the search continues for a second knot which is found at \(x = 4.898114\) (Figure \ref{fig:examples-of-multiple-knots} (B)). This results in three linear models for \texttt{y}:

\begin{equation}
\label{eq:hinge2}
  \text{y} = 
  \begin{cases}
    \beta_0 + \beta_1(1.183606 - \text{x}) & \text{x} < 1.183606, \\
    \beta_0 + \beta_1(\text{x} - 1.183606) & \text{x} > 1.183606 \quad \& \quad \text{x} < 4.898114, \\
    \beta_0 + \beta_1(4.898114 - \text{x}) & \text{x} > 4.898114
  \end{cases}
\end{equation}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/examples-of-multiple-knots-1} 

}

\caption{Examples of fitted regression splines of one (A), two (B), three (C), and four (D) knots.}\label{fig:examples-of-multiple-knots}
\end{figure}

This procedure continues until many knots are found, producing a (potentially) highly non-linear prediction equation. Although including many knots may allow us to fit a really good relationship with our training data, it may not generalize very well to new, unseen data. Consequently, once the full set of knots have been identified, we can sequentially remove knots that do not contribute significantly to predictive accuracy. This process is known as ``pruning'' and we can use cross-validation, as we have with the previous models, to find the optimal number of knots.

\hypertarget{fitting-a-basic-mars-model}{%
\section{Fitting a basic MARS model}\label{fitting-a-basic-mars-model}}

We can fit a direct engine MARS model with the \textbf{earth} package \citep{R-earth}. By default, \texttt{earth::earth()} will assess all potential knots across all supplied features and then will prune to the optimal number of knots based on an expected change in \(R^2\) (for the training data) of less than 0.001. This calculation is performed by the Generalized cross-validation (GCV) procedure, which is a computational shortcut for linear models that produces an approximate leave-one-out cross-validation error metric \citep{golub1979generalized}.

\begin{note}
The term ``MARS'' is trademarked and licensed exclusively to Salford
Systems: \url{https://www.salford-systems.com}. We can use MARS as an
abbreviation; however, it cannot be used for competing software
solutions. This is why the R package uses the name \textbf{earth}.
\end{note}

The following applies a basic MARS model to our \textbf{ames} example. The results show us the final models GCV statistic, generalized \(R^2\) (GRSq), and more.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit a basic MARS model}
\NormalTok{mars1 <-}\StringTok{ }\KeywordTok{earth}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{.,  }
  \DataTypeTok{data =}\NormalTok{ ames_train   }
\NormalTok{)}

\CommentTok{# Print model summary}
\KeywordTok{print}\NormalTok{(mars1)}
\CommentTok{## Selected 36 of 41 terms, and 24 of 307 predictors}
\CommentTok{## Termination condition: RSq changed by less than 0.001 at 41 terms}
\CommentTok{## Importance: First_Flr_SF, Second_Flr_SF, ...}
\CommentTok{## Number of terms at each degree of interaction: 1 35 (additive model)}
\CommentTok{## GCV 511235214    RSS 978736420657    GRSq 0.9206    RSq 0.9259}
\end{Highlighting}
\end{Shaded}

It also shows us that 36 of 41 terms were used from 24 of the 307 original predictors. But what does this mean? If we were to look at all the coefficients, we would see that there are 36 terms in our model (including the intercept). These terms include hinge functions produced from the original 307 predictors (307 predictors because the model automatically dummy encodes categorical features). Looking at the first 10 terms in our model, we see that \texttt{Gr\_Liv\_Area} is included with a knot at 2790 (the coefficient for \(h\left(2790-\text{Gr\_Liv\_Area}\right)\) is -55.26), \texttt{Year\_Built} is included with a knot at 2002, etc.

\begin{tip}
You can check out all the coefficients with \texttt{summary(mars1)} or
\texttt{coef(mars1)}.
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(mars1) }\OperatorTok{%>%}\StringTok{ }\NormalTok{.}\OperatorTok{$}\NormalTok{coefficients }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\CommentTok{##                       Sale_Price}
\CommentTok{## (Intercept)            289316.22}
\CommentTok{## h(Gr_Liv_Area-2790)       -55.26}
\CommentTok{## h(Year_Built-2002)       3040.56}
\CommentTok{## h(2002-Year_Built)       -410.86}
\CommentTok{## h(2220-Total_Bsmt_SF)     -30.71}
\CommentTok{## h(Bsmt_Unf_SF-543)        -25.38}
\CommentTok{## h(543-Bsmt_Unf_SF)         13.83}
\CommentTok{## h(Total_Bsmt_SF-1550)      39.17}
\CommentTok{## h(Garage_Cars-2)        12480.67}
\CommentTok{## h(2-Garage_Cars)        -4834.41}
\end{Highlighting}
\end{Shaded}

The plot method for MARS model objects provides useful performance and residual plots. Figure \ref{fig:basic-mod-plot} illustrates the model selection plot that graphs the GCV \(R^2\) (left-hand \(y\)-axis and solid black line) based on the number of terms retained in the model (\(x\)-axis) which are constructed from a certain number of original predictors (right-hand \(y\)-axis). The vertical dashed lined at 36 tells us the optimal number of non-intercept terms retained where marginal increases in GCV \(R^2\) are less than 0.001.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(mars1, }\DataTypeTok{which =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/basic-mod-plot-1} 

}

\caption{Model summary capturing GCV $R^2$ (left-hand y-axis and solid black line) based on the number of terms retained (x-axis) which is based on the number of predictors used to make those terms (right-hand side y-axis). For this model, 35 non-intercept terms were retained which are based on 26 predictors.  Any additional terms retained in the model, over and above these 35, results in less than 0.001 improvement in the GCV $R^2$.}\label{fig:basic-mod-plot}
\end{figure}

In addition to pruning the number of knots, \texttt{earth::earth()} allows us to also assess potential interactions between different hinge functions. The following illustrates this by including a \texttt{degree\ =\ 2} argument. You can see that now our model includes interaction terms between a maximum of two hinge functions (e.g., \texttt{h(Year\_Built-2002)*h(2362-Gr\_Liv\_Area)} is an interaction effect for those houses built prior to 2002 and have less than 2,362 square feet of living space above ground).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit a basic MARS model}
\NormalTok{mars2 <-}\StringTok{ }\KeywordTok{earth}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{.,  }
  \DataTypeTok{data =}\NormalTok{ ames_train,}
  \DataTypeTok{degree =} \DecValTok{2}
\NormalTok{)}

\CommentTok{# check out the first 10 coefficient terms}
\KeywordTok{summary}\NormalTok{(mars2) }\OperatorTok{%>%}\StringTok{ }\NormalTok{.}\OperatorTok{$}\NormalTok{coefficients }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\CommentTok{##                                        Sale_Price}
\CommentTok{## (Intercept)                            230780.519}
\CommentTok{## h(Gr_Liv_Area-2790)                        94.786}
\CommentTok{## h(2790-Gr_Liv_Area)                       -50.976}
\CommentTok{## h(Year_Built-2002)                       9111.804}
\CommentTok{## h(2002-Year_Built)                       -689.248}
\CommentTok{## h(Year_Built-2002)*h(2362-Gr_Liv_Area)     -7.717}
\CommentTok{## h(Total_Bsmt_SF-1136)                      63.634}
\CommentTok{## h(1136-Total_Bsmt_SF)                     -32.704}
\CommentTok{## h(Bsmt_Unf_SF-504)                        -25.087}
\CommentTok{## h(504-Bsmt_Unf_SF)                         12.957}
\end{Highlighting}
\end{Shaded}

\hypertarget{tuning}{%
\section{Tuning}\label{tuning}}

There are two important tuning parameters associated with our MARS model: the maximum degree of interactions and the number of terms retained inthe final model. We need to perform a grid search to identify the optimal combination of these hyperparameters that minimize prediction error (the above pruning process was based only on an approximation of CV model performance on the training data rather than an exact \emph{k}-fold CV process). As in previous chapters, we'll perform a CV grid search to identify the optimal hyperpameter mix. Below, we set up a grid that assesses 30 different combinations of interaction complexity (\texttt{degree}) and the number of terms to retain in the final model (\texttt{nprune}).

\begin{tip}
Rarely is there any benefit in assessing greater than 3-rd degree
interactions and we suggest starting out with 10 evenly spaced values
for \texttt{nprune} and then you can always zoom in to a region once you
find an approximate optimal solution.
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a tuning grid}
\NormalTok{hyper_grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}
  \DataTypeTok{degree =} \DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }
  \DataTypeTok{nprune =} \KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{floor}\NormalTok{()}
\NormalTok{)}

\KeywordTok{head}\NormalTok{(hyper_grid)}
\CommentTok{##   degree nprune}
\CommentTok{## 1      1      2}
\CommentTok{## 2      2      2}
\CommentTok{## 3      3      2}
\CommentTok{## 4      1     12}
\CommentTok{## 5      2     12}
\CommentTok{## 6      3     12}
\end{Highlighting}
\end{Shaded}

As in the previous chapters, we can use \textbf{caret} to perform a grid search using 10-fold CV. The model that provides the optimal combination includes third degree interaction effects and retains 45 terms. The cross-validated RMSE for these models are displayed in Figure \ref{fig:grid-search}; the optimal model's cross-validated RMSE was \$22,888.

\begin{warning}
This grid search took roughly five minutes to complete.
\end{warning}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for reproducibiity}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# cross validated model}
\NormalTok{cv_mars <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{subset}\NormalTok{(ames_train, }\DataTypeTok{select =} \OperatorTok{-}\NormalTok{Sale_Price),}
  \DataTypeTok{y =}\NormalTok{ ames_train}\OperatorTok{$}\NormalTok{Sale_Price,}
  \DataTypeTok{method =} \StringTok{"earth"}\NormalTok{,}
  \DataTypeTok{metric =} \StringTok{"RMSE"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneGrid =}\NormalTok{ hyper_grid}
\NormalTok{)}

\CommentTok{# best model}
\NormalTok{cv_mars}\OperatorTok{$}\NormalTok{bestTune}
\CommentTok{##    nprune degree}
\CommentTok{## 25     45      3}

\CommentTok{# plot results}
\KeywordTok{ggplot}\NormalTok{(cv_mars)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/grid-search-1} 

}

\caption{Cross-validated RMSE for the 30 different hyperparameter combinations in our grid search. The optimal model retains 45 terms and includes up to 3$^{rd}$ degree interactions.}\label{fig:grid-search}
\end{figure}

The above grid search helps to focus where we can further refine our model tuning. As a next step, we could perform a grid search that focuses in on a refined grid space for \texttt{nprune} (e.g., comparing 35--45 terms retained). However, for brevity we'll leave this as an exercise for the reader.

So how does this compare to our previously built models for the Ames housing data? The following table compares the cross-validated RMSE for our tuned MARS model to an ordinary multiple regression model along with tuned principal component regression (PCR), partial least squares (PLS), and regularized regression (elastic net) models. By incorporating non-linear relationships and interaction effects, the MARS model provides a substantial improvement over the previous linear models that we have explored.

\begin{note}
Notice that our elastic net model is higher than in the last chapter.
This table compares these 5 modeling approaches without performing any
logarithmic transformation on the target variable. However, even
considering the best tuned regularized regression results from last
chapter (RMSE = 23503), our optimal MARS model performs better.
\end{note}

\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
  & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max. & NA's\\
\hline
Multiple\_regression & 20945 & 25674 & 33769 & 37304 & 42967 & 80339 & 0\\
\hline
PCR & 26925 & 29987 & 33891 & 33659 & 37180 & 40713 & 0\\
\hline
PLS & 21147 & 25951 & 28903 & 29970 & 35261 & 41414 & 0\\
\hline
Elastic\_net & 19938 & 24062 & 25261 & 29785 & 33829 & 53333 & 0\\
\hline
MARS & 18967 & 21792 & 22833 & 22888 & 23786 & 27190 & 0\\
\hline
\end{tabular}

\hypertarget{mars-features}{%
\section{Feature interpretation}\label{mars-features}}

MARS models via \texttt{earth::earth()} include a backwards elimination feature selection routine that looks at reductions in the GCV estimate of error as each predictor is added to the model. This total reduction is used as the variable importance measure (\texttt{value\ =\ "gcv"}). Since MARS will automatically include and exclude terms during the pruning process, it essentially performs automated feature selection. If a predictor was never used in any of the MARS basis functions in the final model (after pruning), it has an importance value of zero. This is illustrated in Figure \ref{fig:vip} where 27 features have \(>0\) importance values while the rest of the features have an importance value of zero since they were not included in the final model. Alternatively, you can also monitor the change in the residual sums of squares (RSS) as terms are added (\texttt{value\ =\ "rss"}); however, you will see very little difference between these methods.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# variable importance plots}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{vip}\NormalTok{(cv_mars, }\DataTypeTok{num_features =} \DecValTok{40}\NormalTok{, }\DataTypeTok{bar =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{value =} \StringTok{"gcv"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"GCV"}\NormalTok{)}

\NormalTok{p2 <-}\StringTok{ }\KeywordTok{vip}\NormalTok{(cv_mars, }\DataTypeTok{num_features =} \DecValTok{40}\NormalTok{, }\DataTypeTok{bar =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{value =} \StringTok{"rss"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"RSS"}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/vip-1} 

}

\caption{Variable importance based on impact to GCV (left) and RSS (right) values as predictors are added to the model. Both variable importance measures will usually give you very similar results.}\label{fig:vip}
\end{figure}

Its important to realize that variable importance will only measure the impact of the prediction error as features are included; however, it does not measure the impact for particular hinge functions created for a given feature. For example, in Figure \ref{fig:vip} we see that \texttt{Gr\_Liv\_Area} and \texttt{Year\_Built} are the two most influential variables; however, variable importance does not tell us how our model is treating the non-linear patterns for each feature. Also, if we look at the interaction terms our model retained, we see interactions between different hinge functions for \texttt{Gr\_Liv\_Area} and \texttt{Year\_Built}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extract coefficients, convert to tidy data frame, and}
\CommentTok{# filter for interaction terms}
\NormalTok{cv_mars}\OperatorTok{$}\NormalTok{finalModel }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{coef}\NormalTok{() }\OperatorTok{%>%}\StringTok{  }
\StringTok{  }\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}\StringTok{  }
\StringTok{  }\KeywordTok{filter}\NormalTok{(stringr}\OperatorTok{::}\KeywordTok{str_detect}\NormalTok{(names, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{*"}\NormalTok{)) }
\CommentTok{## # A tibble: 20 x 2}
\CommentTok{##    names                                              x}
\CommentTok{##    <chr>                                          <dbl>}
\CommentTok{##  1 h(Year_Built-2002) * h(2362-Gr_Liv_Area)    -6.98e+0}
\CommentTok{##  2 h(Year_Remod_Add-2007) * h(Total_Bsmt_S~     8.93e+0}
\CommentTok{##  3 h(2007-Year_Remod_Add) * h(Total_Bsmt_S~    -1.22e+0}
\CommentTok{##  4 NeighborhoodEdwards * h(Year_Built-2002~    -6.71e+1}
\CommentTok{##  5 h(Lot_Area-3874) * h(3-Garage_Cars)         -1.17e+0}
\CommentTok{##  6 Bsmt_ExposureGd * h(Total_Bsmt_SF-1136)      3.12e+1}
\CommentTok{##  7 NeighborhoodCrawford * h(2002-Year_Buil~     4.25e+2}
\CommentTok{##  8 h(2002-Year_Built) * h(Year_Remod_Add-1~     7.90e+0}
\CommentTok{##  9 h(2002-Year_Built) * h(1974-Year_Remod_~     5.42e+0}
\CommentTok{## 10 h(Kitchen_AbvGr-1) * FunctionalTyp          -1.55e+4}
\CommentTok{## 11 Overall_QualVery_Excellent * Functional~     9.68e+4}
\CommentTok{## 12 Overall_QualGood * FunctionalTyp             1.32e+4}
\CommentTok{## 13 h(Lot_Area-3874) * h(Latitude-42.0014)       7.65e+0}
\CommentTok{## 14 h(Lot_Area-3874) * h(42.0014-Latitude)      -1.23e+2}
\CommentTok{## 15 h(Total_Bsmt_SF-1136) * h(115-Screen_Po~    -3.04e-1}
\CommentTok{## 16 h(Lot_Area-3874) * h(Gr_Liv_Area-2411) ~    -2.99e-3}
\CommentTok{## 17 h(Lot_Area-3874) * h(2411-Gr_Liv_Area) ~     5.29e-4}
\CommentTok{## 18 Overall_CondGood * h(2002-Year_Built)        3.33e+2}
\CommentTok{## 19 Overall_CondVery_Good * h(2002-Year_Bui~     3.68e+2}
\CommentTok{## 20 Overall_CondAbove_Average * h(2790-Gr_L~     6.20e+0}
\end{Highlighting}
\end{Shaded}

To better understand the relationship between these features and \texttt{Sale\_Price}, we can create partial dependence plots (PDPs) for each feature individually and also together. The individual PDPs illustrate the knots for each feature that our model found provides the best fit. For \texttt{Gr\_Liv\_Area}, as homes exceed 2,790 square feet, each additional square foot demands a higher marginal increase in sale price than homes with less than 2,790 square feet. Similarly, for homes built after 2002, there is a greater marginal effect on sales price based on the age of the home than for homes built prior to 2002. The interaction plot (far right figure) illustrates the stronger effect these two features have when combined.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Construct partial dependence plots}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{partial}\NormalTok{(cv_mars, }\DataTypeTok{pred.var =} \StringTok{"Gr_Liv_Area"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{autoplot}\NormalTok{()}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{partial}\NormalTok{(cv_mars, }\DataTypeTok{pred.var =} \StringTok{"Year_Built"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{autoplot}\NormalTok{()}
\NormalTok{p3 <-}\StringTok{ }\KeywordTok{partial}\NormalTok{(cv_mars, }\DataTypeTok{pred.var =} \KeywordTok{c}\NormalTok{(}\StringTok{"Gr_Liv_Area"}\NormalTok{, }\StringTok{"Year_Built"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{plotPartial}\NormalTok{(}\DataTypeTok{levelplot =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{zlab =} \StringTok{"yhat"}\NormalTok{, }\DataTypeTok{drape =} \OtherTok{TRUE}\NormalTok{, }
              \DataTypeTok{colorkey =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{screen =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{z =} \DecValTok{-20}\NormalTok{, }\DataTypeTok{x =} \DecValTok{-60}\NormalTok{))}

\CommentTok{# Display plots side by side}
\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, p3, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/pdp-1} 

}

\caption{Partial dependence plots to understand the relationship between sale price and the living space and year built features.  The PDPs tell us that as living space increases and for newer homes, predicted sale price increases dramatically.}\label{fig:pdp}
\end{figure}

\hypertarget{attrition-data-1}{%
\section{Attrition data}\label{attrition-data-1}}

The MARS method and algorithm can be extended to handle classification problems and GLMs in general.\footnote{See \citet{esl} and \citet{stone1997polynomial} for technical details regarding various alternative encodings for binary and mulinomial classification approaches.} We saw significant improvement to our predictive accuracy on the Ames data with a MARS model, but how about the emplyee attrition example? In Chapter \ref{logistic-regression} we saw a slight improvement in our cross-validated accuracy rate using regularized regression. Here, we tune a MARS model using the same search grid as we did above. We see our best models include no interaction effects and the optimal model retained 12 terms.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get attrition data}
\NormalTok{df <-}\StringTok{ }\NormalTok{attrition }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_if}\NormalTok{(is.ordered, factor, }\DataTypeTok{ordered =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Create training (70%) and test (30%) sets for the }
\CommentTok{# rsample::attrition data.}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{churn_split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(df, }\DataTypeTok{prop =} \FloatTok{.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Attrition"}\NormalTok{)}
\NormalTok{churn_train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(churn_split)}
\NormalTok{churn_test  <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(churn_split)}


\CommentTok{# for reproducibiity}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# cross validated model}
\NormalTok{tuned_mars <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{subset}\NormalTok{(churn_train, }\DataTypeTok{select =} \OperatorTok{-}\NormalTok{Attrition),}
  \DataTypeTok{y =}\NormalTok{ churn_train}\OperatorTok{$}\NormalTok{Attrition,}
  \DataTypeTok{method =} \StringTok{"earth"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneGrid =}\NormalTok{ hyper_grid}
\NormalTok{)}

\CommentTok{# best model}
\NormalTok{tuned_mars}\OperatorTok{$}\NormalTok{bestTune}
\CommentTok{##   nprune degree}
\CommentTok{## 2     12      1}

\CommentTok{# plot results}
\KeywordTok{ggplot}\NormalTok{(tuned_mars)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/tuned-marts-attrition-1} 

}

\caption{Cross-validated accuracy rate for the 30 different hyperparameter combinations in our grid search. The optimal model retains 12 terms and includes no interaction effects.}\label{fig:tuned-marts-attrition}
\end{figure}

However, comparing our MARS model to the previous linear models (logistic regression and regularized regression), we do not see any improvement in our overall accuracy rate.

\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
  & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max. & NA's\\
\hline
Logistic\_model & 0.8365 & 0.8495 & 0.8792 & 0.8758 & 0.8908 & 0.9314 & 0\\
\hline
Elastic\_net & 0.8447 & 0.8759 & 0.8835 & 0.8836 & 0.8915 & 0.9412 & 0\\
\hline
MARS\_model & 0.8155 & 0.8578 & 0.8781 & 0.8708 & 0.8908 & 0.9029 & 0\\
\hline
\end{tabular}

\hypertarget{final-thoughts-3}{%
\section{Final thoughts}\label{final-thoughts-3}}

There are several advantages to MARS. First, MARS naturally handles mixed types of predictors (quantitative and qualitative). MARS considers all possible binary partitions of the categories for a qualitative predictor into two groups.\footnote{This is very similar to CART-like decision trees which you'll be exposed to in Chapter \ref{DT}.} Each group then generates a pair of piecewise indicator functions for the two categories. MARS also requires minimal feature engineering (e.g., feature scaling) and performs automated feature selection. For example, since MARS scans each predictor to identify a split that improves predictive accuracy, non-informative features will not be chosen. Furthermore, highly correlated predictors do not impede predictive accuracy as much as they do with OLS models.

However, one disadvantage to MARS models is that they're typically slower to train. Since the algorithm scans each value of each predictor for potential cutpoints, computational performance can suffer as both \(n\) and \(p\) increase. Also, although correlated predictors do not necessarily impede model performance, they can make model interpretation difficult. When two features are nearly perfectly correlated, the algorithm will essentially select the first one it happens to come across when scanning the features. Then, since it randomly selected one, the correlated feature will likely not be included as it adds no additional explanatory power.

\bibliography{book.bib,packages.bib}

\backmatter
\printindex

\end{document}
