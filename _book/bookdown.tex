\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Hands-on Machine Learning with R},
            pdfauthor={Brad Boehmke \& Brandon Greenwell},
            colorlinks=true,
            linkcolor=Maroon,
            filecolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\newenvironment{block}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{icons/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{note}
  {\begin{block}{note}}
  {\end{block}}
\newenvironment{caution}
  {\begin{block}{caution}}
  {\end{block}}
\newenvironment{important}
  {\begin{block}{important}}
  {\end{block}}
\newenvironment{tip}
  {\begin{block}{tip}}
  {\end{block}}
\newenvironment{warning}
  {\begin{block}{warning}}
  {\end{block}}

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Hands-on Machine Learning with R}
\author{Brad Boehmke \& Brandon Greenwell}
\date{2019-06-25}

\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
Dedication TBD
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


Welcome to \emph{Hands-on Machine Learning with R}. This book provides hands-on modules for many of the most common machine learning methods to include:

\begin{itemize}
\tightlist
\item
  Generalized low rank models
\item
  Clustering algorithms
\item
  Autoencoders
\item
  Regularized models
\item
  Random forests
\item
  Gradient boosting machines
\item
  Deep neural networks
\item
  Stacking / super learners
\item
  and more!
\end{itemize}

You will learn how to build and tune these various models with R packages that have been tested and approved due to their ability to scale well. However, our motivation in almost every case is to describe the techniques in a way that helps develop intuition for its strengths and weaknesses. For the most part, we minimize mathematical complexity when possible but also provide resources to get deeper into the details if desired.

\hypertarget{who-should-read-this}{%
\section*{Who should read this}\label{who-should-read-this}}


We intend this work to be a practitioner's guide to the machine learning process and a place where one can come to learn about the approach and to gain intuition about the many commonly used, modern, and powerful methods accepted in the machine learning community. If you are familiar with the analytic methodologies, this book may still serve as a reference for how to work with the various R packages for implementation. While an abundance of videos, blog posts, and tutorials exist online, we have long been frustrated by the lack of consistency, completeness, and bias towards singular packages for implementation. This is what inspired this book.

This book is not meant to be an introduction to R or to programming in general; as we assume the reader has familiarity with the R language to include defining functions, managing R objects, controlling the flow of a program, and other basic tasks. If not, we would refer you to \href{http://r4ds.had.co.nz/index.html}{R for Data Science} \citep{wickham2016r} to learn the fundamentals of data science with R such as importing, cleaning, transforming, visualizing, and exploring your data. For those looking to advance their R programming skills and knowledge of the languge, we would refer you to \href{http://adv-r.had.co.nz/}{Advanced R} \citep{wickham2014advanced}. Nor is this book designed to be a deep dive into the theory and math underpinning machine learning algorithms. Several books already exist that do great justice in this arena (i.e. \href{https://web.stanford.edu/~hastie/ElemStatLearn/}{Elements of Statistical Learning} \citep{esl}, \href{https://web.stanford.edu/~hastie/CASI/}{Computer Age Statistical Inference} \citep{efron2016computer}, \href{http://www.deeplearningbook.org/}{Deep Learning} \citep{goodfellow2016deep}).

Instead, this book is meant to help R users learn to use the machine learning stack within R, which includes using various R packages such as \textbf{glmnet}, \textbf{h2o}, \textbf{ranger}, \textbf{xgboost}, \textbf{lime}, and others to effectively model and gain insight from your data. The book favors a hands-on approach, growing an intuitive understanding of machine learning through concrete examples and just a little bit of theory. While you can read this book without opening R, we highly recommend you experiment with the code examples provided throughout.

\hypertarget{why-r}{%
\section*{Why R}\label{why-r}}


R has emerged over the last couple decades as a first-class tool for scientific computing tasks, and has been a consistent leader in implementing statistical methodologies for analyzing data. The usefulness of R for data science stems from the large, active, and growing ecosystem of third-party packages: \textbf{tidyverse} for common data analysis activities; \textbf{h2o}, \textbf{ranger}, \textbf{xgboost}, and others for fast and scalable machine learning; \textbf{iml}, \textbf{pdp}, \textbf{vip}, and others for machine learning interpretability; and many more tools will be mentioned throughout the pages that follow.

\hypertarget{conventions-used-in-this-book}{%
\section*{Conventions used in this book}\label{conventions-used-in-this-book}}


The following typographical conventions are used in this book:

\begin{itemize}
\tightlist
\item
  \textbf{\emph{strong italic}}: indicates new terms,
\item
  \textbf{bold}: indicates package \& file names,
\item
  \texttt{inline\ code}: monospaced highlighted text indicates functions or other commands that could be typed literally by the user,
\item
  code chunk: indicates commands or other text that could be typed literally by the user
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{+}\StringTok{ }\DecValTok{2}
\CommentTok{## [1] 3}
\end{Highlighting}
\end{Shaded}

In addition to the general text used throughout, you will notice the following code chunks with images, which signify:

\begin{tip}
Signifies a tip or suggestion
\end{tip}

\begin{note}
Signifies a general note
\end{note}

\begin{warning}
Signifies a warning or caution
\end{warning}

\hypertarget{additional-resources}{%
\section*{Additional resources}\label{additional-resources}}


There are many great resources available to learn about machine learning. Throughout the chapters we try to include many of the resources that we have found extremely useful for digging deeper into the methodology and applying with code. However, due to print restrictions, the hard copy version of this book limits the concepts and methods discussed. Online supplementary material exists at \url{https://github.com/koalaverse/hands-on-machine-learning-with-r}. The additional material will accumulate over time and include extended chapter material (i.e., random forest package benchmarking) along with brand new content we couldn't fit in (i.e., random hyperparameter search). In addition, you can download the data used throughout the book, find teaching resources (i.e., slides and exercises), and more.

\hypertarget{feedback}{%
\section*{Feedback}\label{feedback}}


Reader comments are greatly appreciated. To report errors or bugs please post an issue at \url{https://github.com/koalaverse/hands-on-machine-learning-with-r/issues}.

\hypertarget{acknowledgments}{%
\section*{Acknowledgments}\label{acknowledgments}}


TBD

\hypertarget{software-information}{%
\section*{Software information}\label{software-information}}


An online version of this book is available at \url{http://bit.ly/HOML_with_R}. The source of the book along with additional content is available at \url{https://github.com/koalaverse/hands-on-machine-learning-with-r}. The book is powered by \url{https://bookdown.org} which makes it easy to turn R markdown files into HTML, PDF, and EPUB.

This book was built with the following packages and R version. All code was executed on 2017 MacBook Pro with a 2.9 GHz Intel Core i7 processor, 16 GB of memory, 2133 MHz speed, and double data rate synchronous dynamic random access memory (DDR3).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# packages used}
\NormalTok{pkgs <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}
  \StringTok{"AmesHousing"}\NormalTok{,}
  \StringTok{"bookdown"}\NormalTok{,}
  \StringTok{"caret"}\NormalTok{,}
  \StringTok{"cluster"}\NormalTok{,}
  \StringTok{"DALEX"}\NormalTok{,}
  \StringTok{"data.table"}\NormalTok{,}
  \StringTok{"dplyr"}\NormalTok{,}
  \StringTok{"dslabs"}\NormalTok{,}
  \StringTok{"e1071"}\NormalTok{,}
  \StringTok{"earth"}\NormalTok{,}
  \StringTok{"emo"}\NormalTok{,}
  \StringTok{"extracat"}\NormalTok{,}
  \StringTok{"factoextra"}\NormalTok{,}
  \StringTok{"ggplot2"}\NormalTok{,}
  \StringTok{"gbm"}\NormalTok{,}
  \StringTok{"glmnet"}\NormalTok{,}
  \StringTok{"h2o"}\NormalTok{,}
  \StringTok{"iml"}\NormalTok{,}
  \StringTok{"ipred"}\NormalTok{,}
  \StringTok{"keras"}\NormalTok{,}
  \StringTok{"kernlab"}\NormalTok{,}
  \StringTok{"MASS"}\NormalTok{,}
  \StringTok{"mclust"}\NormalTok{,}
  \StringTok{"mlbench"}\NormalTok{,}
  \StringTok{"pBrackets"}\NormalTok{,}
  \StringTok{"pdp"}\NormalTok{,}
  \StringTok{"pls"}\NormalTok{,}
  \StringTok{"pROC"}\NormalTok{,}
  \StringTok{"purrr"}\NormalTok{,}
  \StringTok{"ranger"}\NormalTok{,}
  \StringTok{"recipes"}\NormalTok{,}
  \StringTok{"reshape2"}\NormalTok{,}
  \StringTok{"ROCR"}\NormalTok{,}
  \StringTok{"rpart"}\NormalTok{,}
  \StringTok{"rpart.plot"}\NormalTok{,}
  \StringTok{"rsample"}\NormalTok{,}
  \StringTok{"tfruns"}\NormalTok{,}
  \StringTok{"tfestimators"}\NormalTok{,}
  \StringTok{"vip"}\NormalTok{,}
  \StringTok{"xgboost"}
\NormalTok{)}

\CommentTok{# package & session info}
\NormalTok{sessioninfo}\OperatorTok{::}\KeywordTok{session_info}\NormalTok{(pkgs)}
\CommentTok{#> - Session info --------------------------------------}
\CommentTok{#>  setting  value                       }
\CommentTok{#>  version  R version 3.6.0 (2019-04-26)}
\CommentTok{#>  os       macOS Sierra 10.12.6        }
\CommentTok{#>  system   x86_64, darwin15.6.0        }
\CommentTok{#>  ui       RStudio                     }
\CommentTok{#>  language (EN)                        }
\CommentTok{#>  collate  en_US.UTF-8                 }
\CommentTok{#>  ctype    en_US.UTF-8                 }
\CommentTok{#>  tz       America/New_York            }
\CommentTok{#>  date     2019-06-25                  }
\CommentTok{#> }
\CommentTok{#> - Packages ------------------------------------------}
\CommentTok{#>  ! package       * version    date       lib}
\CommentTok{#>    abind           1.4-5      2016-07-21 [1]}
\CommentTok{#>    AmesHousing     0.0.3      2017-12-17 [1]}
\CommentTok{#>    assertthat      0.2.1      2019-03-21 [1]}
\CommentTok{#>    backports       1.1.4      2019-04-10 [1]}
\CommentTok{#>    base64enc       0.1-3      2015-07-28 [1]}
\CommentTok{#>    BH              1.69.0-1   2019-01-07 [1]}
\CommentTok{#>    bitops          1.0-6      2013-08-17 [1]}
\CommentTok{#>    bookdown        0.11       2019-05-28 [1]}
\CommentTok{#>    boot            1.3-22     2019-04-02 [1]}
\CommentTok{#>    car             3.0-3      2019-05-27 [1]}
\CommentTok{#>    carData         3.0-2      2018-09-30 [1]}
\CommentTok{#>    caret         * 6.0-84     2019-04-27 [1]}
\CommentTok{#>    caTools         1.17.1.2   2019-03-06 [1]}
\CommentTok{#>    cellranger      1.1.0      2016-07-27 [1]}
\CommentTok{#>    checkmate       1.9.3      2019-05-03 [1]}
\CommentTok{#>    class           7.3-15     2019-01-01 [1]}
\CommentTok{#>    cli             1.1.0      2019-03-19 [1]}
\CommentTok{#>    clipr           0.6.0      2019-04-15 [1]}
\CommentTok{#>    cluster         2.0.8      2019-04-05 [1]}
\CommentTok{#>    codetools       0.2-16     2018-12-24 [1]}
\CommentTok{#>    colorspace      1.4-1      2019-03-18 [1]}
\CommentTok{#>    config          0.3        2018-03-27 [1]}
\CommentTok{#>    cowplot         0.9.4      2019-01-08 [1]}
\CommentTok{#>    crayon          1.3.4      2017-09-16 [1]}
\CommentTok{#>    curl            3.3        2019-01-10 [1]}
\CommentTok{#>    DALEX           0.3.0      2019-03-25 [1]}
\CommentTok{#>    data.table      1.12.2     2019-04-07 [1]}
\CommentTok{#>    dendextend      1.12.0     2019-05-11 [1]}
\CommentTok{#>    digest          0.6.19     2019-05-20 [1]}
\CommentTok{#>    dplyr         * 0.8.1      2019-05-14 [1]}
\CommentTok{#>    dslabs          0.5.2      2018-12-19 [1]}
\CommentTok{#>    e1071           1.7-1      2019-03-19 [1]}
\CommentTok{#>    earth           5.1.1      2019-04-12 [1]}
\CommentTok{#>    ellipse         0.4.1      2018-01-05 [1]}
\CommentTok{#>    ellipsis        0.1.0      2019-02-19 [1]}
\CommentTok{#>    emo             0.0.0.9000 2019-05-03 [1]}
\CommentTok{#>    evaluate        0.14       2019-05-28 [1]}
\CommentTok{#>  R extracat        <NA>       <NA>       [?]}
\CommentTok{#>    factoextra      1.0.5      2017-08-22 [1]}
\CommentTok{#>    FactoMineR      1.41       2018-05-04 [1]}
\CommentTok{#>    fansi           0.4.0      2018-10-05 [1]}
\CommentTok{#>    flashClust      1.01-2     2012-08-21 [1]}
\CommentTok{#>    forcats       * 0.4.0      2019-02-17 [1]}
\CommentTok{#>    foreach         1.4.4      2017-12-12 [1]}
\CommentTok{#>    foreign         0.8-71     2018-07-20 [1]}
\CommentTok{#>    forge           0.2.0      2019-02-26 [1]}
\CommentTok{#>    Formula         1.2-3      2018-05-03 [1]}
\CommentTok{#>    gbm             2.1.5      2019-01-14 [1]}
\CommentTok{#>    gdata           2.18.0     2017-06-06 [1]}
\CommentTok{#>    generics        0.0.2      2018-11-29 [1]}
\CommentTok{#>    ggplot2       * 3.1.1      2019-04-07 [1]}
\CommentTok{#>    ggpubr          0.2        2018-11-15 [1]}
\CommentTok{#>    ggrepel         0.8.1      2019-05-07 [1]}
\CommentTok{#>    ggsci           2.9        2018-05-14 [1]}
\CommentTok{#>    ggsignif        0.5.0      2019-02-20 [1]}
\CommentTok{#>    glmnet          2.0-16     2018-04-02 [1]}
\CommentTok{#>    glue            1.3.1.9000 2019-05-03 [1]}
\CommentTok{#>    gower           0.2.0      2019-03-07 [1]}
\CommentTok{#>    gplots          3.0.1.1    2019-01-27 [1]}
\CommentTok{#>    gridExtra       2.3        2017-09-09 [1]}
\CommentTok{#>    gtable          0.3.0      2019-03-25 [1]}
\CommentTok{#>    gtools          3.8.1      2018-06-26 [1]}
\CommentTok{#>    h2o           * 3.22.1.1   2019-01-10 [1]}
\CommentTok{#>    haven           2.1.0      2019-02-19 [1]}
\CommentTok{#>    highr           0.8        2019-03-20 [1]}
\CommentTok{#>    hms             0.4.2      2018-03-10 [1]}
\CommentTok{#>    htmltools       0.3.6      2017-04-28 [1]}
\CommentTok{#>    iml             0.9.0      2019-02-05 [1]}
\CommentTok{#>    inum            1.0-1      2019-04-25 [1]}
\CommentTok{#>    ipred           0.9-9      2019-04-28 [1]}
\CommentTok{#>    iterators       1.0.10     2018-07-13 [1]}
\CommentTok{#>    jsonlite        1.6        2018-12-07 [1]}
\CommentTok{#>    keras           2.2.4.1    2019-04-05 [1]}
\CommentTok{#>    kernlab         0.9-27     2018-08-10 [1]}
\CommentTok{#>    KernSmooth      2.23-15    2015-06-29 [1]}
\CommentTok{#>    knitr           1.23       2019-05-18 [1]}
\CommentTok{#>    labeling        0.3        2014-08-23 [1]}
\CommentTok{#>    lattice       * 0.20-38    2018-11-04 [1]}
\CommentTok{#>    lava            1.6.5      2019-02-12 [1]}
\CommentTok{#>    lazyeval        0.2.2      2019-03-15 [1]}
\CommentTok{#>    leaps           3.0        2017-01-10 [1]}
\CommentTok{#>    libcoin         1.0-4      2019-02-28 [1]}
\CommentTok{#>    lme4            1.1-21     2019-03-05 [1]}
\CommentTok{#>    lubridate       1.7.4      2018-04-11 [1]}
\CommentTok{#>    magrittr        1.5        2014-11-22 [1]}
\CommentTok{#>    maptools        0.9-5      2019-02-18 [1]}
\CommentTok{#>    markdown        1.0        2019-06-07 [1]}
\CommentTok{#>    MASS            7.3-51.4   2019-03-31 [1]}
\CommentTok{#>    Matrix          1.2-17     2019-03-22 [1]}
\CommentTok{#>    MatrixModels    0.4-1      2015-08-22 [1]}
\CommentTok{#>    mclust          5.4.3      2019-03-14 [1]}
\CommentTok{#>    Metrics         0.1.4      2018-07-09 [1]}
\CommentTok{#>    mgcv            1.8-28     2019-03-21 [1]}
\CommentTok{#>    mime            0.7        2019-06-11 [1]}
\CommentTok{#>    minqa           1.2.4      2014-10-09 [1]}
\CommentTok{#>    mlbench         2.1-1      2012-07-10 [1]}
\CommentTok{#>    ModelMetrics    1.2.2      2018-11-03 [1]}
\CommentTok{#>    munsell         0.5.0      2018-06-12 [1]}
\CommentTok{#>    mvtnorm         1.0-10     2019-03-05 [1]}
\CommentTok{#>    nlme            3.1-139    2019-04-09 [1]}
\CommentTok{#>    nloptr          1.2.1      2018-10-03 [1]}
\CommentTok{#>    nnet            7.3-12     2016-02-02 [1]}
\CommentTok{#>    numDeriv        2016.8-1   2016-08-27 [1]}
\CommentTok{#>    openxlsx        4.1.0.1    2019-05-28 [1]}
\CommentTok{#>    partykit        1.2-3      2019-01-31 [1]}
\CommentTok{#>    pbkrtest        0.4-7      2017-03-15 [1]}
\CommentTok{#>    pBrackets       1.0        2014-10-17 [1]}
\CommentTok{#>    pdp             0.7.0      2018-08-27 [1]}
\CommentTok{#>    pillar          1.4.1      2019-05-28 [1]}
\CommentTok{#>    pkgconfig       2.0.2      2018-08-16 [1]}
\CommentTok{#>    plogr           0.2.0      2018-03-25 [1]}
\CommentTok{#>    plotmo          3.5.4      2019-04-06 [1]}
\CommentTok{#>    plotrix         3.7-5      2019-04-07 [1]}
\CommentTok{#>    pls             2.7-1      2019-03-23 [1]}
\CommentTok{#>    plyr            1.8.4      2016-06-08 [1]}
\CommentTok{#>    polynom         1.4-0      2019-03-22 [1]}
\CommentTok{#>    prediction      0.3.6.2    2019-01-31 [1]}
\CommentTok{#>    prettyunits     1.0.2      2015-07-13 [1]}
\CommentTok{#>    pROC            1.14.0     2019-03-12 [1]}
\CommentTok{#>    processx        3.3.0      2019-03-10 [1]}
\CommentTok{#>    prodlim         2018.04.18 2018-04-18 [1]}
\CommentTok{#>    progress        1.2.2      2019-05-16 [1]}
\CommentTok{#>    ps              1.3.0      2018-12-21 [1]}
\CommentTok{#>    purrr         * 0.3.2      2019-03-15 [1]}
\CommentTok{#>    quantreg        5.38       2018-12-18 [1]}
\CommentTok{#>    R6              2.4.0      2019-02-14 [1]}
\CommentTok{#>    ranger          0.11.2     2019-03-07 [1]}
\CommentTok{#>    RColorBrewer    1.1-2      2014-12-07 [1]}
\CommentTok{#>    Rcpp            1.0.1      2019-03-17 [1]}
\CommentTok{#>    RcppEigen       0.3.3.5.0  2018-11-24 [1]}
\CommentTok{#>    RcppRoll        0.3.0      2018-06-05 [1]}
\CommentTok{#>    RCurl           1.95-4.12  2019-03-04 [1]}
\CommentTok{#>    readr         * 1.3.1      2018-12-21 [1]}
\CommentTok{#>    readxl          1.3.1      2019-03-13 [1]}
\CommentTok{#>    recipes         0.1.5      2019-03-21 [1]}
\CommentTok{#>    rematch         1.0.1      2016-04-21 [1]}
\CommentTok{#>    reshape2        1.4.3      2017-12-11 [1]}
\CommentTok{#>    reticulate      1.12       2019-04-12 [1]}
\CommentTok{#>    rio             0.5.16     2018-11-26 [1]}
\CommentTok{#>    rlang           0.3.4      2019-04-07 [1]}
\CommentTok{#>    rmarkdown       1.13       2019-05-22 [1]}
\CommentTok{#>    ROCR            1.0-7      2015-03-26 [1]}
\CommentTok{#>    rpart           4.1-15     2019-04-12 [1]}
\CommentTok{#>    rpart.plot      3.0.7      2019-04-12 [1]}
\CommentTok{#>    rsample       * 0.0.4      2019-01-07 [1]}
\CommentTok{#>    rstudioapi      0.10       2019-03-19 [1]}
\CommentTok{#>    scales          1.0.0      2018-08-09 [1]}
\CommentTok{#>    scatterplot3d   0.3-41     2018-03-14 [1]}
\CommentTok{#>    sp              1.3-1      2018-06-05 [1]}
\CommentTok{#>    SparseM         1.77       2017-04-23 [1]}
\CommentTok{#>    SQUAREM         2017.10-1  2017-10-07 [1]}
\CommentTok{#>    stringi         1.4.3      2019-03-12 [1]}
\CommentTok{#>    stringr       * 1.4.0      2019-02-10 [1]}
\CommentTok{#>    survival        2.44-1.1   2019-04-01 [1]}
\CommentTok{#>    TeachingDemos   2.10       2016-02-12 [1]}
\CommentTok{#>    tensorflow      1.13.1     2019-04-05 [1]}
\CommentTok{#>    tfestimators    1.9.1      2018-11-07 [1]}
\CommentTok{#>    tfruns          1.4        2018-08-25 [1]}
\CommentTok{#>    tibble        * 2.1.2      2019-05-29 [1]}
\CommentTok{#>    tidyr         * 0.8.3      2019-03-01 [1]}
\CommentTok{#>    tidyselect      0.2.5      2018-10-11 [1]}
\CommentTok{#>    timeDate        3043.102   2018-02-21 [1]}
\CommentTok{#>    tinytex         0.13       2019-05-14 [1]}
\CommentTok{#>    utf8            1.1.4      2018-05-24 [1]}
\CommentTok{#>    vctrs           0.1.0      2018-11-29 [1]}
\CommentTok{#>    vip             0.1.2.9000 2019-06-04 [1]}
\CommentTok{#>    viridis         0.5.1      2018-03-29 [1]}
\CommentTok{#>    viridisLite     0.3.0      2018-02-01 [1]}
\CommentTok{#>    whisker         0.3-2      2013-04-28 [1]}
\CommentTok{#>    withr           2.1.2      2018-03-15 [1]}
\CommentTok{#>    xfun            0.7        2019-05-14 [1]}
\CommentTok{#>    xgboost         0.82.1     2019-03-11 [1]}
\CommentTok{#>    yaImpute        1.0-31     2019-01-09 [1]}
\CommentTok{#>    yaml            2.2.0      2018-07-25 [1]}
\CommentTok{#>    zeallot         0.1.0      2018-01-28 [1]}
\CommentTok{#>    zip             2.0.2      2019-05-13 [1]}
\CommentTok{#>  source                         }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  Github (hadley/emo@02a5206)    }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  <NA>                           }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  Github (tidyverse/glue@ea0edcb)}
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  Github (koalaverse/vip@9d537bb)}
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#>  CRAN (R 3.6.0)                 }
\CommentTok{#> }
\CommentTok{#> [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library}
\CommentTok{#> }
\CommentTok{#>  R -- Package was removed from disk.}
\end{Highlighting}
\end{Shaded}

\mainmatter

\hypertarget{part-fundamentals}{%
\part{Fundamentals}\label{part-fundamentals}}

\hypertarget{intro}{%
\chapter{Introduction to Machine Learning}\label{intro}}

Machine learning (ML) continues to grow in importance for many organizations across nearly all domains. Some example applications of machine learning in practice include:

\begin{itemize}
\tightlist
\item
  Predicting the likelihood of a patient returning to the hospital (\emph{readmission}) within 30 days of discharge.
\item
  Segmenting customers based on common attributes or purchasing behavior for targeted marketing.
\item
  Predicting coupon redemption rates for a given marketing campaign.
\item
  Predicting customer churn so an organization can perform preventative intervention.
\item
  And many more!
\end{itemize}

In essence, these tasks all seek to learn from data. To address each scenario, we can use a given set of \emph{features}\index{features} to train an algorithm and extract insights. These algorithms, or \emph{learners}\index{learners}, can be classified according to the amount and type of supervision needed during training. The two main groups this book focuses on are: \textbf{\emph{supervised learners}} which construct predictive models, and \textbf{\emph{unsupervised learners}} which build descriptive models. Which type you will need to use depends on the learning task you hope to accomplish.

\hypertarget{supervised-learning}{%
\section{Supervised learning}\label{supervised-learning}}

A \textbf{\emph{predictive model}}\index{predictive model} is used for tasks that involve the prediction of a given output (or target) using other variables (or features) in the data set. Or, as stated by \citet[p.~2]{apm}, predictive modeling is ``\ldots{}the process of developing a mathematical tool or model that generates an accurate prediction.'' The learning algorithm in a predictive model attempts to discover and model the relationships among the target variable (the variable being predicted) and the other features (aka predictor variables). Examples of predictive modeling include:

\begin{itemize}
\tightlist
\item
  using customer attributes to predict the probability of the customer churning in the next 6 weeks;
\item
  using home attributes to predict the sales price;
\item
  using employee attributes to predict the likelihood of attrition;
\item
  using patient attributes and symptoms to predict the risk of readmission;
\item
  using production attributes to predict time to market.
\end{itemize}

Each of these examples have a defined learning task; they each intend to use attributes (\(X\)) to predict an outcome measurement (\(Y\)).

\begin{note}
Throughout this text we'll use various terms interchangeably for:

\begin{itemize}
\tightlist
\item
  \(X\): ``predictor variables'', ``independent variables'',
  ``attributes'', ``features'', ``predictors''
\item
  \(Y\): ``target variable'', ``dependent variable'', ``response'',
  ``outcome measurement''
\end{itemize}
\end{note}

The predictive modeling examples above describe what is known as \emph{supervised learning}\index{supervised learning}. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible.

\begin{note}
In supervised learning, the training data you feed the algorithm
includes the target values. Consequently, the solutions can be used to
help \emph{supervise} the training process to find the optimal algorithm
parameters.
\end{note}

Most supervised learning problems can be bucketed into one of two categories: \emph{regression}\index{regression} or \emph{classification}\index{classification}, which we discuss next.

\hypertarget{regression-problems}{%
\subsection{Regression problems}\label{regression-problems}}

When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a \textbf{\emph{regression problem}} (not to be confused with linear regression modeling). Regression problems revolve around predicting output that falls on a continuum. In the examples above, predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous. This means, given the combination of predictor values, the response value could fall anywhere along some continuous spectrum (e.g., the predicted sales price of a particular home could be between \$80,000 and \$755,000). Figure \ref{fig:regression-problem} illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along a plane.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/regression-problem-1} 

}

\caption{Average home sales price as a function of year built and total square footage.}\label{fig:regression-problem}
\end{figure}

\hypertarget{classification-problems}{%
\subsection{Classification problems}\label{classification-problems}}

When the objective of our supervised learning is to predict a categorical outcome, we refer to this as a \textbf{\emph{classification problem}}. Classification problems most commonly revolve around predicting a binary or multinomial response measure such as:

\begin{itemize}
\tightlist
\item
  Did a customer redeem a coupon (coded as yes/no or 1/0).
\item
  Did a customer churn (coded as yes/no or 1/0).
\item
  Did a customer click on our online ad (coded as yes/no or 1/0).
\item
  Classifying customer reviews:

  \begin{itemize}
  \tightlist
  \item
    Binary: positive vs.~negative.
  \item
    Multinomial: extremely negative to extremely positive on a 0--5 Likert scale.
  \end{itemize}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/classification_problem} 

}

\caption{Classification problem modeling 'Yes'/'No' response based on three features.}\label{fig:classification-problem}
\end{figure}

However, when we apply machine learning models for classification problems, rather than predict a particular class (i.e., ``yes'' or ``no''), we often want to predict the \emph{probability} of a particular class (i.e., yes: 0.65, no: 0.35). By default, the class with the highest predicted probability becomes the predicted class. Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability). However, the essence of the problem still makes it a classification problem.

Although there are machine learning algorithms that can be applied to regression problems but not classification and vice versa, most of the supervised learning algorithms we cover in this book can be applied to both. These algorithms have become the most popular machine learning applications in recent years.

\hypertarget{unsupervised-learning}{%
\section{Unsupervised learning}\label{unsupervised-learning}}

\textbf{\emph{Unsupervised learning}}\index{unsupervised learning}, in contrast to supervised learning, includes a set of statistical tools to better understand and describe your data, but performs the analysis without a target variable. In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., \emph{clustering}) or the columns (i.e., \emph{dimension reduction}); however, the motive in each case is quite different.

The goal of \textbf{\emph{clustering}}\index{clustering} is to segment observations into similar groups based on the observed variables; for example, to divide consumers into different homogeneous groups, a process known as market segmentation. In \textbf{dimension reduction}\index{dimension reduction}, we are often concerned with reducing the number of variables in a data set. For example, classical linear regression models break down in the presence of highly correlated features. Some dimension reduction techniques can be used to reduce the feature set to a potentially smaller set of uncorrelated variables. Such a reduced feature set is often used as input to downstream supervised learning models (e.g., principal component regression).

Unsupervised learning is often performed as part of an exploratory data analysis (EDA). However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e., linear regression), then it is possible to check our work by seeing how well our model predicts the response \emph{Y} on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don't know the true answer---the problem is unsupervised!

Despide its subjectivity, the importance of unsupervised learning should not be overlooked and such techniques are often used in organizations to:

\begin{itemize}
\tightlist
\item
  Divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment.
\item
  Identify groups of online shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers.
\item
  Identify products that have similar purchasing behavior so that managers can manage them as product groups.
\end{itemize}

These questions, and many more, can be addressed with unsupervised learning. Moreover, the outputs of an unsupervised learning models can be used as inputs to downstream supervised learning models.

\hypertarget{roadmap}{%
\section{Roadmap}\label{roadmap}}

The goal of this book is to provide effective tools for uncovering relevant and useful patterns in your data by using R's ML stack. We begin by providing an overview of the ML modeling process and discussing fundamental concepts that will carry through the rest of the book. These include feature engineering, data splitting, model validation and tuning, and performance measurement. These concepts will be discussed in Chapters \ref{process}-\ref{engineering}.

Chapters \ref{linear-regression}-\ref{svm} focus on common supervised learners ranging from simpler linear regression models to the more complicated gradient boosting machines and deep neural networks. Here we will illustrate the fundamental concepts of each base learning algorithm and how to tune its hyperparameters to maximize predictive performance.

Chapters \ref{stacking}-\ref{iml} delve into more advanced approaches to maximize effectiveness, efficiency, and interpretation of your ML models. We discuss how to combine multiple models to create a stacked model (aka \emph{super learner}\index{super learnier}), which allows you to combine the strengths from each base learner and further maximize predictive accuracy. We then illustrate how to make the training and validation process more efficient with automated ML (aka AutoML). Finally, we illustrate many ways to extract insight from your ``black box'' models with various ML interpretation techniques.

The latter part of the book focuses on unsupervised techniques aimed at reducing the dimensions of your data for more effective data representation (Chapters \ref{pca}-\ref{autoencoders}) and identifying common groups among your observations with clustering techniques (Chapters \ref{kmeans}-\ref{model-clustering}).

\hypertarget{data}{%
\section{The data sets}\label{data}}

The data sets chosen for this book allow us to illustrate the different features of the presented machine learning algorithms. Since the goal of this book is to demonstrate how to implement R's ML stack, we make the assumption that you have already spent significant time cleaning and getting to know your data via EDA. This would allow you to perform many necessary tasks prior to the ML tasks outlined in this book such as:

\begin{itemize}
\tightlist
\item
  Feature selection (i.e., removing unnecessary variables and retaining only those variables you wish to include in your modeling process).
\item
  Recoding variable names and values so that they are meaningful and more interpretable.
\item
  Recoding, removing, or some other approach to handling missing values.
\end{itemize}

Consequently, the exemplar data sets we use throughout this book have, for the most part, gone through the necessary cleaning processes. In some cases we illustrate concepts with stereotypical data sets (i.e. \texttt{mtcars}, \texttt{iris}, \texttt{geyser}); however, we tend to focus most of our discussion around the following data sets:

\begin{itemize}
\tightlist
\item
  Property sales information as described in \citet{de2011ames}.

  \begin{itemize}
  \tightlist
  \item
    \textbf{problem type}: supervised regression
  \item
    \textbf{response variable}: \texttt{Sale\_Price} (i.e., \$195,000, \$215,000)
  \item
    \textbf{features}: 80
  \item
    \textbf{observations}: 2,930
  \item
    \textbf{objective}: use property attributes to predict the sale price of a home
  \item
    \textbf{access}: provided by the \texttt{AmesHousing} package \citep{R-ames}
  \item
    \textbf{more details}: See \texttt{?AmesHousing::ames\_raw}
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# access data}
\NormalTok{ames <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{()}

\CommentTok{# initial dimension}
\KeywordTok{dim}\NormalTok{(ames)}
\CommentTok{## [1] 2930   81}

\CommentTok{# response variable}
\KeywordTok{head}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Sale_Price)}
\CommentTok{## [1] 215000 105000 172000 244000 189900 195500}
\end{Highlighting}
\end{Shaded}

  \begin{note}
    You can see the entire data cleaning process to transform the raw Ames
    housing data (\texttt{AmesHousing::ames\_raw}) to the final clean data
    (\texttt{AmesHousing::make\_ames}) that we will use in machine learning
    algorithms throughout this book at:

    \url{https://github.com/topepo/AmesHousing/blob/master/R/make_ames.R}
    \end{note}
\item
  Employee attrition information originally provided by \href{https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/}{IBM Watson Analytics Lab}.

  \begin{itemize}
  \tightlist
  \item
    \textbf{problem type}: supervised binomial classification
  \item
    \textbf{response variable}: \texttt{Attrition} (i.e., ``Yes'', ``No'')
  \item
    \textbf{features}: 30
  \item
    \textbf{observations}: 1,470
  \item
    \textbf{objective}: use employee attributes to predict if they will attrit (leave the company)
  \item
    \textbf{access}: provided by the \texttt{rsample} package \citep{R-rsample}
  \item
    \textbf{more details}: See \texttt{?rsample::attrition}
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# access data}
\NormalTok{attrition <-}\StringTok{ }\NormalTok{rsample}\OperatorTok{::}\NormalTok{attrition}

\CommentTok{# initial dimension}
\KeywordTok{dim}\NormalTok{(attrition)}
\CommentTok{## [1] 1470   31}

\CommentTok{# response variable}
\KeywordTok{head}\NormalTok{(attrition}\OperatorTok{$}\NormalTok{Attrition)}
\CommentTok{## [1] Yes No  Yes No  No  No }
\CommentTok{## Levels: No Yes}
\end{Highlighting}
\end{Shaded}
\item
  Image information for handwritten numbers originally presented to AT\&T Bell Lab's to help build automatic mail-sorting machines for the USPS. Has been used since early 1990s to compare machine learning performance on pattern recognition (i.e., \citet{lecun1990handwritten}; \citet{lecun1998gradient}; \citet{cirecsan2012multi}).

  \begin{itemize}
  \tightlist
  \item
    \textbf{Problem type}: supervised multinomial classification
  \item
    \textbf{response variable}: \texttt{V785} (i.e., numbers to predict: 0, 1, \ldots{}, 9)
  \item
    \textbf{features}: 784
  \item
    \textbf{observations}: 60,000 (train) / 10,000 (test)
  \item
    \textbf{objective}: use attributes about the ``darkness'' of each of the 784 pixels in images of handwritten numbers to predict if the number is 0, 1, \ldots{}, or 9.
  \item
    \textbf{access}: provided by the \texttt{dslabs} package \citep{R-dslabs}
  \item
    \textbf{more details}: See \texttt{?dslabs::read\_mnist()} and \href{http://yann.lecun.com/exdb/mnist/}{online MNIST documentation}
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#access data}
\NormalTok{mnist <-}\StringTok{ }\NormalTok{dslabs}\OperatorTok{::}\KeywordTok{read_mnist}\NormalTok{()}
\KeywordTok{names}\NormalTok{(mnist)}
\CommentTok{## [1] "train" "test"}

\CommentTok{# initial feature dimensions}
\KeywordTok{dim}\NormalTok{(mnist}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{images)}
\CommentTok{## [1] 60000   784}

\CommentTok{# response variable}
\KeywordTok{head}\NormalTok{(mnist}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{labels)}
\CommentTok{## [1] 5 0 4 1 9 2}
\end{Highlighting}
\end{Shaded}
\item
  Grocery items and quantities purchased. Each observation represents a single basket of goods that were purchased together.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Problem type}: unsupervised basket analysis
  \item
    \textbf{response variable}: NA
  \item
    \textbf{features}: 42
  \item
    \textbf{observations}: 2,000
  \item
    \textbf{objective}: use attributes of each basket to identify common groupings of items purchased together.
  \item
    \textbf{access}: available via additional online material
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# access data}
\NormalTok{my_basket <-}\StringTok{ }\NormalTok{readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"data/my_basket.csv"}\NormalTok{)}

\CommentTok{# initial dimension}
\KeywordTok{dim}\NormalTok{(my_basket)}
\CommentTok{## [1] 2000   42}

\CommentTok{# response variable}
\NormalTok{my_basket}
\CommentTok{## # A tibble: 2,000 x 42}
\CommentTok{##    `7up` lasagna pepsi   yop `red-wine` cheese   bbq}
\CommentTok{##    <dbl>   <dbl> <dbl> <dbl>      <dbl>  <dbl> <dbl>}
\CommentTok{##  1     0       0     0     0          0      0     0}
\CommentTok{##  2     0       0     0     0          0      0     0}
\CommentTok{##  3     0       0     0     0          0      0     0}
\CommentTok{##  4     0       0     0     2          1      0     0}
\CommentTok{##  5     0       0     0     0          0      0     0}
\CommentTok{##  6     0       0     0     0          0      0     0}
\CommentTok{##  7     1       1     0     0          0      0     1}
\CommentTok{##  8     0       0     0     0          0      0     0}
\CommentTok{##  9     0       1     0     0          0      0     0}
\CommentTok{## 10     0       0     0     0          0      0     0}
\CommentTok{## # ... with 1,990 more rows, and 35 more variables:}
\CommentTok{## #   bulmers <dbl>, mayonnaise <dbl>, horlics <dbl>,}
\CommentTok{## #   `chicken-tikka` <dbl>, milk <dbl>, mars <dbl>,}
\CommentTok{## #   coke <dbl>, lottery <dbl>, bread <dbl>,}
\CommentTok{## #   pizza <dbl>, `sunny-delight` <dbl>, ham <dbl>,}
\CommentTok{## #   lettuce <dbl>, kronenbourg <dbl>, leeks <dbl>,}
\CommentTok{## #   fanta <dbl>, tea <dbl>, whiskey <dbl>, peas <dbl>,}
\CommentTok{## #   newspaper <dbl>, muesli <dbl>, `white-wine` <dbl>,}
\CommentTok{## #   carrots <dbl>, spinach <dbl>, pate <dbl>,}
\CommentTok{## #   `instant-coffee` <dbl>, twix <dbl>,}
\CommentTok{## #   potatoes <dbl>, fosters <dbl>, soup <dbl>,}
\CommentTok{## #   `toad-in-hole` <dbl>, `coco-pops` <dbl>,}
\CommentTok{## #   kitkat <dbl>, broccoli <dbl>, cigarettes <dbl>}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\hypertarget{process}{%
\chapter{Modeling Process}\label{process}}

Much like EDA, the ML process is very iterative and heurstic-based. With minimal knowledge of the problem or data at hand, it is difficult to know which ML method will perform best. This is known as the \emph{no free lunch}\index{no free lunch} theorem for ML \citep{wolpert1996lack}. Consequently, it is common for many ML approaches to be applied, evaluated, and modified before a final, optimal model can be determined. Performing this process correctly provides great confidence in our outcomes. If not, the results will be useless and, potentially, damaging \footnote{See \url{https://www.fatml.org/resources/relevant-scholarship} for many discussions regarding implications of poorly applied and interpreted ML.}.

Approaching ML modeling correctly means approaching it strategically by spending our data wisely on learning and validation procedures, properly pre-processing the feature and target variables, minimizing \emph{data leakage}\index{data leakage} (Section \ref{data-leakage}), tuning hyperparameters, and assessing model performance. Many books and courses portray the modeling process as a short sprint. A better analogy would be a marathon where many iterations of these steps are repeated before eventually finding the final optimal model. This process is illustrated in Figure \ref{fig:02-modeling-process}. Before introducing specific algorithms, this chapter, and the next, introduce concepts that are fundamental to the ML modeling process and that you'll see briskly covered in future modeling chapters.

\begin{note}
Although the discussions in this chapter focuses on supervised ML
modeling, many of the topics also apply to unsupervised methods.
\end{note}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth,height=0.9\textheight]{images/modeling_process} 

}

\caption{General predictive machine learning process.}\label{fig:02-modeling-process}
\end{figure}

\hypertarget{prerequisites}{%
\section{Prerequisites}\label{prerequisites}}

This chapter leverages the following packages.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Helper packages}
\KeywordTok{library}\NormalTok{(dplyr)     }\CommentTok{# for data manipulation}
\KeywordTok{library}\NormalTok{(ggplot2)   }\CommentTok{# for awesome graphics}

\CommentTok{# Modeling process packages}
\KeywordTok{library}\NormalTok{(rsample)   }\CommentTok{# for resampling procedures}
\KeywordTok{library}\NormalTok{(caret)     }\CommentTok{# for resampling and model training}
\KeywordTok{library}\NormalTok{(h2o)       }\CommentTok{# for resampling and model training}

\CommentTok{# h2o set-up }
\KeywordTok{h2o.no_progress}\NormalTok{()  }\CommentTok{# turn off h2o progress bars}
\KeywordTok{h2o.init}\NormalTok{()         }\CommentTok{# launch h2o}
\CommentTok{##  Connection successful!}
\CommentTok{## }
\CommentTok{## R is connected to the H2O cluster: }
\CommentTok{##     H2O cluster uptime:         17 minutes 19 seconds }
\CommentTok{##     H2O cluster timezone:       America/New_York }
\CommentTok{##     H2O data parsing timezone:  UTC }
\CommentTok{##     H2O cluster version:        3.22.1.1 }
\CommentTok{##     H2O cluster version age:    5 months and 28 days !!! }
\CommentTok{##     H2O cluster name:           H2O_started_from_R_b294776_vmp196 }
\CommentTok{##     H2O cluster total nodes:    1 }
\CommentTok{##     H2O cluster total memory:   3.28 GB }
\CommentTok{##     H2O cluster total cores:    8 }
\CommentTok{##     H2O cluster allowed cores:  8 }
\CommentTok{##     H2O cluster healthy:        TRUE }
\CommentTok{##     H2O Connection ip:          localhost }
\CommentTok{##     H2O Connection port:        54321 }
\CommentTok{##     H2O Connection proxy:       NA }
\CommentTok{##     H2O Internal Security:      FALSE }
\CommentTok{##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 }
\CommentTok{##     R Version:                  R version 3.6.0 (2019-04-26)}
\end{Highlighting}
\end{Shaded}

To illustrate some of the concepts, we'll use the Ames Housing and employee attrition data sets introduced in Chapter \ref{intro}. Throughout this book, we'll demonstrate approaches with ordinary R data frames. However, since many of the supervised machine learning chapters leverage the \textbf{h2o} package, we'll also show how to do some of the tasks with H2O objects. You can convert any R data frame to an H2O object (i.e., import it to the H2O cloud) easily with \texttt{as.h2o(\textless{}my-data-frame\textgreater{})}.

\begin{warning}
If you try to convert the original \texttt{rsample::attrition} data set
to an H2O object an error will occur. This is because several variables
are \emph{ordered factors} and H2O has no way of handling this data
type. Consequently, you must convert any ordered factors to unordered;
see \texttt{?base::ordered} for details.
\end{warning}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ames data}
\NormalTok{ames <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{()}
\NormalTok{ames.h2o <-}\StringTok{ }\KeywordTok{as.h2o}\NormalTok{(ames)}

\CommentTok{# attrition data}
\NormalTok{churn <-}\StringTok{ }\NormalTok{rsample}\OperatorTok{::}\NormalTok{attrition }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate_if}\NormalTok{(is.ordered, factor, }\DataTypeTok{ordered =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{churn.h2o <-}\StringTok{ }\KeywordTok{as.h2o}\NormalTok{(churn)}
\end{Highlighting}
\end{Shaded}

\hypertarget{splitting}{%
\section{Data splitting}\label{splitting}}

A major goal of the machine learning process is to find an algorithm \(f\left(X\right)\) that most accurately predicts future values (\(\hat{Y}\)) based on a set of features (\(X\)). In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the \textbf{\emph{generalizability}}\index{generalizability} of our algorithm. How we ``spend'' our data will help us understand how well our algorithm generalizes to unseen data.

To provide an accurate understanding of the generalizability of our final optimal model, we can split our data into training and test data sets:

\begin{itemize}
\tightlist
\item
  \textbf{Training set}: these data are used to develop feature sets, train our algorithms, tune hyperparameters, compare models, and all of the other activities required to choose a final model (e.g., the model we want to put into production).
\item
  \textbf{Test set}: having chosen a final model, these data are used to estimate an unbiased assessment of the model's performance, which we refer to as the \emph{generalization error}.
\end{itemize}

\begin{warning}
It is critical that the test set not be used prior to selecting your
final model. Assessing results on the test set prior to final model
selection biases the model selection process since the testing data will
have become part of the model development process.
\end{warning}

\begin{figure}

{\centering \includegraphics[width=3.12in]{images/data_split} 

}

\caption{Splitting data into training and test sets.}\label{fig:02-split}
\end{figure}

Given a fixed amount of data, typical recommendations for splitting your data into training-test splits include 60\% (training)--40\% (testing), 70\%--30\%, or 80\%--20\%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep the following points in mind:

\begin{itemize}
\tightlist
\item
  Spending too much in training (e.g., \(>80\%\)) won't allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (\emph{overfitting}).
\item
  Sometimes too much spent in testing (\(>40\%\)) won't allow us to get a good assessment of model parameters.
\end{itemize}

Other factors should also influence the allocation proportions. For example, very large training sets (e.g., \(n > 100\texttt{K}\)) often result in only marginal gains compared to smaller sample sizes. Consequently, you may use a smaller training sample to increase computation speed (e.g., models built on larger training sets often take longer to score new data sets in production). In contrast, as \(p \geq n\) (where \(p\) represents the number of features), larger samples sizes are often required to identify consistent signals in the features.

The two most common ways of splitting data include \textbf{\emph{simple random sampling}}\index{simple random sampling} and \textbf{\emph{stratified sampling}}\index{stratified sampling}.

\hypertarget{simple-random-sampling}{%
\subsection{Simple random sampling}\label{simple-random-sampling}}

The simplest way to split the data into training and test sets is to take a simple random sample. This does not control for any data attributes, such as the distribution your response variable (\(Y\)). There are multiple ways to split our data in R. Here we show four options to produce a 70--30 split in the Ames housing data:

\begin{note}
Sampling is a random process so setting the random number generator with
a common seed allows for reproducible results. Throughout this book
we'11 often use the seed \texttt{123} for reproducibility but the number
itself has no special meaning.
\end{note}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# base R}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{index_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(ames), }\KeywordTok{round}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(ames) }\OperatorTok{*}\StringTok{ }\FloatTok{0.7}\NormalTok{))}
\NormalTok{train_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\NormalTok{ames[index_}\DecValTok{1}\NormalTok{, ]}
\NormalTok{test_}\DecValTok{1}\NormalTok{  <-}\StringTok{ }\NormalTok{ames[}\OperatorTok{-}\NormalTok{index_}\DecValTok{1}\NormalTok{, ]}

\CommentTok{# caret package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{index_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Sale_Price, }\DataTypeTok{p =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{train_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\NormalTok{ames[index_}\DecValTok{2}\NormalTok{, ]}
\NormalTok{test_}\DecValTok{2}\NormalTok{  <-}\StringTok{ }\NormalTok{ames[}\OperatorTok{-}\NormalTok{index_}\DecValTok{2}\NormalTok{, ]}

\CommentTok{# rsample package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split_}\DecValTok{1}\NormalTok{  <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(ames, }\DataTypeTok{prop =} \FloatTok{0.7}\NormalTok{)}
\NormalTok{train_}\DecValTok{3}\NormalTok{  <-}\StringTok{ }\KeywordTok{training}\NormalTok{(split_}\DecValTok{1}\NormalTok{)}
\NormalTok{test_}\DecValTok{3}\NormalTok{   <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(split_}\DecValTok{1}\NormalTok{)}

\CommentTok{# h2o package}
\NormalTok{split_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{h2o.splitFrame}\NormalTok{(ames.h2o, }\DataTypeTok{ratios =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{seed =} \DecValTok{123}\NormalTok{)}
\NormalTok{train_}\DecValTok{4}\NormalTok{ <-}\StringTok{ }\NormalTok{split_}\DecValTok{2}\NormalTok{[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{test_}\DecValTok{4}\NormalTok{  <-}\StringTok{ }\NormalTok{split_}\DecValTok{2}\NormalTok{[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

With sufficient sample size, this sampling approach will typically result in a similar distribution of \(Y\) (e.g., \texttt{Sale\_Price} in the \texttt{ames} data) between your training and test sets, as illustrated below.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/distributions-1} 

}

\caption{Training (black) vs. test (red) response distribution.}\label{fig:distributions}
\end{figure}

\hypertarget{stratified}{%
\subsection{Stratified sampling}\label{stratified}}

If we want to explicitly control the sampling so that our training and test sets have similar \(Y\) distributions, we can use stratified sampling. This is more common with classification problems where the response variable may be severely imbalanced (e.g., 90\% of observations with response ``Yes'' and 10\% with response ``No''). However, we can also apply stratified sampling to regression problems for data sets that have a small sample size and where the response variable deviates strongly from normality (i.e., positively skewed like \texttt{Sale\_Price}). With a continuous response variable, stratified sampling will segment \(Y\) into quantiles and randomly sample from each. Consequently, this will help ensure a balanced representation of the response distribution in both the training and test sets.

The easiest way to perform stratified sampling on a response variable is to use the \textbf{rsample} package, where you specify the response variable to \texttt{strata}fy. The following illustrates that in our original employee attrition data we have an imbalanced response (No: 84\%, Yes: 16\%). By enforcing stratified sampling, both our training and testing sets have approximately equal response distributions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# orginal response distribution}
\KeywordTok{table}\NormalTok{(churn}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{prop.table}\NormalTok{()}
\CommentTok{## }
\CommentTok{##     No    Yes }
\CommentTok{## 0.8388 0.1612}

\CommentTok{# stratified sampling with the rsample package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split_strat  <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(churn, }\DataTypeTok{prop =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Attrition"}\NormalTok{)}
\NormalTok{train_strat  <-}\StringTok{ }\KeywordTok{training}\NormalTok{(split_strat)}
\NormalTok{test_strat   <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(split_strat)}

\CommentTok{# consistent response ratio between train & test}
\KeywordTok{table}\NormalTok{(train_strat}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{prop.table}\NormalTok{()}
\CommentTok{## }
\CommentTok{##     No    Yes }
\CommentTok{## 0.8388 0.1612}
\KeywordTok{table}\NormalTok{(test_strat}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{prop.table}\NormalTok{()}
\CommentTok{## }
\CommentTok{##     No    Yes }
\CommentTok{## 0.8386 0.1614}
\end{Highlighting}
\end{Shaded}

\hypertarget{class-imbalances}{%
\subsection{Class imbalances}\label{class-imbalances}}

Imbalanced data can have a significant impact on model predictions and performance \citep{apm}. Most often this involves classification problems where one class has a very small proportion of observations (e.g., defaults - 5\% versus nondefaults - 95\%). Several sampling methods have been developed to help remedy class imbalance and most of them can be categorized as either \emph{up-sampling}\index{up-sampling} or \emph{down-sampling}\index{down-sampling}.

Down-sampling balances the dataset by reducing the size of the abundant class(es) to match the frequencies in the least prevalent class. This method is used when the quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modeling. Furthermore, the reduced sample size reduces the computation burden imposed by further steps in the ML process.

On the contrary, up-sampling is used when the quantity of data is insufficient. It tries to balance the dataset by increasing the size of rarer samples. Rather than getting rid of abundant samples, new rare samples are generated by using repetition or bootstrapping (described further in Section \ref{bootstrapping}).

Note that there is no absolute advantage of one sampling method over another. Application of these two methods depends on the use case it applies to and the data set itself. A combination of over- and under-sampling is often successful and a common approach is known as Synthetic Minority Over-Sampling Technique, or SMOTE \citep{chawla2002smote}. This alternative sampling approach, as well as others, can be implemented in R (see the \texttt{sampling} argument in \texttt{?caret::trainControl()}). Furthermore, many ML algorithms implemented in R have class weighting schemes to remedy imbalances internally (e.g., most \textbf{h2o} algorithms have a \texttt{weights\_column} and \texttt{balance\_classes} argument).

\hypertarget{creating-models-in-r}{%
\section{Creating models in R}\label{creating-models-in-r}}

The R ecosystem provides a wide variety of ML algorithm implementations. This makes many powerful algorithms available at your fingertips. Moreover, there are almost always more than one package to perform each algorithm (e.g., there are over 20 packages for fitting random forests). There are pros and cons to this wide selection; some implementations may be more computationally efficient while others may be more flexible (i.e., have more hyperparameter tuning options). Future chapters will expose you to many of the packages and algorithms that perform and scale best to the kinds of tabular data and problems encountered by most organizations.

However, this also has resulted in some drawbacks as there are inconsistencies in how algorithms allow you to define the formula of interest and how the results and predictions are supplied. In addition to illustrating the more popular and powerful packages, we'll also show you how to use implementations that provide more consistency.

\hypertarget{many-formula-interfaces}{%
\subsection{Many formula interfaces}\label{many-formula-interfaces}}

To fit a model to our data, the model terms must be specified. Historically, there are two main interfaces for doing this. The formula interface using R formula rules to specify a symbolic representation of the terms. For example, \texttt{Y\ \textasciitilde{}\ X} where we say ``Y is a function of X''. To illustrate, suppose we have some generic modeling function called \texttt{model\_fn()} which accepts an R formula, as in the following examples:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sale price as a function of neighborhood and year sold}
\KeywordTok{model_fn}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Neighborhood }\OperatorTok{+}\StringTok{ }\NormalTok{Year_Sold, }\DataTypeTok{data =}\NormalTok{ ames)}

\CommentTok{# Variables + interactions}
\KeywordTok{model_fn}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Neighborhood }\OperatorTok{+}\StringTok{ }\NormalTok{Year_Sold }\OperatorTok{+}\StringTok{ }\NormalTok{Neighborhood}\OperatorTok{:}\NormalTok{Year_Sold, }\DataTypeTok{data =}\NormalTok{ ames)}

\CommentTok{# Shorthand for all predictors}
\KeywordTok{model_fn}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames)}

\CommentTok{# Inline functions / transformations}
\KeywordTok{model_fn}\NormalTok{(}\KeywordTok{log10}\NormalTok{(Sale_Price) }\OperatorTok{~}\StringTok{ }\KeywordTok{ns}\NormalTok{(Longitude, }\DataTypeTok{df =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ns}\NormalTok{(Latitude, }\DataTypeTok{df =} \DecValTok{3}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ ames)}
\end{Highlighting}
\end{Shaded}

This is very convenient but it has some disadvantages. For example:

\begin{itemize}
\tightlist
\item
  You can't nest in-line functions such as performing principal components analysis on the feature set prior to executing the model (\texttt{model\_fn(y\ \textasciitilde{}\ pca(scale(x1),\ scale(x2),\ scale(x3)),\ data\ =\ df)}).
\item
  All the model matrix calculations happen at once and can't be recycled when used in a model function.
\item
  For very wide data sets, the formula method can be extremely inefficient \citep{kuhnFormula}.
\item
  There are limited roles that variables can take which has led to several re-implementations of formulas.
\item
  Specifying multivariate outcomes is clunky and inelegant.
\item
  Not all modeling functions have a formula method (lack of consistency!).
\end{itemize}

Some modeling functions have a non-formula (XY) interface. These functions have separate arguments for the predictors and the outcome(s):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# use separate inputs for X and Y}
\NormalTok{features <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Year_Sold"}\NormalTok{, }\StringTok{"Longitude"}\NormalTok{, }\StringTok{"Latitude"}\NormalTok{)}
\KeywordTok{model_fn}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ ames[, features], }\DataTypeTok{y =}\NormalTok{ ames}\OperatorTok{$}\NormalTok{Sale_Price)}
\end{Highlighting}
\end{Shaded}

This provides more efficient calculations but can be inconvenient if you have transformations, factor variables, interactions, or any other operations to apply to the data prior to modeling.

Overall, it is difficult to determine if a package has one or both of these interfaces. For example, the \texttt{lm()} function, which performs linear regression, only has the formula method. Consequently, until you are familiar with a particular implementation you will need to continue referencing the corresponding help documentation.

A third interface, is to use \emph{variable name specification} where we provide all the data combined in one training frame but we specify the features and response with character strings. This is the interface used by the \textbf{h2o} package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{model_fn}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"Year_Sold"}\NormalTok{, }\StringTok{"Longitude"}\NormalTok{, }\StringTok{"Latitude"}\NormalTok{),}
  \DataTypeTok{y =} \StringTok{"Sale_Price"}\NormalTok{,}
  \DataTypeTok{data =}\NormalTok{ ames.h2o}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

One approach to get around these inconsistencies is to use a meta engine, which we discuss next.

\hypertarget{many-engines}{%
\subsection{Many engines}\label{many-engines}}

Although there are many individual ML packages available, there is also an abundance of meta engines that can be used to help provide consistency. For example, the following all produce the same linear regression model output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm_lm    <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames)}
\NormalTok{lm_glm   <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames, }\DataTypeTok{family =}\NormalTok{ gaussian)}
\NormalTok{lm_caret <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames, }\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{lm()} and \texttt{glm()} are two different algorithm engines that can be used to fit the linear model and \texttt{caret::train()} is a meta engine (aggregator) that allows you to apply almost any direct engine with \texttt{method\ =\ "\textless{}method-name\textgreater{}"}. There are trade-offs to consider when using direct versus meta engines. For example, using direct engines can allow for extreme flexibility but also requires you to familiarize yourself with the unique differences of each implementation. For example, the following highlights the various syntax nuances required to compute and extract predicted class probabilities across different direct engines.\footnote{This table was modified from \citet{kuhnMLtraining2019}}

\begin{longtable}[]{@{}lll@{}}
\caption{Table 1: Syntax for computing predicted class probabilities with direct engines.}\tabularnewline
\toprule
Algorithm & Package & Code\tabularnewline
\midrule
\endfirsthead
\toprule
Algorithm & Package & Code\tabularnewline
\midrule
\endhead
Linear discriminant analysis & \textbf{MASS} & \texttt{predict(obj)}\tabularnewline
Generalized linear model & \textbf{stats} & \texttt{predict(obj,\ type\ =\ "response")}\tabularnewline
Mixture discriminant analysis & \textbf{mda} & \texttt{predict(obj,\ type\ =\ "posterior")}\tabularnewline
Decision tree & \textbf{rpart} & \texttt{predict(obj,\ type\ =\ "prob")}\tabularnewline
Random Forest & \textbf{ranger} & \texttt{predict(obj)\$predictions}\tabularnewline
Gradient boosting machine & \textbf{gbm} & \texttt{predict(obj,\ type\ =\ "response",\ n.trees)}\tabularnewline
\bottomrule
\end{longtable}

Meta engines provide you with more consistency in how you specify inputs and extract outputs but can be less flexible than direct engines. Future chapters will illustrate both approaches. For meta engines, we'll focus on the \textbf{caret} package in the hardcopy of the book while also demonstrating the newer \textbf{parsnip} package in the additional online resources.\footnote{The \textbf{caret} package has been the preferred meta engine over the years; however, the author is now transitioning to fulltime development on \textbf{parsnip}, which is designed to be a more robust and tidy meta engine.}

\hypertarget{resampling}{%
\section{Resampling methods}\label{resampling}}

In section \ref{splitting} we split our data into training and testing sets. Furthermore, we were very explicit about the fact that we \textbf{\emph{do not}} use the test set to assess model performance during the training phase. So how do we assess the generalization performance of the model?

One option is to assess an error metric based on the training data. Unfortunately, this leads to biased results as some models can perform very well on the training data but not generalize well to a new data set (we'll illustrate this in Section \ref{bias-var}).

A second method is to use a \emph{validation}\index{validation} approach, which involves splitting the training set further to create two parts (as in Section \ref{splitting}): a training set and a validation set (or \emph{holdout set}). We can then train our model(s) on the new training set and estimate the performance on the validation set. Unfortunately, validation using a single holdout set can be highly variable and unreliable unless you are working with very large data sets \citep{molinaro2005prediction, hawkins2003assessing}. As the size of your data set reduces, this concern increases.

\begin{note}
Although we stick to our definitions of test, validation, and holdout
sets, these terms are sometimes used interchangeably in other literature
and software. What's important to remember is to always put a portion of
the data under lock and key until a final model has been selected (we
refer to this as the test data, but others refer to it as the holdout
set).
\end{note}

\textbf{Resampling methods}\index{resampling methods} provide an alternative approach by allowing us to repeatedly fit a model of interest to parts of the training data and testing the performance on other parts. The two most commonly used resampling methods include \emph{k-fold cross validation}\index{k-fold cross validation} and \emph{bootstrapping}\index{bootstrapping}.

\hypertarget{k-fold-cross-validation}{%
\subsection{\texorpdfstring{\emph{k}-fold cross validation}{k-fold cross validation}}\label{k-fold-cross-validation}}

\emph{k}-fold cross-validation (aka \emph{k}-fold CV) is a resampling method that randomly divides the training data into \emph{k} groups (aka folds) of approximately equal size. The model is fit on \(k-1\) folds and then the remaining fold is used to compute model performance. This procedure is repeated \emph{k} times; each time, a different fold is treated as the validation set. This process results in \emph{k} estimates of the generalization error (say \(\epsilon_1, \epsilon_2, \dots, \epsilon_k\)). Thus, the \emph{k}-fold CV estimate is computed by averaging the \emph{k} test errors, providing us with an approximation of the error we might expect on unseen data.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth,height=0.9\textheight]{images/cv} 

}

\caption{Illustration of the k-fold cross validation process.}\label{fig:02-cv}
\end{figure}

Consequently, with \emph{k}-fold CV, every observation in the training data will be held out one time to be included in the test set as illustrated in Figure \ref{fig:crossv}. In practice, one typically uses \(k = 5\) or \(k = 10\). There is no formal rule as to the size of \emph{k}; however, as \emph{k} gets larger, the difference between the estimated performance and the true performance to be seen on the test set will decrease. On the other hand, using too large of \emph{k} can introduce computational burdens. Moreover, \citet{molinaro2005prediction} found that \(k=10\) performed similarly to leave-one-out cross validation (LOOCV) which is the most extreme approach (i.e., setting \(k = n\)).

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/crossv-1} 

}

\caption{10-fold cross validation on 32 observations. Each observation is used once for validation and nine times for training.}\label{fig:crossv}
\end{figure}

Although using \(k \geq 10\) helps to minimize the variability in the estimated performance, \emph{k}-fold CV still tends to have higher variability than bootstrapping (discussed next). \citet{kim2009estimating} showed that repeating \emph{k}-fold CV can help to increase the precision of the estimated generalization error. Consequently, for smaller data sets (say \(n < 10,000\)), 10-fold CV repeated 5 or 10 times will improve the accuracy of your estimated performance and also provide an estimate of its variability.

Throughout this book we'll cover multiple ways to incorporate CV as you can often perform CV directly within certain ML functions:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# example in h2o}
\NormalTok{h2o.cv <-}\StringTok{ }\KeywordTok{h2o.glm}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ x, }
  \DataTypeTok{y =}\NormalTok{ y, }
  \DataTypeTok{training_frame =}\NormalTok{ ames.h2o,}
  \DataTypeTok{nfolds =} \DecValTok{10}  \CommentTok{# perform 10-fold CV}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Or externally as in the below chunk\footnote{\texttt{rsample::vfold\_cv()} results in a nested data frame where each element in \texttt{splits} is a list containing the training data frame and the observation IDs that will be used for training the model vs.~model validation.}. When applying it externally to an ML algorithm as below, we'll need a process to apply the ML model to each resample, which we'll also cover.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vfold_cv}\NormalTok{(ames, }\DataTypeTok{v =} \DecValTok{10}\NormalTok{)}
\CommentTok{## #  10-fold cross-validation }
\CommentTok{## # A tibble: 10 x 2}
\CommentTok{##    splits             id    }
\CommentTok{##    <list>             <chr> }
\CommentTok{##  1 <split [2.6K/293]> Fold01}
\CommentTok{##  2 <split [2.6K/293]> Fold02}
\CommentTok{##  3 <split [2.6K/293]> Fold03}
\CommentTok{##  4 <split [2.6K/293]> Fold04}
\CommentTok{##  5 <split [2.6K/293]> Fold05}
\CommentTok{##  6 <split [2.6K/293]> Fold06}
\CommentTok{##  7 <split [2.6K/293]> Fold07}
\CommentTok{##  8 <split [2.6K/293]> Fold08}
\CommentTok{##  9 <split [2.6K/293]> Fold09}
\CommentTok{## 10 <split [2.6K/293]> Fold10}
\end{Highlighting}
\end{Shaded}

\hypertarget{bootstrapping}{%
\subsection{Bootstrapping}\label{bootstrapping}}

A bootstrap sample is a random sample of the data taken \emph{with replacement} \citep{efron1986bootstrap}. This means that, after a data point is selected for inclusion in the subset, it's still available for further selection. A bootstrap sample is the same size as the original data set from which it was constructed. Figure \ref{fig:bootstrapscheme} provides a schematic of bootstrap sampling where each bootstrap sample contains 12 observations just as in the original data set. Furthermore, bootstrap sampling will contain approximately the same distribution of values (represented by colors) as the original data set.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth,height=0.7\textheight]{images/bootstrap-scheme} 

}

\caption{Illustration of the bootstrapping process.}\label{fig:bootstrapscheme}
\end{figure}

Since samples are drawn with replacement, each bootstrap sample is likely to contain duplicate values. In fact, on average, \(\approx 63.21\)\% of the original sample ends up in any particular bootstrap sample. The original observations not contained in a particular bootstrap sample are considered \emph{out-of-bag} (OOB). When bootstrapping, a model can be built on the selected samples and validated on the OOB samples; this is often done, for example, in random forests (\ref{random-forest}).

Since observations are replicated in bootstrapping, there tends to be less variability in the error measure compared with \emph{k}-fold CV \citep{efron1983estimating}. However, this can also increase the bias of your error estimate. This can be problematic with smaller data sets; however, for most average-to-large data sets (say \(n \geq 1,000\)) this concern is often negligable.

Figure \ref{fig:sampling-comparison} compares bootstrapping to 10-fold CV on a small data set with \(n = 32\) observations. A thorough introduction to the bootstrap and its use in R is provided in \citet{davison1997bootstrap}.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/sampling-comparison-1} 

}

\caption{Bootstrap sampling (left) versus 10-fold cross validation (right) on 32 observations. For bootstrap sampling, the observations that have zero replications (white) are the out-of-bag observations used for validation.}\label{fig:sampling-comparison}
\end{figure}

We can create bootstrap samples easily with \texttt{rsample::bootstraps()};

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{bootstraps}\NormalTok{(ames, }\DataTypeTok{times =} \DecValTok{10}\NormalTok{)}
\CommentTok{## # Bootstrap sampling }
\CommentTok{## # A tibble: 10 x 2}
\CommentTok{##    splits              id         }
\CommentTok{##    <list>              <chr>      }
\CommentTok{##  1 <split [2.9K/1.1K]> Bootstrap01}
\CommentTok{##  2 <split [2.9K/1.1K]> Bootstrap02}
\CommentTok{##  3 <split [2.9K/1.1K]> Bootstrap03}
\CommentTok{##  4 <split [2.9K/1K]>   Bootstrap04}
\CommentTok{##  5 <split [2.9K/1.1K]> Bootstrap05}
\CommentTok{##  6 <split [2.9K/1.1K]> Bootstrap06}
\CommentTok{##  7 <split [2.9K/1.1K]> Bootstrap07}
\CommentTok{##  8 <split [2.9K/1.1K]> Bootstrap08}
\CommentTok{##  9 <split [2.9K/1.1K]> Bootstrap09}
\CommentTok{## 10 <split [2.9K/1K]>   Bootstrap10}
\end{Highlighting}
\end{Shaded}

Bootstrapping is, typically, more of an internal resampling procedure that is naturally built into certain ML algorithms. This will become more apparent in the bagging and random forest chapters (\ref{bagging}-\ref{random-forest}).

\hypertarget{alternatives}{%
\subsection{Alternatives}\label{alternatives}}

Its important to note that there are other useful resampling procedures. If you're working with time-series specific data then you will want to incorporate rolling origin and other time series resampling procedures. \citet{hyndman2018forecasting} is the dominant, R-focused, time series resource\footnote{See their open source book at \url{https://www.otexts.org/fpp2}}.

Additionally, \citet{efron1983estimating} developed the ``632 method'' and \citet{efron1997improvements} discuss the ``632+ method''; both approaches seek to minimize biases experienced with bootstrapping on smaller data sets and are available via \textbf{caret} (see \texttt{?caret::trainControl} for details).

\hypertarget{bias-variance-trade-off-bias-var}{%
\section{\texorpdfstring{Bias variance trade-off \{\#bias-var\}\index{bias variance trade-off}}{Bias variance trade-off \{\#bias-var\}}}\label{bias-variance-trade-off-bias-var}}

Prediction errors can be decomposed into two important subcomponents: error due to ``bias'' and error due to ``variance''. There is often a tradeoff between a model's ability to minimize bias and variance. Understanding how different sources of error lead to bias and variance helps us improve the data fitting process resulting in more accurate models.

\hypertarget{bias}{%
\subsection{Bias}\label{bias}}

\emph{Bias}\index{bias} is the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. It measures how far off in general a model's predictions are from the correct value, which provides a sense of how well a model can conform to the underlying structure of the data. Figure \ref{fig:bias-model} illustrates an example where the polynomial model does not capture the underlying structure well. Linear models are classical examples of high bias models as they are less flexible and rarely capture non-linear, non-monotonic relationships.

We also need to think of bias-variance in relation to resampling. Models with high bias are rarely effected by the noise introduced by resampling. If a model has high bias, it will have consistency in its resampling performance as illustrated by Figure \ref{fig:bias-model}.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/bias-model-1} 

}

\caption{A biased polynomial model fit to a single data set does not capture the underlying non-linear, non-monotonic data structure (left).  Models fit to 25 bootstrapped replicates of the data are underterred by the noise and generates similar, yet still biased, predictions (right).}\label{fig:bias-model}
\end{figure}

\hypertarget{variance}{%
\subsection{Variance}\label{variance}}

On the other hand, error due to \emph{variance}\index{variance} is defined as the variability of a model prediction for a given data point. Many models (e.g., \emph{k}-nearest neighbor, decision trees, gradient boosting machines) are very adaptable and offer extreme flexibility in the patterns that they can fit to. However, these models offer their own problems as they run the risk of overfitting to the training data. Although you may achieve very good performance on your training data, the model will not automatically generalize well to unseen data.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/variance-model-1} 

}

\caption{A high variance k-nearest neighbor model fit to a single data set captures the underlying non-linear, non-monotonic data structure well but also overfits to individual data points (left).  Models fit to 25 bootstrapped replicates of the data are deterred by the noise and generate highly variable predictions (right).}\label{fig:variance-model}
\end{figure}

Since high variance models are more prone to overfitting, using resampling procedures are critical to reduce this risk. Moreover, many algorithms that are capable of achieving high generalization performance have lots of \emph{hyperparameters}\index{hyperparameters} that control the level of model complexity (i.e., the tradeoff between bias and variance).

\hypertarget{tune-overfit}{%
\subsection{Hyperparameter tuning}\label{tune-overfit}}

Hyperparameters (aka \emph{tuning parameters}) are the ``knobs to twiddle''\footnote{This phrase comes from Brad Efron's comments in \citet{breiman2001statistical}} to control the complexity of machine learning algorithms and, therefore, the bias-variance trade-off. Not all algorithms have hyperparameters (e.g., ordinary least squares\footnote{At least in the ordinary sense. You could think of polynomial regression as having a single hyperparamer: the degree of the polynomial.}); however, most have at least one or more.

The proper setting of these hyperparameters are often dependent on the data and problem at hand and cannot always be estimated by the training data alone. Consequently, we need a method of identifying the optimal setting. For example, in the high variance example in the previous section, we illustrated a high variance \emph{k}-nearest neighbor model (we'll discuss \emph{k}-nearest neighbor in Chapter \ref{knn}). \emph{k}-nearest neighbor models have a single hyperparameter (\emph{k}) that determines the predicted value to be made based on the \emph{k} nearest observations in the training data to the one being predicted. If \emph{k} is small (e.g., \(k=3\)), the model will make a prediction for a given observation based on the average of the response values for the 3 observations in the training data most similar to the observation being predicted. This often results in highly variable predicted values because we are basing the prediction (in this case, an average) on a very small subset of the training data. As \emph{k} gets bigger, we base our predictions on an average of a larger subset of the training data, which naturally reduces the variance in our predicted values (remember this for later, averaging reduces variance!). Figure \ref{fig:knn-options} illustrates this point. Smaller \emph{k} values (e.g., 2, 5, or 10) lead to high variance (but lower bias) and larger values (e.g., 150) lead to high bias (but lower variance). The optimal \emph{k} value might exist somewhere between 20--50, but how do we know which value of \emph{k} to use?

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/knn-options-1} 

}

\caption{k-nearest neighbor model with differing values for k.}\label{fig:knn-options}
\end{figure}

One way to perform hyperparameter tuning is to fiddle with hyperparameters manually until you find a great combination of hyperparameter values that result in high predictive accuracy (as measured using \emph{k}-fold CV, for instance). However, this can be very tedious work depending on the number of hyperparameters. An alternative approach is to perform a \emph{grid search}\index{grid search}. A grid search is an automated approach to searching across many combinations of hyperparameter values.

For our \emph{k}-nearest neighbor example, a grid search would predefine a candidate set of values for \emph{k} (e.g., \(k = 1, 2, \dots, j\)) and perform a resampling method (e.g., \emph{k}-fold CV) to estimate which \emph{k} value generalizes the best to unseen data. Figure \ref{fig:knn-tune} illustrates the results from a grid search to assess \(k = 2, 12, 14, \dots, 150\) using repeated 10-fold CV. The error rate displayed represents the average error for each value of \emph{k} across all the repeated CV folds. On average, \(k=46\) was the optimal hyperparameter value to minimize error (in this case, RMSE which is discussed in Section \ref{model-eval})) on unseen data.

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/knn-tune-1} 

}

\caption{Results from a grid search for a k-nearest neighbor model assessing values for k ranging from 2-150.  We see high error values due to high model variance when k is small and we also see high errors values due to high model bias when k is large.  The optimal model is found at k = 46.}\label{fig:knn-tune}
\end{figure}

Throughout this book you'll be exposed to different approaches to performing grid searches. In the above example, we used a \emph{full cartesian grid search}, which assesses every hyperparameter value manually defined. However, as models get more complex and offer more hyperparameters, this approach can become computationally burdensome and requires you to define the optimal hyperparameter grid settings to explore. Additional approaches we'll illustrate include \emph{random grid searches} \citep{bergstra2012random} which explores randomly selected hyperparameter values from a range of possible values, \emph{early stopping} which allows you to stop a grid search once reduction in the error stops marginally improving, \emph{adaptive resampling} via futility analysis \citep{kuhn2014futility} which adaptively resamples candidate hyperparameter values based on approximately optimal performance, and more.

\hypertarget{model-eval}{%
\section{Model evaluation}\label{model-eval}}

Historically, the performance of statistical models was largely based on goodness-of-fit tests and assessment of residuals. Unfortunately, misleading conclusions may follow from predictive models that pass these kinds of assessments \citep{breiman2001statistical}. Today, it has become widely accepted that a more sound approach to assessing model performance is to assess the predictive accuracy via \emph{loss functions}\index{loss functions}. Loss functions are metrics that compare the predicted values to the actual value (the outpuf of a loss function is often referred to as the \emph{error} or pseudo \emph{residual}). When performing resampling methods, we assess the predicted values for a validation set compared to the actual target value. For example, in regression, one way to measure error is to take the difference between the actual and predicted value for a given observation (this is the usual definition of a residual in ordinary linear regression). The overall validation error of the model is computed by aggregating the errors across the entire validation data set.

There are many loss functions to choose when assessing the performance of a predictive model; each providing a unique understanding of the predictive accuracy and differing between regression and classification models. Furthermore, the way a loss function is computed will tend to emphasize certain types of errors over others and can lead to drastic differences in how we interpret the ``optimal model''. Its important to consider the problem context when identifying the preferred performance metric to use. And when comparing multiple models, we need to compare them across the same metric.

\hypertarget{regression-models}{%
\subsection{Regression models}\label{regression-models}}

\begin{itemize}
\item
  \textbf{MSE}: Mean squared error\index{mean squared error} is the average of the squared error (\(MSE = \frac{1}{n} \sum^n_{i=1}(Y_i - \hat Y_i)^2\))\footnote{This deviates slightly from the usual definition of MSE in ordinary linear regression, where we divide by \(n-p\) (to adjust for bias) as opposed to \(n\).}. The squared component results in larger errors having larger penalties. This (along with RMSE) is the most common error metric to use. \textbf{Objective: minimize}
\item
  \textbf{RMSE}: Root mean squared error\index{root mean squared error}. This simply takes the square root of the MSE metric (\(RMSE = \sqrt{\frac{1}{n} \sum^n_{i=1}(y_i - \hat y_i)^2}\)) so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. \textbf{Objective: minimize}
\item
  \textbf{Deviance}: Short for mean residual deviance\index{deviance}. In essence, it provides a degree to which a model explains the variation in a set of data when using maximum likelihood estimation. Essentially this computes a saturated model (i.e.~fully featured model) to an unsaturated model (i.e.~intercept only or average). If the response variable distribution is Gaussian, then it will be approximately equal to MSE. When not, it usually gives a more useful estimate of error. Deviance is often used with classification models. \footnote{See this StackExchange thread (\url{http://bit.ly/what-is-deviance}) for a good overview of deviance for different models and in the context of regression versus classification.} \textbf{Objective: minimize}
\item
  \textbf{MAE}: Mean absolute error\index{mean absolute error}. Similar to MSE but rather than squaring, it just takes the mean absolute difference between the actual and predicted values (\(MAE = \frac{1}{n} \sum^n_{i=1}(\vert y_i - \hat y_i \vert)\)). This results in less emphasis on larger errors than MSE. \textbf{Objective: minimize}
\item
  \textbf{RMSLE}: Root mean squared logarithmic error\index{root mean squared logarithmic error}. Similiar to RMSE but it performs a \texttt{log()} on the actual and predicted values prior to computing the difference (\(RMSLE = \sqrt{\frac{1}{n} \sum^n_{i=1}(log(y_i + 1) - log(\hat y_i + 1))^2}\)). When your response variable has a wide range of values, large repsonse values with large errors can dominate the MSE/RMSE metric. RMSLE minimizes this impact so that small response values with large errors can have just as meaningful of an impact as large response values with large errors. \textbf{Objective: minimize}
\item
  \textbf{\(R^2\)}\index{R squared}: This is a popular metric that represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Unfortunately, it has several limitations. For example, two models built from two different data sets could have the exact same RMSE but if one has less variability in the response variable then it would have a lower \(R^2\) than the other. You should not place too much emphasis on this metric. \textbf{Objective: maximize}
\end{itemize}

Most models we assess in this book will report most, if not all, of these metrics. We will emphasize MSE and RMSE but its important to realize that certain situations warrant emphasis on some metrics more than others.

\hypertarget{classification-models}{%
\subsection{Classification models}\label{classification-models}}

\begin{itemize}
\item
  \textbf{Misclassification}\index{misclassification}: This is the overall error. For example, say you are predicting 3 classes ( \emph{high}, \emph{medium}, \emph{low} ) and each class has 25, 30, 35 observations respectively (90 observations total). If you misclassify 3 observations of class \emph{high}, 6 of class \emph{medium}, and 4 of class \emph{low}, then you misclassified 13 out of 90 observations resulting in a 14\% misclassification rate. \textbf{Objective: minimize}
\item
  \textbf{Mean per class error}\index{mean per class error}: This is the average error rate for each class. For the above example, this would be the mean of \(\frac{3}{25}, \frac{6}{30}, \frac{4}{35}\), which is 12\%. If your classes are balanced this will be identical to misclassification. \textbf{Objective: minimize}
\item
  \textbf{MSE}: Mean squared error. Computes the distance from 1.0 to the probability suggested. So, say we have three classes, A, B, and C, and your model predicts a probability of 0.91 for A, 0.07 for B, and 0.02 for C. If the correct answer was A the \(MSE = 0.09^2 = 0.0081\), if it is B \(MSE = 0.93^2 = 0.8649\), if it is C \(MSE = 0.98^2 = 0.9604\). The squared component results in large differences in probabilities for the true class having larger penalties. \textbf{Objective: minimize}
\item
  \textbf{Cross-entropy (aka Log Loss or Deviance)}\index{cross-entropy}: Similar to MSE but it incorporates a log of the predicted probability multiplied by the true class. Consequently, this metric disproportionately punishes predictions where we predict a small probability for the true class, which is another way of saying having high confidence in the wrong answer is really bad. \textbf{Objective: minimize}
\item
  \textbf{Gini index}\index{Gini index}: Mainly used with tree-based methods and commonly referred to as a measure of \emph{purity} where a small value indicates that a node contains predominantly observations from a single class. \textbf{Objective: minimize}
\end{itemize}

When applying classification models, we often use a \emph{confusion matrix}\index{confusion matrix} to evaluate certain performance measures. A confusion matrix is simply a matrix that compares actual categorical levels (or events) to the predicted categorical levels. When we predict the right level, we refer to this as a \emph{true positive}. However, if we predict a level or event that did not happen this is called a \emph{false positive} (i.e.~we predicted a customer would redeem a coupon and they did not). Alternatively, when we do not predict a level or event and it does happen that this is called a \emph{false negative} (i.e.~a customer that we did not predict to redeem a coupon does).

\begin{figure}

{\centering \includegraphics[width=1\linewidth,height=1\textheight]{images/confusion-matrix} 

}

\caption{Confusion matrix and relationships to terms such as true-positive and false-negative.}\label{fig:confusion-matrix}
\end{figure}

We can extract different levels of performance for binary classifiers. For example, given the classification (or confusion) matrix illustrated in Figure \ref{fig:confusion-matrix2} we can assess the following:

\begin{itemize}
\item
  \textbf{Accuracy}\index{accuracy}: Overall, how often is the classifier correct? Opposite of misclassification above. Example: \(\frac{TP + TN}{total} = \frac{100+50}{165} = 0.91\). \textbf{Objective: maximize}
\item
  \textbf{Precision}\index{precision}: How accurately does the classifier predict events? This metric is concerned with maximizing the true positives to false positive ratio. In other words, for the number of predictions that we made, how many were correct? Example: \(\frac{TP}{TP + FP} = \frac{100}{100+10} = 0.91\). \textbf{Objective: maximize}
\item
  \textbf{Sensitivity\index{sensitivity} (aka recall)}: How accurately does the classifier classify actual events? This metric is concerned with maximizing the true positives to false negatives ratio. In other words, for the events that occurred, how many did we predict? Example: \(\frac{TP}{TP + FN} = \frac{100}{100+5} = 0.95\). \textbf{Objective: maximize}
\item
  \textbf{Specificity}\index{specificity}: How accurately does the classifier classify actual non-events? Example: \(\frac{TN}{TN + FP} = \frac{50}{50+10} = 0.83\). \textbf{Objective: maximize}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth,height=0.5\textheight]{images/confusion-matrix2} 

}

\caption{Example confusion matrix.}\label{fig:confusion-matrix2}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{AUC}: Area under the curve\index{area under the curve}. A good binary classifier will have high precision and sensitivity. This means the classifier does well when it predicts an event will and will not occur, which minimizes false positives and false negatives. To capture this balance, we often use a ROC curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis. A line that is diagonal from the lower left corner to the upper right corner represents a random guess. The higher the line is in the upper left-hand corner, the better. AUC computes the area under this curve. \textbf{Objective: maximize}
\end{itemize}

\begin{figure}

{\centering \includegraphics{bookdown_files/figure-latex/roc-1} 

}

\caption{ROC curve.}\label{fig:roc}
\end{figure}

\hypertarget{put-process-together}{%
\section{Putting the processes together}\label{put-process-together}}

To illustrate how this process works together via R code, let's do a simple assessment on the \texttt{ames} housing data. First, we perform stratified sampling as illustrated in Section \ref{stratified} to break our data into training vs.~test data while ensuring we have consistent distributions between the training and test sets.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# stratified sampling with the rsample package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split  <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(ames, }\DataTypeTok{prop =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Sale_Price"}\NormalTok{)}
\NormalTok{ames_train  <-}\StringTok{ }\KeywordTok{training}\NormalTok{(split)}
\NormalTok{ames_test   <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(split)}
\end{Highlighting}
\end{Shaded}

Next, we're going to apply a \emph{k}-nearest neighbor regressor to our data. To do so, we'll use \textbf{caret}, which is a meta-engine to simplify the resampling, grid search, and model application processes. The following defines:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Resampling method}: we use 10-fold CV repeated 5 times.
\item
  \textbf{Grid search}: we specify the hyperparameter values to assess (\(k = 2, 4, 6, \dots, 25\)).
\item
  \textbf{Model training \& Validation}: we train a \emph{k}-nearest neighbor (\texttt{method\ =\ "knn"}) model using our pre-specified resampling procedure (\texttt{trControl\ =\ cv}), grid search (\texttt{tuneGrid\ =\ hyper\_grid}), and preferred loss function (\texttt{metric\ =\ "RMSE"}).
\end{enumerate}

\begin{warning}
This grid search takes approximately 3.5 minutes
\end{warning}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a resampling method}
\NormalTok{cv <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}
  \DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
  \DataTypeTok{number =} \DecValTok{10}\NormalTok{, }
  \DataTypeTok{repeats =} \DecValTok{5}
\NormalTok{  )}

\CommentTok{# create a hyperparameter grid search}
\NormalTok{hyper_grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{k =} \KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DataTypeTok{by =} \DecValTok{1}\NormalTok{))}

\CommentTok{# fit knn model and perform grid search}
\NormalTok{knn_fit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ ames_train, }
  \DataTypeTok{method =} \StringTok{"knn"}\NormalTok{, }
  \DataTypeTok{trControl =}\NormalTok{ cv, }
  \DataTypeTok{tuneGrid =}\NormalTok{ hyper_grid,}
  \DataTypeTok{metric =} \StringTok{"RMSE"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Looking at our results we see that the best model coincided with \(k=\) 5, which resulted in an RMSE of 44738. This implies that, on average, our model mispredicts the expected sale price of a home by \$44,738. Figure \ref{fig:example-process-assess} illustrates the cross-validated error rate across the spectrum of hyperparameter values that we specified.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# print model results}
\NormalTok{knn_fit}
\CommentTok{## k-Nearest Neighbors }
\CommentTok{## }
\CommentTok{## 2054 samples}
\CommentTok{##   80 predictor}
\CommentTok{## }
\CommentTok{## No pre-processing}
\CommentTok{## Resampling: Cross-Validated (10 fold, repeated 5 times) }
\CommentTok{## Summary of sample sizes: 1849, 1848, 1848, 1849, 1849, 1847, ... }
\CommentTok{## Resampling results across tuning parameters:}
\CommentTok{## }
\CommentTok{##   k   RMSE   Rsquared  MAE  }
\CommentTok{##    2  47138  0.6592    30432}
\CommentTok{##    3  45374  0.6806    29403}
\CommentTok{##    4  45055  0.6847    29194}
\CommentTok{##    5  44738  0.6898    28966}
\CommentTok{##    6  44773  0.6908    28926}
\CommentTok{##    7  44816  0.6918    28970}
\CommentTok{##    8  44911  0.6921    29022}
\CommentTok{##    9  45012  0.6929    29047}
\CommentTok{##   10  45058  0.6945    28972}
\CommentTok{##   11  45057  0.6967    28908}
\CommentTok{##   12  45229  0.6962    28952}
\CommentTok{##   13  45339  0.6961    29031}
\CommentTok{##   14  45492  0.6958    29124}
\CommentTok{##   15  45584  0.6961    29188}
\CommentTok{##   16  45668  0.6964    29277}
\CommentTok{##   17  45822  0.6959    29410}
\CommentTok{##   18  46000  0.6943    29543}
\CommentTok{##   19  46206  0.6927    29722}
\CommentTok{##   20  46417  0.6911    29845}
\CommentTok{##   21  46612  0.6895    29955}
\CommentTok{##   22  46824  0.6877    30120}
\CommentTok{##   23  47009  0.6863    30257}
\CommentTok{##   24  47256  0.6837    30413}
\CommentTok{##   25  47454  0.6819    30555}
\CommentTok{## }
\CommentTok{## RMSE was used to select the optimal model using}
\CommentTok{##  the smallest value.}
\CommentTok{## The final value used for the model was k = 5.}

\CommentTok{# plot cross validation results}
\KeywordTok{ggplot}\NormalTok{(knn_fit)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{bookdown_files/figure-latex/example-process-assess-1}

\}

\textbackslash{}caption\{Results from a grid search for a k-nearest neighbor model on the Ames housing data assessing values for \emph{k} ranging from 2-25.\}\label{fig:example-process-assess}
\textbackslash{}end\{figure\}

The question remains: ``Is this the best predictive model we can find?'' We may have identified the optimal \emph{k}-nearest neighbor model for our given data set, but this doesn't mean we've found the best possible overall model. Nor have we considered potential feature and target engineering options. The remainder of this book will walk you through the journey of identifying alternative solutions and, hopefully, a much more optimal model.

\bibliography{book.bib,packages.bib}

\backmatter
\printindex

\end{document}
