--- 
title: "Hands-on Machine Learning with R"
author: "Brad Boehmke & Brandon Greenwell"
date: "`r Sys.Date()`"
documentclass: krantz
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
colorlinks: yes
lot: yes
lof: yes
site: bookdown::bookdown_site
description: "A Machine Learning Algorithmic Deep Dive Using R."
github-repo: bradleyboehmke/HOML-PDF
graphics: yes
#cover-image: images/cover.jpg
---

```{r setup, include=FALSE}
options(
  htmltools.dir.version = FALSE, formatR.indent = 2, width = 55, digits = 4
)

# install the packages needed by this book; you fill out c(), e.g. c('ggplot2', 'dplyr')
lapply(c('xfun'), function(pkg) {
  if (system.file(package = pkg) == '') install.packages(pkg)
})
```

# Preface {-}

```{block, type = "note"}
**Note to readers**: this text is a work in progress. It will eventually be published by Chapman & Hall/CRC. Prior to the formal copyediting process, we wanted to open it up to public review to get feedback on the content. Any feedback would be greatly appreciated and can be given at https://github.com/koalaverse/hands-on-machine-learning-with-r/issues. Public reviewers that help improve this book will be recognized in the Acknowledge Section.
```

Welcome to *Hands-on Machine Learning with R*.  This book provides hands-on modules for many of the most common machine learning methods to include:

- Generalized low rank models
- Clustering algorithms
- Autoencoders
- Regularized models
- Random forests 
- Gradient boosting machines 
- Deep neural networks
- Stacking / super learners
- and more!

You will learn how to build and tune these various models with R packages that have been tested and approved due to their ability to scale well. However, our motivation in almost every case is to describe the techniques in a way that helps develop intuition for its strengths and weaknesses.  For the most part, we minimize mathematical complexity when possible but also provide resources to get deeper into the details if desired.


## Who should read this {-}

We intend this work to be a practitioner's guide to the machine learning process and a place where one can come to learn about the approach and to gain intuition about the many commonly used, modern, and powerful methods accepted in the machine learning community. If you are familiar with the analytic methodologies, this book may still serve as a reference for how to work with the various R packages for implementation.  While an abundance of videos, blog posts, and tutorials exist online, we have long been frustrated by the lack of consistency, completeness, and bias towards singular packages for implementation. This is what inspired this book. 

This book is not meant to be an introduction to R or to programming in general; as we assume the reader has familiarity with the R language to include defining functions, managing R objects, controlling the flow of a program, and other basic tasks.  If not, we would refer you to [R for Data Science](http://r4ds.had.co.nz/index.html) [@wickham2016r] to learn the fundamentals of data science with R such as importing, cleaning, transforming, visualizing, and exploring your data. For those looking to advance their R programming skills and knowledge of the languge, we would refer you to [Advanced R](http://adv-r.had.co.nz/) [@wickham2014advanced]. Nor is this book designed to be a deep dive into the theory and math underpinning machine learning algorithms. Several books already exist that do great justice in this arena (i.e. [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/) [@esl], [Computer Age Statistical Inference](https://web.stanford.edu/~hastie/CASI/) [@efron2016computer], [Deep Learning](http://www.deeplearningbook.org/) [@goodfellow2016deep]). 

Instead, this book is meant to help R users learn to use the machine learning stack within R, which includes using various R packages such as __glmnet__, __h2o__, __ranger__, __xgboost__, __lime__, and others to effectively model and gain insight from your data. The book favors a hands-on approach, growing an intuitive understanding of machine learning through concrete examples and just a little bit of theory.  While you can read this book without opening R, we highly recommend you experiment with the code examples provided throughout.

## Why R {-}

R has emerged over the last couple decades as a first-class tool for scientific computing tasks, and has been a consistent leader in implementing statistical methodologies for analyzing data. The usefulness of R for data science stems from the large, active, and growing ecosystem of third-party packages: __tidyverse__ for common data analysis activities; __h2o__, __ranger__, __xgboost__, and others for fast and scalable machine learning; __iml__, __pdp__, __vip__, and others for machine learning interpretability; and many more tools will be mentioned throughout the pages that follow.  


## Conventions used in this book {-}

The following typographical conventions are used in this book:

* ___strong italic___: indicates new terms,
* __bold__: indicates package & file names,
* `inline code`: monospaced highlighted text indicates functions or other commands that could be typed literally by the user,
* code chunk: indicates commands or other text that could be typed literally by the user

```{r, first-code-chunk, collapse=TRUE}
1 + 2
```

In addition to the general text used throughout, you will notice the following code chunks with images, which signify:

```{block, type = "tip"}
Signifies a tip or suggestion
```

```{block, type = "note"}
Signifies a general note
```

```{block, type = "warning"}
Signifies a warning or caution
```


## Additional resources {-}

There are many great resources available to learn about machine learning.  Throughout the chapters we try to include many of the resources that we have found extremely useful for digging deeper into the methodology and applying with code. However, due to print restrictions, the hard copy version of this book limits the concepts and methods discussed. Online supplementary material exists at https://github.com/koalaverse/hands-on-machine-learning-with-r. The additional material will accumulate over time and include extended chapter material (i.e., random forest package benchmarking) along with brand new content we couldn’t fit in (i.e., random hyperparameter search). In addition, you can download the data used throughout the book, find teaching resources (i.e., slides and exercises), and more. 


## Feedback {-}

Reader comments are greatly appreciated. To report errors or bugs please post an issue at https://github.com/koalaverse/hands-on-machine-learning-with-r/issues.


## Acknowledgments {-} 

TBD


## Software information {-} 

An online version of this book is available at http://bit.ly/HOML_with_R.  The source of the book along with additional content is available at https://github.com/koalaverse/hands-on-machine-learning-with-r. The book is powered by https://bookdown.org which makes it easy to turn R markdown files into HTML, PDF, and EPUB.

This book was built with the following packages and R version.  All code was executed on 2017 MacBook Pro with a 2.9 GHz Intel Core i7 processor, 16 GB of memory, 2133 MHz speed, and double data rate synchronous dynamic random access memory (DDR3). 

```{r, collapse=TRUE, comment = "#>"}
# packages used
pkgs <- c(
  "AmesHousing",
  "bookdown",
  "caret",
  "cluster",
  "DALEX",
  "data.table",
  "dplyr",
  "dslabs",
  "e1071",
  "earth",
  "emo",
  "extracat",
  "factoextra",
  "ggplot2",
  "gbm",
  "glmnet",
  "h2o",
  "iml",
  "ipred",
  "keras",
  "kernlab",
  "MASS",
  "mclust",
  "mlbench",
  "pBrackets",
  "pdp",
  "pls",
  "pROC",
  "purrr",
  "ranger",
  "recipes",
  "reshape2",
  "ROCR",
  "rpart",
  "rpart.plot",
  "rsample",
  "tfruns",
  "tfestimators",
  "vip",
  "xgboost"
)

# package & session info
sessioninfo::session_info(pkgs)
```

<!--chapter:end:index.Rmd-->

# (PART) Fundamentals {-} 

# Introduction to Machine Learning {#intro}

```{r setup-fundamentals, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center",
  message = FALSE,
  warning = FALSE,
  collapse = TRUE,
  cache = TRUE
)
```

Machine learning (ML) continues to grow in importance for many organizations across nearly all domains. Some example applications of machine learning in practice include:

* Predicting the likelihood of a patient returning to the hospital (_readmission_) within 30 days of discharge.
* Segmenting customers based on common attributes or purchasing behavior for targeted marketing.
* Predicting coupon redemption rates for a given marketing campaign.
* Predicting customer churn so an organization can perform preventative intervention.
* And many more!

In essence, these tasks all seek to learn from data.  To address each scenario, we can use a given set of _features_ to train an algorithm and extract insights. These algorithms, or _learners_, can be classified according to the amount and type of supervision needed during training.  The two main groups this book focuses on are: ___supervised learners___ which construct predictive models, and ___unsupervised learners___ which build descriptive models. Which type you will need to use depends on the learning task you hope to accomplish.


## Supervised learning

A ___predictive model___ is used for tasks that involve the prediction of a given output (or target) using other variables (or features) in the data set. Or, as stated by @apm [p. 2], predictive modeling is "...the process of developing a mathematical tool or model that generates an accurate prediction."  The learning algorithm in a predictive model attempts to discover and model the relationships among the <font color="red">target</font> variable (the variable being predicted) and the other <font color="blue">features</font> (aka predictor variables). Examples of predictive modeling include:

* using customer attributes to predict the probability of the customer churning in the next 6 weeks;
* using <font color="blue">home attributes</font> to predict the <font color="red">sales price</font>;
* using <font color="blue">employee attributes</font> to predict the likelihood of <font color="red">attrition</font>;
* using <font color="blue">patient attributes</font> and symptoms to predict the risk of <font color="red">readmission</font>;
* using <font color="blue">production attributes</font> to predict <font color="red">time to market</font>.

Each of these examples have a defined learning task; they each intend to use attributes ($X$) to predict an outcome measurement ($Y$).

```{block, type = "note"}
Throughout this text we'll use various terms interchangeably for:

- $X$: "predictor variables", "independent variables", "attributes", "features", "predictors"
- $Y$: "target variable", "dependent variable", "response", "outcome measurement"

```

The predictive modeling examples above describe what is known as _supervised learning_.  The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible.

```{block, type = "note"}
In supervised learning, the training data you feed the algorithm includes the target values.  Consequently, the solutions can be used to help _supervise_ the training process to find the optimal algorithm parameters.
```

Most supervised learning problems can be bucketed into one of two categories: _regression_ or _classification_, which we discuss next.

### Regression problems

When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a ___regression problem___ (not to be confused with linear regression modeling).  Regression problems revolve around predicting output that falls on a continuum. In the examples above, predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous.  This means, given the combination of predictor values, the response value could fall anywhere along some continuous spectrum (e.g., the predicted sales price of a particular home could be between \$80,000 and \$755,000).  Figure \@ref(fig:regression-problem) illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along a plane.

```{r regression-problem, echo=FALSE, fig.cap="Average home sales price as a function of year built and total square footage.", fig.height=3, fig.width=3}

library(plotly)
df <- AmesHousing::make_ames()
x <- matrix(sort(df$Gr_Liv_Area)[floor(seq(1, nrow(df), length.out = 15))], 15, 1)
y <- matrix(sort(df$Year_Built)[floor(seq(1, nrow(df), length.out = 15))], 1, 15)
z <- 25051 + 3505*(log(x^.9) %*% log(y)) - 5*as.vector(x) 
c <- matrix(c(.92, .95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, .95), 1, 15)
z <- sweep(z, MARGIN = 2, c, `*`)


# plot_ly(x = as.vector(x), y = as.vector(y), z = z, showscale = FALSE) %>%  
#     add_surface() %>%
#     layout(
#         scene = list(
#             xaxis = list(title = "Feature: square footage"),
#             yaxis = list(title = "Feature: year built"),
#             zaxis = list(title = "Response: sale price")
#         )
#     )

# code for 3D print version
par(mar = c(0.1, 0.1, 0.1, 0.1))  # remove extra white space
persp(
  x = x,
  y = y,
  z = z,
  xlab = "Square footage",
  ylab = "Year built",
  zlab = "Sale price",
  theta = -45,
  phi = 25,
  col = viridis::viridis(100)
)
```


### Classification problems

When the objective of our supervised learning is to predict a categorical outcome, we refer to this as a ___classification problem___.  Classification problems most commonly revolve around predicting a binary or multinomial response measure such as:

* Did a customer redeem a coupon (coded as yes/no or 1/0).
* Did a customer churn (coded as yes/no or 1/0).
* Did a customer click on our online ad (coded as yes/no or 1/0).
* Classifying customer reviews:
    * Binary: positive vs. negative.
    * Multinomial: extremely negative to extremely positive on a 0--5 Likert scale.
    
```{r classification-problem, echo=FALSE, out.width="75%", out.height="75%", fig.cap="Classification problem modeling 'Yes'/'No' response based on three features.", cache=FALSE}

## code to create graphic
#library(DiagrammeR)
# grViz("
#   
#   digraph boxes_and_circles {
#     node [shape = circle]
#     x1; x2; x3;
#     
#     node [shape = box]
#     Model;
#     
#     node [shape = triangle]
#     Yes; No;
# 
#     x1->Model; x2->Model; x3->Model; Model->No; Model->Yes;
# }")

knitr::include_graphics("images/classification_problem.png")
```

However, when we apply machine learning models for classification problems, rather than predict a particular class (i.e., "yes" or "no"), we often want to predict the _probability_ of a particular class (i.e., yes: 0.65, no: 0.35).  By default, the class with the highest predicted probability becomes the predicted class.  Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability).  However, the essence of the problem still makes it a classification problem.

Although there are machine learning algorithms that can be applied to regression problems but not classification and vice versa, most of the supervised learning algorithms we cover in this book can be applied to both.  These algorithms have become the most popular machine learning applications in recent years. 


## Unsupervised learning

___Unsupervised learning___, in contrast to supervised learning, includes a set of statistical tools to better understand and describe your data, but performs the analysis without a target variable.  In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., *clustering*) or the columns (i.e., *dimension reduction*); however, the motive in each case is quite different.

The goal of ___clustering___ is to segment observations into similar groups based on the observed variables; for example, to divide consumers into different homogeneous groups, a process known as market segmentation.  In __dimension reduction__, we are often concerned with reducing the number of variables in a data set. For example, classical linear regression models break down in the presence of highly correlated features.  Some dimension reduction techniques can be used to reduce the feature set to a potentially smaller set of uncorrelated variables. Such a reduced feature set is often used as input to downstream supervised learning models (e.g., principal component regression).

Unsupervised learning is often performed as part of an exploratory data analysis (EDA). However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e., linear regression), then it is possible to check our work by seeing how well our model predicts the response _Y_ on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don’t know the true answer---the problem is unsupervised!  

Despide its subjectivity, the importance of unsupervised learning should not be overlooked and such techniques are often used in organizations to: 

- Divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment.
- Identify groups of online shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers.
- Identify products that have similar purchasing behavior so that managers can manage them as product groups.

These questions, and many more, can be addressed with unsupervised learning.  Moreover, the outputs of an unsupervised learning models can be used as inputs to downstream supervised learning models.



## Roadmap

The goal of this book is to provide effective tools for uncovering relevant and useful patterns in your data by using R's ML stack. We begin by providing an overview of the ML modeling process and discussing fundamental concepts that will carry through the rest of the book. These include feature engineering, data splitting, model validation and tuning, and performance measurement. These concepts will be discussed in Chapters \@ref(process)-\@ref(engineering).

Chapters \@ref(linear-regression)-\@ref(svm) focus on common supervised learners ranging from simpler linear regression models to the more complicated gradient boosting machines and deep neural networks. Here we will illustrate the fundamental concepts of each base learning algorithm and how to tune its hyperparameters to maximize predictive performance.

Chapters \@ref(stacking)-\@ref(iml) delve into more advanced approaches to maximize effectiveness, efficiency, and interpretation of your ML models.  We discuss how to combine multiple models to create a stacked model (aka _super learner_), which allows you to combine the strengths from each base learner and further maximize predictive accuracy. We then illustrate how to make the training and validation process more efficient with automated ML (aka AutoML). Finally, we illustrate many ways to extract insight from your "black box" models with various ML interpretation techniques.

The latter part of the book focuses on unsupervised techniques aimed at reducing the dimensions of your data for more effective data representation (Chapters \@ref(pca)-\@ref(autoencoders)) and identifying common groups among your observations with clustering techniques (Chapters \@ref(kmeans)-\@ref(model-clustering)).


## The data sets {#data}

The data sets chosen for this book allow us to illustrate the different features of the presented machine learning algorithms.  Since the goal of this book is to demonstrate how to implement R's ML stack, we make the assumption that you have already spent significant time cleaning and getting to know your data via EDA. This would allow you to perform many necessary tasks prior to the ML tasks outlined in this book such as:

* Feature selection (i.e., removing unnecessary variables and retaining only those variables you wish to include in your modeling process).
* Recoding variable names and values so that they are meaningful and more interpretable.
* Recoding, removing, or some other approach to handling missing values.

Consequently, the exemplar data sets we use throughout this book have, for the most part, gone through the necessary cleaning processes. In some cases we illustrate concepts with stereotypical data sets (i.e. `mtcars`, `iris`, `geyser`); however, we tend to focus most of our discussion around the following data sets:

* Property sales information as described in @de2011ames.
    - __problem type__: supervised regression
    - __response variable__: `Sale_Price` (i.e., \$195,000, \$215,000)
    - __features__: 80 
    - __observations__: 2,930
    - __objective__: use property attributes to predict the sale price of a home
    - __access__: provided by the `AmesHousing` package [@R-ames]
    - __more details__: See `?AmesHousing::ames_raw`
    
    ```{r import-ames-data}
    # access data
    ames <- AmesHousing::make_ames()

    # initial dimension
    dim(ames)

    # response variable
    head(ames$Sale_Price)
    ```
    
    ```{block, type = "note"}
    You can see the entire data cleaning process to transform the raw Ames housing data (`AmesHousing::ames_raw`) to the final clean  data (`AmesHousing::make_ames`) that we will use in machine learning algorithms throughout this book at:

    https://github.com/topepo/AmesHousing/blob/master/R/make_ames.R

    ```
    
* Employee attrition information originally provided by [IBM Watson Analytics Lab](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/).
    - __problem type__: supervised binomial classification
    - __response variable__: `Attrition` (i.e., "Yes", "No")
    - __features__: 30 
    - __observations__: 1,470
    - __objective__: use employee attributes to predict if they will attrit (leave the company)
    - __access__: provided by the `rsample` package [@R-rsample]
    - __more details__: See `?rsample::attrition`
    
    ```{r import-attrition-data}
    # access data
    attrition <- rsample::attrition

    # initial dimension
    dim(attrition)

    # response variable
    head(attrition$Attrition)
    ```    
    
* Image information for handwritten numbers originally presented to AT&T Bell Lab’s to help build automatic mail-sorting machines for the USPS. Has been used since early 1990s to compare machine learning performance on pattern recognition (i.e., @lecun1990handwritten; @lecun1998gradient; @cirecsan2012multi).
    - __Problem type__: supervised multinomial classification
    - __response variable__: `V785` (i.e., numbers to predict: 0, 1, ..., 9)
    - __features__: 784 
    - __observations__: 60,000 (train) / 10,000 (test)
    - __objective__: use attributes about the "darkness" of each of the 784 pixels in images of handwritten numbers to predict if the number is 0, 1, ..., or 9.
    - __access__: provided by the `dslabs` package [@R-dslabs]
    - __more details__: See `?dslabs::read_mnist()` and [online MNIST documentation](http://yann.lecun.com/exdb/mnist/)

    ```{r import-mnist-data, eval = FALSE}
    #access data
    mnist <- dslabs::read_mnist()
    names(mnist)
    ## [1] "train" "test"

    # initial feature dimensions
    dim(mnist$train$images)
    ## [1] 60000   784

    # response variable
    head(mnist$train$labels)
    ## [1] 5 0 4 1 9 2
    ```   

* Grocery items and quantities purchased. Each observation represents a single basket of goods that were purchased together.
    - __Problem type__: unsupervised basket analysis
    - __response variable__: NA
    - __features__: 42
    - __observations__: 2,000
    - __objective__: use attributes of each basket to identify common groupings of items purchased together.
    - __access__: available via additional online material

    ```{r import-mybasket-data}
    # access data
    my_basket <- readr::read_csv("data/my_basket.csv")

    # initial dimension
    dim(my_basket)

    # response variable
    my_basket
    ```   




<!--chapter:end:01-introduction.Rmd-->

`r if (knitr::is_html_output()) '# References {-}'`

<!--chapter:end:99-references.Rmd-->

